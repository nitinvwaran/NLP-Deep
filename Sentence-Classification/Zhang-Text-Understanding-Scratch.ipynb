{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**A Tensorflow Implementation of 'Text Understanding From Scratch', a Character-Level Deep ConvNet for Sentence Classification**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tensorflow implementation of 'Text Understanding From Scratch' by Xiang Zhang and Yann LeCun. The paper can be found here: https://arxiv.org/abs/1502.01710\n",
    "\n",
    "\n",
    "The paper describes a Deep Character-Level CNN which has been implemented in tensorflow in this notebook. There are a few differences\n",
    "1. 43 characters are used instead of the range of 69 characters in the original.\n",
    "2. The length sequence was taken as upto the 75th percentile of the length of all the sentences. This number is 584 characters, after stripping the unwanted characters as above\n",
    "3. Batch Normalization was applied in all layers except the character quantization layer and final layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 1: Initialize all the dependencies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, random , math, shutil, glob, uuid\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan)\n",
    "\n",
    "'''\n",
    "# For removal of stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from stop_words import get_stop_words\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Step 2: Initialize all the global variables including character dictionary size**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Location of the dataset and disk cache files\n",
    "home_dir = ''\n",
    "\n",
    "# Files for train dataset disk cache\n",
    "one_hot_train_dir = home_dir + 'train/'\n",
    "\n",
    "# Files for validation dataset disk cache\n",
    "one_hot_valid_dir = home_dir + 'valid/\n",
    "\n",
    "# Files for test dataset disk cache\n",
    "one_hot_test_dir = home_dir + 'test/'\n",
    "\n",
    "# The training file\n",
    "train_file = home_dir + 'train_sample.csv'\n",
    "\n",
    "# Testing file\n",
    "test_file = home_dir + 'test_sample.csv'\n",
    "\n",
    "\n",
    "# Stores all the checkpoints\n",
    "checkpoint_dir = ''\n",
    "\n",
    "\n",
    "# The original dictionary from the paper\n",
    "'''\n",
    "one_hot_column_label = {'a':68,'b':67,'c':66,'d':65,'e':64,'f':63,'g':62,'h':61,'i':60,'j':59,'k':58,'l':57,'m':56,'n':55,'o':54,\n",
    "                        'p':53,'q':52,'r':51,'s':50,'t':49,'u':48,'v':47,'w':46,'x':45,'y':44,'z':43,'0':42,'1':41,'2':40,'3':39,'4':38,'5':37,'6':36,'7':35,\n",
    "                        '8':34,'9':33,'-':32,',':31,';':30,'.':29,'!':28,'?':27,':':26,'\"':25,'\\'':24,'/':23,'\\\\':22,'|':21,'_':20,\n",
    "                        '@':19,'#':18,'$':17,'%':16,'^':15,'&':14,'*':13,'~':12,'`':11,'+':10,'-':9,'=':8,'<':7,'>':6,'(':5,')':4,'[':3,\n",
    "                        ']':2,'{':1,'}':0}\n",
    "'''\n",
    "\n",
    "\n",
    "# Index denotes the column placement in the one-hot vector matrix\n",
    "# This is used to create the one-hot character vector of sentence inputs\n",
    "one_hot_column_label = {'a':42,'b':41,'c':40,'d':39,'e':38,'f':37,'g':36,'h':35,'i':34,'j':33,'k':32,'l':31,'m':30,'n':29,'o':28,\n",
    "                        'p':27,'q':26,'r':25,'s':24,'t':23,'u':22,'v':21,'w':20,'x':19,'y':18,'z':17,'0':16,'1':15,'2':14,'3':13,'4':12,'5':11,'6':10,'7':9,\n",
    "                        '8':8,'9':7,'-':6,'#':5,'.':4,'!':3,'?':2,':':1,';':0}\n",
    "\n",
    "\n",
    "# Dimension attribute for one-hot character matrix\n",
    "num_chars_dict = 43\n",
    "\n",
    "# Number of labels to classify\n",
    "num_labels = 5\n",
    "\n",
    "# Tensorboard directories\n",
    "train_tensorboard_dir = home_dir + 'train/tensorboard/'\n",
    "valid_tensorboard_dir = home_dir + 'valid/tensorboard/'\n",
    "\n",
    "\n",
    "# A log directory where confusion matrix, loss value, accuracy value will be written to\n",
    "log_dir = ''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 3: Pre-processing of sentences**\n",
    "<br />\n",
    "1. Remove unwanted characters <br/>\n",
    "2. Get sentence Length parameter for the Network (cutoff_len) <br/>\n",
    "3. Get the train-dev dataset split <br/>\n",
    "4. Verify that stratified sampling was done correctly in the train-dev split <br/>\n",
    "5. Optional - remove stopwords\n",
    "<br/>\n",
    "<br/>\n",
    "There are ~624,000 examples in the training dataset, and 20,000 in the dev dataset\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "The longest sentence is of length:1003\n",
      "The shortest sentence is of length:9\n",
      "The average length is:428.683941052217\n",
      "The 75th percentile is:584.0\n",
      "The 90th percentile is:778.0\n",
      "Verify Stratified Sampling\n",
      "584\n",
      "The cutoff length and counts\n",
      "584\n",
      "623999\n",
      "26000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def strip_characters(sent):\n",
    "    \n",
    "    \"\"\"\n",
    "    Function strips the unwanted characters from the sentence\n",
    "    Add extra functionality here (e.g, removal of stopwords)\n",
    "    \"\"\"\n",
    "    \n",
    "    #sent = sent.replace('-','')\n",
    "    sent = sent.replace(',','')\n",
    "    #sent = sent.replace(';','')\n",
    "    #sent = sent.replace('.','')\n",
    "    #sent = sent.replace(':','')\n",
    "    sent = sent.replace('\"','')\n",
    "    \n",
    "    sent = sent.replace('/','')\n",
    "    sent = sent.replace('\\\\','')\n",
    "    sent = sent.replace('|','')\n",
    "    sent = sent.replace('_','')\n",
    "    sent = sent.replace('@','')\n",
    "    #sent = sent.replace('#','')\n",
    "    sent = sent.replace('$','')\n",
    "    sent = sent.replace('^','')                        \n",
    "    sent = sent.replace('&','')                        \n",
    "    sent = sent.replace('*','')                        \n",
    "    sent = sent.replace('~','')                            \n",
    "    sent = sent.replace('`','')    \n",
    "    sent = sent.replace('+','')                        \n",
    "    sent = sent.replace('-','')    \n",
    "    sent = sent.replace('=','')\n",
    "    sent = sent.replace('<','')\n",
    "    sent = sent.replace('>','')                        \n",
    "    sent = sent.replace('(','')                        \n",
    "    sent = sent.replace(')','')                        \n",
    "    sent = sent.replace('[','')                        \n",
    "    sent = sent.replace(']','')                        \n",
    "    sent = sent.replace('{','')                        \n",
    "    sent = sent.replace('}','')        \n",
    "    \n",
    "    '''\n",
    "    # remove the stop words\n",
    "    stop_words = list(get_stop_words('en'))\n",
    "    nltk_words = list(stopwords.words('english'))\n",
    "    stop_words.extend(nltk_words)\n",
    "    sent_words = sent.split(' ')\n",
    "    sent_stopped_words = [w for w in sent_words if not w in stop_words]\n",
    "    stopped_sent = ' '.join(sent_stopped_words)\n",
    "    '''\n",
    "    \n",
    "    return sent\n",
    "\n",
    "                        \n",
    "def main():                        \n",
    "\n",
    "    train_data = pd.read_csv(train_file)\n",
    "    test_data = pd.read_csv(test_file)\n",
    "\n",
    "    # We get the max_len for padding and for the convolution filter definition in the model\n",
    "    max_len = 0\n",
    "    total_len = 0\n",
    "    min_len = 999\n",
    "    ls_len = []\n",
    "    \n",
    "    # Used to store the processed reviews\n",
    "    stripped_file_train =  home_dir + 'file_strip_train.csv'   \n",
    "    stripped_file_test = home_dir + 'file_strip_test.csv'\n",
    "\n",
    "    with open(stripped_file_train,'w') as strip:   \n",
    "    \n",
    "        strip.write('Review' + ',' + 'Description' + '\\n')\n",
    "\n",
    "        for index,row in train_data.iterrows():\n",
    "    \n",
    "            sent_row = strip_characters(row[2])\n",
    "            strip.write(str(row[0]) + ','  + sent_row + '\\n')    \n",
    "        \n",
    "        \n",
    "    with open(stripped_file_test,'w') as strip:\n",
    "     \n",
    "        strip.write('Review' + ',' + 'Description' + '\\n')\n",
    "    \n",
    "        for index,row in test_data.iterrows():\n",
    "        \n",
    "            sent_row = strip_characters(row[2])\n",
    "            strip.write(str(row[0]) + ','  + sent_row + '\\n')    \n",
    "\n",
    "        \n",
    "\n",
    "    strip_train_data = pd.read_csv(stripped_file_train)\n",
    "    X_test = pd.read_csv(stripped_file_test)\n",
    "        \n",
    "    label_data = strip_train_data.loc[:,'Review']\n",
    "    all_data = strip_train_data\n",
    "    all_data = all_data.append(X_test,ignore_index=True)\n",
    "\n",
    "    for index, row in all_data.iterrows():\n",
    "        total_len += len(row[1])\n",
    "        ls_len.append(len(row[1]))\n",
    "        if (max_len < len(row[1])):\n",
    "            max_len = len(row[1])\n",
    "        if (min_len > len(row[1])):\n",
    "            min_len = len(row[1])\n",
    "            \n",
    "    np_len = np.asarray(ls_len)\n",
    "    \n",
    "    # Statistics on the reviews\n",
    "    print ('The longest sentence is of length:' + str(max_len))\n",
    "    print ('The shortest sentence is of length:' + str(min_len))\n",
    "    print ('The average length is:' + str(total_len/strip_train_data.shape[0]))\n",
    "    print ('The 75th percentile is:' + str(np.percentile(np_len,75)))\n",
    "    print ('The 90th percentile is:' + str(np.percentile(np_len,90)))\n",
    "\n",
    "\n",
    "\n",
    "    # Train and test split (stratified sampling, 96-04 split)\n",
    "    X_train,X_valid,y_train,y_valid = train_test_split(strip_train_data,label_data,test_size=0.04,train_size=0.96,random_state=13814,shuffle=True,stratify=label_data)\n",
    " \n",
    "    # Verify that stratified sampling worked (histogram distributions must be same)\n",
    "    print ('Verify Stratified Sampling')\n",
    "    X_train.hist(column=\"Review\") # First histogram is training\n",
    "    X_valid.hist(column=\"Review\") # Second histogram is validation\n",
    "\n",
    "\n",
    "    # Return the important global variables\n",
    "    cutoff_len = int(np.percentile(np_len,75)) # a graph parameter\n",
    "    print(cutoff_len)\n",
    "    train_count = X_train.shape[0]\n",
    "    valid_count = X_valid.shape[0]\n",
    "    \n",
    "    return cutoff_len, train_count, valid_count, X_train, X_valid\n",
    "    \n",
    "\n",
    "    \n",
    "# Main cell execution    \n",
    "cutoff_len, train_count, valid_count, X_train, X_valid = main()\n",
    "print ('The cutoff length and counts')\n",
    "print (str(cutoff_len))\n",
    "print (str(train_count))       \n",
    "print (str(valid_count))       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 4: Create One-Hot Character Matrix for each sentence, on disk**\n",
    "<br />\n",
    "\n",
    "1. Now convert each sentence to one-hot character matrix and store this character matrix representation in disk cache. \n",
    "<br />\n",
    "Characters not in the dictionary will be defaulted to an all-zero vector <br/>\n",
    "2. Also store in disk cache, the label and a one-hot representation of the label, for the cross-entropy loss. <br/>\n",
    "3. To re-generate the cache, delete the 'train','valid',and 'test' folders and all contents from disk. Otherwise, re-running this cell will not re-generate the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_numpy_one_hot(X_dataset,unique_char_size,save_dir,one_hot_column_label):\n",
    "    \n",
    "    for index,row in X_dataset.iterrows():\n",
    "        \n",
    "        sent = row[1]\n",
    "        np_one_hot = np.zeros(shape=(len(sent),unique_char_size),dtype=np.int32)\n",
    "        \n",
    "        for i in range(0,len(sent)):\n",
    "            if (sent[i].lower() in one_hot_column_label):\n",
    "                np_one_hot[i][one_hot_column_label[sent[i].lower()]] = 1 # else, it remains 0 encoded.\n",
    "                \n",
    "        # Save the one-hot encoding for each sentence to disk\n",
    "        np.save(save_dir + 'inputs/' + 'one_hot_sentence_' + str(index) + '.npy',np_one_hot)\n",
    "        \n",
    "        #Save the label in .npy disk cache\n",
    "        lbl = int(row[0]) - 1 # Must start with 0-indexing\n",
    "        np.save(save_dir + 'labels/' +  'sentence_label_' + str(index) + '.npy',np.asarray(lbl))\n",
    "        \n",
    "        # Save a one-hot version of the label (for xentropy loss function)\n",
    "        np_one_hot_label = np.zeros(shape=(5),dtype=np.int32)\n",
    "        np_one_hot_label[lbl] = 1\n",
    "        np.save(save_dir + 'one_hot_labels/' + 'one_hot_sentence_label_' + str(index) + '.npy',np_one_hot_label)\n",
    "        \n",
    "\n",
    "# Separate implementation for test data    \n",
    "def save_numpy_one_hot_test(X_dataset,unique_char_size,save_dir,one_hot_column_label):\n",
    "    \n",
    "    for index,row in X_dataset.iterrows():\n",
    "        \n",
    "        sent = row[1]\n",
    "        np_one_hot = np.zeros(shape=(len(sent),unique_char_size),dtype=np.int32)\n",
    "        \n",
    "        for i in range(0,len(sent)):\n",
    "            if (sent[i].lower() in one_hot_column_label):\n",
    "                np_one_hot[i][one_hot_column_label[sent[i].lower()]] = 1\n",
    "       \n",
    "        np.save(save_dir + 'inputs/' + 'one_hot_sentence_' + str(index) + '.npy',np_one_hot)\n",
    "        \n",
    "        \n",
    "def inp_create_one_hot_char_mat(unique_char_size,train_dir,valid_dir,test_dir):\n",
    "    \n",
    "    save_numpy_one_hot(X_train,unique_char_size,train_dir,one_hot_column_label)\n",
    "    save_numpy_one_hot(X_valid,unique_char_size,valid_dir,one_hot_column_label)\n",
    "    #save_numpy_one_hot_test(X_test,unique_char_size,test_dir,one_hot_column_label)\n",
    "    \n",
    "\n",
    "# This cell can be rerun. To rerun the cell, remove the train, valid, and test folders in the disk cache.\n",
    "# If the folders are not removed, re-running this cell does nothing\n",
    "if (not os.path.exists(one_hot_train_dir)):\n",
    "        \n",
    "        print ('Setting up directories and preparing one-hot character vectors')\n",
    "        print ('Number chars:' + str(num_chars_dict))\n",
    "        os.mkdir(one_hot_train_dir)\n",
    "        os.mkdir(one_hot_valid_dir)\n",
    "        os.mkdir(one_hot_test_dir)\n",
    "        os.mkdir(one_hot_train_dir + 'inputs/')\n",
    "        os.mkdir(one_hot_valid_dir + 'inputs/')\n",
    "        os.mkdir(one_hot_test_dir + 'inputs/')\n",
    "        os.mkdir(one_hot_train_dir + 'labels/')\n",
    "        os.mkdir(one_hot_valid_dir + 'labels/')\n",
    "        os.mkdir(one_hot_train_dir + 'one_hot_labels/')\n",
    "        os.mkdir(one_hot_valid_dir + 'one_hot_labels/')\n",
    "        os.mkdir(one_hot_train_dir + 'tensorboard/')\n",
    "        os.mkdir(one_hot_valid_dir + 'tensorboard/')\n",
    "        inp_create_one_hot_char_mat(num_chars_dict,one_hot_train_dir,one_hot_valid_dir,one_hot_test_dir)\n",
    "        X_train = None\n",
    "        X_valid = None\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 5: Build the graph, and loss / optimizer operations**\n",
    "<br />\n",
    "Adam optimizer is used, with initial learning rate = 0.01 <br/>\n",
    "Cross-entropy loss function is used\n",
    "Mini-batch SGD with batch_size = 128 and number of batches per epoch = 12,800\n",
    "<br/>\n",
    "<br />\n",
    "The graph structure is as below and replicates that of the paper. Batch normalization is applied immediately after convolution layer and fully connected layer, in all layers except the first and last layer. <br />\n",
    "\n",
    "1. Batch Input as [batch_size, cutoff_len, char_dict_dim] where cutoff_len = 584 and char_dict_dim = 43 <br/>\n",
    "2. Conv1D with number filters = 1024, kernel size = 7 <br/>\n",
    "3. MaxPool1D with pool size = stride length = 3 <br/>\n",
    "4. Conv1D with number filters = 1024, kernel size = 7 <br/>\n",
    "5. MaxPool1D with pool size = stride length = 3 <br/>\n",
    "6. Conv1D with number filters = 1024, kernel size = 3 <br/>\n",
    "7. Conv1D with number filters = 1024, kernel size = 3 <br/>\n",
    "8. Conv1D with number filters = 1024, kernel size = 3 <br/>\n",
    "9. Conv1D with number filters = 1024, kernel size = 3 <br/>\n",
    "10. MaxPool1D with pool size = stride length = 3 <br/>\n",
    "11. Fully Connected Layer with n = 2048 and dropout = 0.6\n",
    "12. Fully Connected Layer with n = 2048 and dropout = 0.6\n",
    "13. Fully Connected Layer with n = number of labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def build_graph_convnets(cutoff_len):\n",
    "    \n",
    "    \"\"\"\n",
    "    Defines the graph according to the paper it is based on ('Text Understanding from Scratch' by Xiang Zhang and Yann LeCun)\n",
    "    \"\"\"\n",
    "    \n",
    "    num_chars = num_chars_dict\n",
    "    \n",
    "    first_window_size = 7 # First window applied to sentence\n",
    "    second_window_size = 7 # Second window\n",
    "    third_window_size = 3 # And so on...\n",
    "    fourth_window_size = 3\n",
    "    fifth_window_size = 3\n",
    "    sixth_window_size = 3\n",
    "    \n",
    "    # No third, fourth, or fifth pool size, copying the paper's approach \n",
    "    first_pool_size = 3\n",
    "    second_pool_size = 3\n",
    "    sixth_pool_size = 3\n",
    "    \n",
    "    # One channel output for each of the six conv layers\n",
    "    first_layer_no_filters = 1024 \n",
    "    second_layer_no_filters = 1024\n",
    "    third_layer_no_filters = 1024\n",
    "    fourth_layer_no_filters = 1024\n",
    "    fifth_layer_no_filters = 1024\n",
    "    sixth_layer_no_filters = 1024\n",
    "    \n",
    "    first_fc_out = 2048\n",
    "    second_fc_out = 2048\n",
    "    third_fc_out = num_labels \n",
    "    # Hyper parameters end \n",
    "    \n",
    "    reuse_flag = tf.AUTO_REUSE\n",
    "    \n",
    "\n",
    "    with tf.variable_scope('layer_zero_convnet',reuse=reuse_flag):\n",
    "        \n",
    "        input_tensor = tf.placeholder(dtype=tf.float32,shape=[None,cutoff_len,num_chars_dict],name=\"input_tensor\") \n",
    "        conv_keep_prob = tf.placeholder(dtype=tf.float32,shape=[],name=\"conv_dropout_prob\")\n",
    "        fc_keep_prob = tf.placeholder(dtype=tf.float32,shape=[],name=\"fc_dropout_prob\")\n",
    "        batch_norm_train = tf.placeholder(name=\"batch_norm_train\", dtype=tf.bool)\n",
    "        print (input_tensor)\n",
    "        \n",
    "    with tf.variable_scope('layer_one_convnet',reuse=reuse_flag):\n",
    "        \n",
    "        #l1w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l1w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        \n",
    "        first_conv_layer = tf.layers.conv1d(inputs = input_tensor, filters = first_layer_no_filters,kernel_size = first_window_size,strides=1,padding='valid',\n",
    "                                            activation=tf.nn.relu, use_bias=True, kernel_initializer= l1w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"first_conv_layer\")\n",
    "        first_conv_batch = tf.contrib.layers.batch_norm(first_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        first_maxpool_layer = tf.layers.max_pooling1d(inputs = first_conv_batch, pool_size = first_pool_size, strides=first_pool_size,\n",
    "                                                      padding='valid',name=\"first_maxpool_layer\")\n",
    "        \n",
    "\n",
    "        \n",
    "    with tf.variable_scope('layer_two_convnet',reuse=reuse_flag):\n",
    "        \n",
    "        #l2w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l2w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        second_conv_layer = tf.layers.conv1d(inputs = first_maxpool_layer, filters = second_layer_no_filters,kernel_size = second_window_size,strides=1,padding='valid'\n",
    "                                            ,activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"second_conv_layer\")\n",
    "        second_conv_batch = tf.contrib.layers.batch_norm(second_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        second_maxpool_layer = tf.layers.max_pooling1d(inputs = second_conv_batch, pool_size = second_pool_size, strides= second_pool_size,\n",
    "                                                      padding='valid',name=\"second_maxpool_layer\")\n",
    "\n",
    "\n",
    "        \n",
    "    with tf.variable_scope('layer_three_convnet',reuse=reuse_flag):\n",
    "        #l3w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l3w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        third_conv_layer = tf.layers.conv1d(inputs = second_maxpool_layer, filters = third_layer_no_filters,kernel_size = third_window_size,strides=1,padding='valid',\n",
    "                                            activation=tf.nn.relu, use_bias=True, kernel_initializer= l3w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"third_conv_layer\")\n",
    "        third_conv_batch = tf.contrib.layers.batch_norm(third_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        \n",
    "    with tf.variable_scope('layer_four_convnet',reuse=reuse_flag):\n",
    "        #l4w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l4w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        fourth_conv_layer = tf.layers.conv1d(inputs = third_conv_batch, filters = fourth_layer_no_filters,kernel_size = fourth_window_size,strides=1,padding='valid',\n",
    "                                            activation=tf.nn.relu, use_bias=True, kernel_initializer= l4w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"fourth_conv_layer\")\n",
    "        fourth_conv_batch = tf.contrib.layers.batch_norm(fourth_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        \n",
    "    with tf.variable_scope('layer_five_convnet',reuse=reuse_flag):\n",
    "        #l5w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l5w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        fifth_conv_layer = tf.layers.conv1d(inputs = fourth_conv_batch, filters = fifth_layer_no_filters,kernel_size = fifth_window_size,strides=1,padding='valid',\n",
    "                                            activation=tf.nn.relu, use_bias=True, kernel_initializer= l5w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"fifth_conv_layer\")\n",
    "        fifth_conv_batch = tf.contrib.layers.batch_norm(fifth_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        \n",
    "    with tf.variable_scope('layer_six_convnet',reuse=reuse_flag):\n",
    "        #l6w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l6w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        sixth_conv_layer = tf.layers.conv1d(inputs = fifth_conv_batch, filters = sixth_layer_no_filters,kernel_size = sixth_window_size,strides=1,padding='valid',\n",
    "                                            activation=tf.nn.relu, use_bias=True, kernel_initializer= l6w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"sixth_conv_layer\")\n",
    "        sixth_conv_batch = tf.contrib.layers.batch_norm(sixth_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        sixth_maxpool_layer = tf.layers.max_pooling1d(inputs = sixth_conv_batch, pool_size = sixth_pool_size, strides=sixth_pool_size,\n",
    "                                                      padding='valid',name=\"sixth_maxpool_layer\")\n",
    "      \n",
    "        sixth_reshaped_layer = tf.contrib.layers.flatten(sixth_maxpool_layer) \n",
    "        print (sixth_reshaped_layer)\n",
    "        \n",
    "    \n",
    "    with tf.variable_scope('layer_seven_convnet',reuse=reuse_flag):\n",
    "        #l7w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l7w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        l7b_init = tf.zeros_initializer()\n",
    "        \n",
    "        seventh_fc = tf.contrib.layers.fully_connected(inputs=sixth_reshaped_layer,num_outputs=first_fc_out,activation_fn = tf.nn.relu,\n",
    "                                                       weights_initializer = l7w_init,biases_initializer = l7b_init,scope=\"seventh_fc\")\n",
    "        seventh_fc_batch = tf.contrib.layers.batch_norm(seventh_fc,center = True, scale=True,is_training=batch_norm_train)\n",
    "        seventh_fc_final = tf.nn.dropout(seventh_fc_batch,keep_prob = fc_keep_prob)\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('layer_eight_convnet',reuse=reuse_flag):\n",
    "        #l8w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l8w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        l8b_init = tf.zeros_initializer()\n",
    "        \n",
    "        eigth_fc = tf.contrib.layers.fully_connected(inputs=seventh_fc_final,num_outputs=second_fc_out,activation_fn = tf.nn.relu,\n",
    "                                                       weights_initializer = l8w_init,biases_initializer = l8b_init,scope=\"eigth_fc\")\n",
    "        eigth_fc_batch = tf.contrib.layers.batch_norm(eigth_fc,center = True, scale=True,is_training=batch_norm_train)\n",
    "        eigth_fc_final = tf.nn.dropout(eigth_fc_batch,keep_prob = fc_keep_prob)\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('layer_ninth_convnet',reuse=reuse_flag):\n",
    "        #l9w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32) \n",
    "        l9w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        l9b_init = tf.zeros_initializer()\n",
    "        \n",
    "        ninth_fc = tf.contrib.layers.fully_connected(inputs=eigth_fc_final,num_outputs=num_labels,activation_fn = None,\n",
    "                                                     weights_initializer = l9w_init,biases_initializer = l9b_init,scope=\"ninth_fc\")\n",
    "\n",
    "        ninth_final_softmax = tf.nn.softmax(ninth_fc,name=\"ninth_final_softmax\")\n",
    "        \n",
    "    return input_tensor, batch_norm_train, ninth_fc,ninth_final_softmax, conv_keep_prob,fc_keep_prob\n",
    "\n",
    "\n",
    "def build_loss_optimizer(logits,softmax_logits,num_labels):\n",
    "    \n",
    "    \"\"\"\n",
    "    Defines the loss and optimizer \n",
    "    Can be reused across graph definitions\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    with tf.variable_scope('cross_entropy'):\n",
    "        \n",
    "        learning_rate_input = tf.placeholder(\n",
    "            tf.float32, [], name='learning_rate_input')\n",
    "        one_hot_labels = tf.placeholder(\n",
    "            name=\"one_hot_labels\",shape=[None,num_labels],dtype=tf.int32)\n",
    "        labels = tf.placeholder(\n",
    "            name=\"labels\",shape=[None],dtype=tf.int64)\n",
    "        \n",
    "        #cross_entropy_mean = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits=logits)\n",
    "        #cross_entropy_mean = tf.nn.softmax_cross_entropy_with_logits_v2(labels = one_hot_labels,logits=logits,name=\"cross_entropy_mean_v2\")\n",
    "        cross_entropy_mean = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logits,name=\"cross_entropy_mean_sparse\")\n",
    "        loss = tf.reduce_mean(cross_entropy_mean,name=\"cross_entropy_loss\")\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_input)\n",
    "        #optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate_input)\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate_input)\n",
    "        \n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # For batch normalization ops update\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            train_step = optimizer.minimize(loss)\n",
    "        \n",
    "        predicted_indices = tf.argmax(softmax_logits, 1, name=\"predicted_indices\")\n",
    "        correct_prediction = tf.equal(predicted_indices, labels, name='correct_prediction') # Labels are 0-indexed\n",
    "        confusion_matrix = tf.confusion_matrix(\n",
    "            labels, predicted_indices, num_classes=num_labels, name=\"confusion_matrix\")\n",
    "        \n",
    "    return train_step, confusion_matrix,predicted_indices,cross_entropy_mean, learning_rate_input,labels, one_hot_labels,loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 6: Create a cache of all the training files, to be sampled from**\n",
    "<br />\n",
    "\n",
    "This significantly speeds up training performance during mini-batch descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def helper():\n",
    "    all_files = os.listdir(one_hot_train_dir + 'inputs/')\n",
    "\n",
    "    with open (home_dir + 'train_npy.txt','w') as tnpy:\n",
    "        for row in all_files:\n",
    "            tnpy.write(row + '\\n')\n",
    "            \n",
    "helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 7: Create mini-batches, and begin training**\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def prepare_randomized_batches(save_dir,batch_size,cutoff_len,file_count):\n",
    "    \n",
    "    \"\"\"\n",
    "    Prepares randomized batches of the batch_size and stores the batches in cache.\n",
    "    Used for dev data batch preparation and processing\n",
    "    \n",
    "    file_count is a parameter that can be used to induce a force stop\n",
    "    after batching a file_count number of files\n",
    "    \"\"\"\n",
    "    \n",
    "    # Clean up for rerunning\n",
    "    if (os.path.exists(save_dir + 'inputs/batch/')):\n",
    "        shutil.rmtree(save_dir + 'inputs/batch/')\n",
    "    os.mkdir(save_dir + 'inputs/batch/')\n",
    "        \n",
    "    if (os.path.exists(save_dir + 'labels/batch/')):\n",
    "        shutil.rmtree(save_dir + 'labels/batch/')\n",
    "    os.mkdir(save_dir + 'labels/batch/')\n",
    "        \n",
    "    if (os.path.exists(save_dir + 'one_hot_labels/batch/')):\n",
    "        shutil.rmtree(save_dir + 'one_hot_labels/batch/')\n",
    "    os.mkdir(save_dir + 'one_hot_labels/batch/')\n",
    "    \n",
    "    # For batch preparation\n",
    "    ls_arrays = []\n",
    "    ls_arrays_one_hot_label = []\n",
    "    ls_arrays_label = []\n",
    "    \n",
    "    # A counter\n",
    "    j = 0\n",
    "    \n",
    "    # Randomize the Batch preparation\n",
    "    os.chdir(save_dir + 'inputs/')\n",
    "    \n",
    "    print ('The input directory is:' + save_dir + 'inputs/')\n",
    "    \n",
    "    file_list = glob.glob('*.npy')\n",
    "    random.shuffle(file_list)\n",
    "    \n",
    "    \n",
    "    # Prepare the batches\n",
    "    for file_item in file_list:\n",
    "    \n",
    "        nparr = np.load(save_dir + 'inputs/' + file_item)\n",
    "                \n",
    "        # Make sure it isnt an empty file\n",
    "        if nparr is not None and nparr.shape[0] != 0:\n",
    "                    \n",
    "            # Bottom-Pad  the sentences by adding 0s to the end of the sentence\n",
    "            # Or Truncate sentences that are longer than the cutoff length\n",
    "            if (nparr.shape[0] <= cutoff_len):\n",
    "                npad = ((0,cutoff_len - nparr.shape[0]),(0,0))\n",
    "                nparr_pad = np.pad(nparr, pad_width=npad, mode='constant', constant_values=0)\n",
    "            else: # Truncate to the cutoff length\n",
    "                nparr_pad = nparr[:cutoff_len,:]\n",
    "            \n",
    "            ls_arrays.append(nparr_pad.tolist())\n",
    "            \n",
    "            one_hot_label = np.load(save_dir + 'one_hot_labels/' + 'one_hot_sentence_label_' + file_item.split('_')[3])\n",
    "            ls_arrays_one_hot_label.append(one_hot_label.tolist())\n",
    "            \n",
    "            label = np.load(save_dir + 'labels/' + 'sentence_label_' + file_item.split('_')[3])\n",
    "            ls_arrays_label.append(label.tolist())\n",
    "            \n",
    "            j += 1\n",
    "                \n",
    "            # Save batches of the files every batch_size step\n",
    "            if (j % batch_size == 0 or j == file_count):\n",
    "                    \n",
    "                nparr_batch = np.asarray(ls_arrays)\n",
    "                nparr_batch_one_hot_labels = np.asarray(ls_arrays_one_hot_label)\n",
    "                nparr_batch_labels = np.asarray(ls_arrays_label)\n",
    "                    \n",
    "                np.save(save_dir + 'inputs/batch/nparr_batch_' + str(j) + '.npy',nparr_batch)\n",
    "                np.save(save_dir + 'labels/batch/nparr_batch_labels_' + str(j) +  '.npy',nparr_batch_labels)\n",
    "                np.save(save_dir + 'one_hot_labels/batch/nparr_batch_one_hot_labels_' + str(j) +'.npy',nparr_batch_one_hot_labels)\n",
    "                    \n",
    "                ls_arrays = []\n",
    "                ls_arrays_one_hot_label = []\n",
    "                ls_arrays_label = []\n",
    "                \n",
    "            if (j == file_count):\n",
    "                print ('breaking afetr processing files:' + str(j))\n",
    "                break\n",
    "                \n",
    "\n",
    "def get_a_random_batch(save_dir,batch_size,cutoff_len, file_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Gets a random training mini-batch of a set batch_size\n",
    "    Used to get randomly sampled training batches of size 128\n",
    "    \n",
    "    file_list is a list object containing the full list of training .npy cache files\n",
    "    Storing this list object in cache and sampling the mini-batch from it \n",
    "    improved performance by at least 5X during training\n",
    "    \"\"\"\n",
    "\n",
    "    ls_batch_list = [] # Stores the names of all the files randomly selected without replacement\n",
    "    ls_batch_list = random.sample(file_list,batch_size)\n",
    "        \n",
    "    ls_arrays = []\n",
    "    ls_arrays_one_hot_label = []\n",
    "    ls_arrays_label = []\n",
    "        \n",
    "    for item in ls_batch_list:\n",
    "            \n",
    "        nparr = np.load(save_dir + 'inputs/' + item)\n",
    "        if nparr is not None and nparr.shape[0] != 0:\n",
    "                \n",
    "            # Bottom-Pad  the sentences by adding 0s to the end of the sentence\n",
    "            # Or truncate the sentence upto the length\n",
    "            if (nparr.shape[0] <= cutoff_len):\n",
    "                npad = ((0,cutoff_len - nparr.shape[0]),(0,0))\n",
    "                nparr_pad = np.pad(nparr, pad_width=npad, mode='constant', constant_values=0)\n",
    "            else:\n",
    "                nparr_pad = nparr[:cutoff_len,:]\n",
    "                \n",
    "            ls_arrays.append(nparr_pad.tolist())\n",
    "                \n",
    "            one_hot_label = np.load(save_dir + 'one_hot_labels/' + 'one_hot_sentence_label_' + item.split('_')[3])\n",
    "            ls_arrays_one_hot_label.append(one_hot_label.tolist())\n",
    "                \n",
    "            label = np.load(save_dir + 'labels/' + 'sentence_label_' + item.split('_')[3])\n",
    "            ls_arrays_label.append(label.tolist())\n",
    "                \n",
    "                \n",
    "    nparr_batch = np.asarray(ls_arrays)\n",
    "    nparr_batch_one_hot_labels = np.asarray(ls_arrays_one_hot_label)\n",
    "    nparr_batch_labels = np.asarray(ls_arrays_label)\n",
    "        \n",
    "    return nparr_batch, nparr_batch_labels, nparr_batch_one_hot_labels\n",
    "\n",
    "def execute_training():\n",
    "    \n",
    "    '''\n",
    "    The main training function below. Trains the data using mini-batch gradient descent, \n",
    "    and saves checkpoints of the model \n",
    "    '''    \n",
    "    \n",
    "    print ('Starting the training process')\n",
    "    \n",
    "    num_epochs = 5000\n",
    "    mini_batch_size = 200\n",
    "    batch_size = 200\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    mini_batch_runs = 100\n",
    "    \n",
    "    # Cleans up tensorboard statistics\n",
    "    if (os.path.exists(train_tensorboard_dir)):\n",
    "        shutil.rmtree(train_tensorboard_dir)\n",
    "    os.mkdir(train_tensorboard_dir)\n",
    "    \n",
    "    if (os.path.exists(valid_tensorboard_dir)):\n",
    "        shutil.rmtree(valid_tensorboard_dir)\n",
    "    os.mkdir(valid_tensorboard_dir)\n",
    "    \n",
    "    print ('Building the graph')\n",
    "    \n",
    "    # Build graph object\n",
    "    with tf.Graph().as_default() as grap:\n",
    "    \n",
    "        input_tensor, batch_norm_train, logits, softmax_logits, conv_keep_prob,fc_keep_prob = build_graph_convnets(cutoff_len)\n",
    "        train_step, confusion_matrix,predicted_indices,cross_entropy_mean, learning_rate_input,labels,one_hot_labels,loss =  build_loss_optimizer(logits,softmax_logits,num_labels)\n",
    "        \n",
    "    print ('Kicking off the session')\n",
    "    \n",
    "    # Initiate session for execution\n",
    "    with tf.Session(graph=grap) as sess:\n",
    "        \n",
    "        saver = tf.train.Saver() # For saving checkpoints for test inference \n",
    "        \n",
    "        # Initialize\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Tensorboard writers\n",
    "        train_writer = tf.summary.FileWriter(train_tensorboard_dir,sess.graph)\n",
    "        valid_writer = tf.summary.FileWriter(valid_tensorboard_dir)\n",
    "        \n",
    "        print ('Preparing randomized validation batches')\n",
    "        valid_count = 20000 # Force stop after 20K batches\n",
    "        prepare_randomized_batches(one_hot_valid_dir,batch_size,cutoff_len,valid_count)\n",
    "        \n",
    "        xent_counter = 0\n",
    "        _guid = uuid.uuid4() # For log file creation\n",
    "        \n",
    "        # Reads the training file list into the cache\n",
    "        with open (home_dir + 'train_npy.txt','r') as d:\n",
    "            train_file_list = [line.replace('\\n','') for line in d]\n",
    "            print ('Read training file list to cache. Sample file and cache length as below:')\n",
    "            print (train_file_list[0])\n",
    "            print (len(train_file_list))\n",
    "        \n",
    "        for i in range(0,num_epochs):\n",
    "            \n",
    "            print ('Starting Training, the Epoch is:' + str(i + 1))\n",
    "            xent_summary = 0\n",
    "            \n",
    "            train_conf_matrix = None\n",
    "            \n",
    "            #for file in train_batch_files:\n",
    "            for j in range(0,mini_batch_runs): #mini batch runs per epoch\n",
    "                \n",
    "                # Gets random training batches using the train list cache\n",
    "                train_batch, train_batch_labels, train_batch_one_hot = get_a_random_batch(one_hot_train_dir,mini_batch_size,cutoff_len,train_file_list)\n",
    "                \n",
    "                # Kick off the training\n",
    "                _,los,conf_matrix,xent_mean,pi = sess.run(\n",
    "                [train_step,loss,confusion_matrix,cross_entropy_mean,predicted_indices],\n",
    "                feed_dict = {input_tensor : train_batch,\n",
    "                             labels: train_batch_labels,\n",
    "                             one_hot_labels : train_batch_one_hot,\n",
    "                             learning_rate_input : learning_rate,\n",
    "                             batch_norm_train : True,\n",
    "                             conv_keep_prob : 1.0,\n",
    "                             fc_keep_prob : 0.6\n",
    "                    })\n",
    "                \n",
    "                if (train_conf_matrix is None):\n",
    "                    train_conf_matrix = conf_matrix\n",
    "                else:\n",
    "                    train_conf_matrix += conf_matrix\n",
    "            \n",
    "                \n",
    "                xent_counter += 1\n",
    "                xent_summary += np.sum(xent_mean)\n",
    "                \n",
    "                # Write batch average loss and sum of loss to log\n",
    "                with open (log_dir + str(_guid) + '.txt','a') as eg:\n",
    "                    eg.write('The training cross entropy sum and avg at step:' + str(xent_counter) + ' is:')\n",
    "                    eg.write(str(np.sum(xent_mean)) + ';')\n",
    "                    eg.write(str(los) + '\\n')\n",
    "                \n",
    "                \n",
    "                # Write cross entropy batch sum scalar to tensorboard\n",
    "                xent_train_summary = tf.Summary(value=[tf.Summary.Value(tag=\"cross_entropy_sum\",simple_value=np.sum(xent_mean))])\n",
    "                train_writer.add_summary(xent_train_summary,xent_counter)\n",
    "                \n",
    "            #Write mini-batch run confusion matrix, average loss, and accuracy to log\n",
    "            with open (log_dir + str(_guid) + '.txt','a') as eg:\n",
    "                eg.write('\\n' + 'Training Confusion Matrix:' + '\\n' + str(train_conf_matrix))\n",
    "                true_pos = np.sum(np.diag(train_conf_matrix))\n",
    "                all_pos = np.sum(train_conf_matrix)\n",
    "                eg.write('\\n' + 'Training Accuracy is: ' + str(float(true_pos / all_pos)) + '\\n')  # Another way to get accuracy!\n",
    "                eg.write('Average cross entropy error is:' + str(xent_summary/(mini_batch_runs * mini_batch_size)) + '\\n')\n",
    "            \n",
    "            # Write epoch average loss and train accuracy to tensorboard\n",
    "            loss_train_summary = tf.Summary(value=[tf.Summary.Value(tag=\"loss_train_summary\",simple_value=xent_summary/(mini_batch_runs * mini_batch_size))])\n",
    "            acc_train_summary = tf.Summary(value=[tf.Summary.Value(tag=\"acc_train_summary\",simple_value=float(true_pos / all_pos))])\n",
    "            train_writer.add_summary(loss_train_summary,i)\n",
    "            train_writer.add_summary(acc_train_summary,i)\n",
    "            \n",
    "            # Save model every 10 epochs\n",
    "            if ((i + 1) % 10 == 0):\n",
    "                saver.save(sess=sess,save_path=checkpoint_dir + 'Char_Sent_Classification_CNN.ckpt',global_step = i )\n",
    "            \n",
    "            \n",
    "            # Shuffle validation batch and get validation batch statistics\n",
    "            os.chdir(one_hot_valid_dir + 'inputs/batch/')\n",
    "            valid_batch_files = glob.glob('*.npy')\n",
    "            random.shuffle(valid_batch_files)\n",
    "            \n",
    "            valid_conf_matrix = None\n",
    "            \n",
    "            for file in valid_batch_files:\n",
    "                \n",
    "                valid_batch = np.load(one_hot_valid_dir + 'inputs/batch/' + file)\n",
    "                valid_labels = np.load(one_hot_valid_dir + 'labels/batch/nparr_batch_labels_' + file.split('_')[2])\n",
    "                valid_one_hot_labels = np.load(one_hot_valid_dir + 'one_hot_labels/batch/nparr_batch_one_hot_labels_' + file.split('_')[2])\n",
    "                \n",
    "                val_conf_matrix = sess.run(\n",
    "                confusion_matrix,\n",
    "                feed_dict = {input_tensor : valid_batch,\n",
    "                            labels: valid_labels,\n",
    "                            one_hot_labels : valid_one_hot_labels,\n",
    "                            batch_norm_train : False,\n",
    "                            conv_keep_prob : 1.0,\n",
    "                            fc_keep_prob : 1.0\n",
    "                         \n",
    "                })\n",
    "                \n",
    "                if (valid_conf_matrix is None):\n",
    "                    valid_conf_matrix = val_conf_matrix\n",
    "                else:\n",
    "                    valid_conf_matrix += val_conf_matrix\n",
    "            \n",
    "            # Write validation confusion matrix and accuracy to tensorboard \n",
    "            with open (log_dir + str(_guid) + '.txt','a') as eg:\n",
    "                eg.write('Validation Confusion Matrix:' + '\\n' + str(valid_conf_matrix))\n",
    "                true_pos = np.sum(np.diag(valid_conf_matrix))\n",
    "                all_pos = np.sum(valid_conf_matrix)\n",
    "                eg.write('\\n' + 'Validation Accuracy is: ' + str(float(true_pos / all_pos)) + '\\n') \n",
    "\n",
    "            \n",
    "            # Write validation accuracy to tensorboard after each epoch\n",
    "            acc_valid_summary = tf.Summary(value=[tf.Summary.Value(tag=\"acc_valid_summary\",simple_value=float(true_pos / all_pos))])\n",
    "            valid_writer.add_summary(acc_valid_summary,i)\n",
    "            \n",
    "            \n",
    "            \n",
    "execute_training()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Step 8: Inference on the test set**\n",
    "<br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(cutoff_len):\n",
    "    \n",
    "    with tf.Graph().as_default() as grap:\n",
    "        input_tensor, batch_norm_train, logits, softmax_logits,conv_keep_prob,fc_keep_prob = build_graph(cutoff_len)\n",
    "        \n",
    "    with open(one_hot_test_dir + 'ytest.txt','w') as predfile:\n",
    "    \n",
    "        with tf.Session(graph=grap) as sess:\n",
    "        \n",
    "            init = tf.global_variables_initializer()\n",
    "            sess = tf.Session()\n",
    "            sess.run(init)\n",
    "        \n",
    "            saver = tf.train.Saver()\n",
    "        \n",
    "            checkpoint_file_path = '/home/ubuntu/Desktop/challenge/offline_challenge_to_send/offline_challenge_to_send/checkpoints/Char_Sent_Classification_CNN.ckpt-99'\n",
    "\n",
    "            print ('Loading checkpoint file:' + checkpoint_file_path)\n",
    "            saver.restore(sess,checkpoint_file_path)\n",
    "\n",
    "            os.chdir(one_hot_test_dir + 'inputs/')\n",
    "            test_files = glob.glob('*.npy')\n",
    "               \n",
    "            for file in test_files:\n",
    "            \n",
    "                _idx = file.split('_')[3].replace('.npy','')\n",
    "            \n",
    "            \n",
    "                nparr = np.load(one_hot_test_dir + 'inputs/' + file)\n",
    "                npad = ((0,cutoff_len - nparr.shape[0]),(0,0))\n",
    "                nparr_pad = np.pad(nparr, pad_width=npad, mode='constant', constant_values=0)\n",
    "            \n",
    "                nparr_2 = np.expand_dims(nparr_pad,axis=0)\n",
    "             \n",
    "                predictions,soft = sess.run(\n",
    "                [\n",
    "                    logits,softmax_logits\n",
    "                ],feed_dict = {input_tensor : nparr_2,\n",
    "                                batch_norm_train : False,\n",
    "                                conv_keep_prob : 1.0,\n",
    "                                fc_keep_prob : 1.0})\n",
    "                \n",
    "                predicted_index = tf.argmax(soft, axis=1)\n",
    "                \n",
    "                predfile.write(str(_idx,) + ',' + str(predicted_index.eval(session=sess)[0]) + '\\n')\n",
    "            \n",
    "        \n",
    "#inference(cutoff_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
