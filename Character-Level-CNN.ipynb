{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Desktop/tensorflow-gpu/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/Desktop/tensorflow-gpu/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import os, random , math, shutil, glob, uuid\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Initialize all the Global Variables for Train, Validation, and Test Data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Below are all the \"global\" constants used in the model (not hyperparameters or graph parameters)'''\n",
    "\n",
    "\n",
    "\n",
    "home_dir = '/home/ubuntu/Desktop/nlp_deep/amazon_data_sample/'\n",
    "one_hot_train_dir = home_dir + 'train/'\n",
    "one_hot_valid_dir = home_dir + 'valid/'\n",
    "one_hot_test_dir = home_dir + 'test/'\n",
    "train_file = home_dir + 'train_sample.csv'\n",
    "test_file = home_dir + 'test_sample.csv'\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "home_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/'\n",
    "one_hot_train_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/train/'\n",
    "one_hot_valid_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/valid/'\n",
    "one_hot_test_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/test/'\n",
    "train_file = home_dir + 'amazon_review_full_csv/train_sample.csv'\n",
    "test_file = home_dir + 'amazon_review_full_csv/test_sample.csv'\n",
    "'''\n",
    "\n",
    "\n",
    "# Stores all the checkpoint models\n",
    "checkpoint_dir = '/home/ubuntu/Desktop/nlp_deep/checkpoints/'\n",
    "#checkpoint_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/checkpoints/'\n",
    "\n",
    "\n",
    "\n",
    "# Index denotes the column placement in the character embedding matrix\n",
    "# This is used to create the one-hot character vector of sentence inputs\n",
    "'''\n",
    "one_hot_column_label = {'a':0,'b':1,'c':2,'d':3,'e':4,'f':5,'g':6,'h':7,'i':8,'j':9,'k':10,'l':11,'m':12,'n':13,'o':14,\n",
    "                        'p':15,'q':16,'r':17,'s':18,'t':19,'u':20,'v':21,'w':22,'x':23,'y':24,'z':25,'A':26,'B':27,'C':28,'D':29,'E':30,'F':31,\n",
    "                        'G':32,'H':33,'I':34,'J':35,'K':36,'L':37,'M':38,'N':39,'O':40,'P':41,'Q':42,'R':43,'S':44,'T':45,\n",
    "                        'U':46,'V':47,'W':48,'X':49,'Y':50,'Z':51,'0':52,'1':53,'2':54,'3':55,'4':56,'5':57,'6':58,'7':59,\n",
    "                        '8':60,'9':61,'-':62,',':63,';':64,'.':65,'!':66,'?':67,':':68,'\"':69,'\\'':70,'/':71,'\\\\':72,'|':73,'_':74,\n",
    "                        '@':75,'#':76,'$':77,'%':78,'^':79,'&':80,'*':81,'~':82,'`':83,'+':84,'-':85,'=':86,'<':87,'>':88,'(':89,')':90,'[':91,\n",
    "                        ']':92,'{':93,'}':94}\n",
    "'''                        \n",
    "\n",
    "\n",
    "'''\n",
    "# For 1-1 mapping of character to letter encoding\n",
    "one_hot_column_label = {'a':1,'b':2,'c':3,'d':4,'e':5,'f':6,'g':7,'h':8,'i':9,'j':10,'k':11,'l':12,'m':13,'n':14,'o':15,\n",
    "                        'p':16,'q':17,'r':18,'s':19,'t':20,'u':21,'v':22,'w':23,'x':24,'y':25,'z':26,'0':27,'1':28,'2':29,'3':30,'4':31,'5':32,'6':33,'7':34,\n",
    "                        '8':35,'9':36,'?':37,'!':38,' ':39,'$':40,'.':41}\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "one_hot_column_label = {'a':0,'b':1,'c':2,'d':3,'e':4,'f':5,'g':6,'h':7,'i':8,'j':9,'k':10,'l':11,'m':12,'n':13,'o':14,\n",
    "                        'p':15,'q':16,'r':17,'s':18,'t':19,'u':20,'v':21,'w':22,'x':23,'y':24,'z':25,'0':26,'1':27,'2':28,'3':29,'4':30,'5':31,'6':32,'7':33,\n",
    "                        '8':34,'9':35,'?':36,'!':37,'$':38,'.':39}\n",
    "\n",
    "\n",
    "num_chars_dict = 40 # including one for the unknown character, and one padding char\n",
    "\n",
    "# Number of labels to classify\n",
    "num_labels = 5\n",
    "\n",
    "# Tensorboard directories\n",
    "train_tensorboard_dir = home_dir + 'train/tensorboard/'\n",
    "valid_tensorboard_dir = home_dir + 'valid/tensorboard/'\n",
    "\n",
    "log_dir = '/home/ubuntu/Desktop/nlp_deep/Char_CNN_log/'\n",
    "#log_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/log/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a 80-20 train-validation split. Get the length of the longest sentence, which will be used to build the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest sentence is of length:1000\n",
      "The shortest sentence is of length:39\n",
      "The average length is:502.05344\n",
      "The 75th percentile is:588.0\n",
      "The 90th percentile is:779.0\n",
      "Verify Stratified Sampling\n",
      "588\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def strip_characters(sent):\n",
    "    \n",
    "    # Strip unnecessary characters from the review\n",
    "    \n",
    "    sent = sent.replace('-','')\n",
    "    sent = sent.replace(',','')\n",
    "    sent = sent.replace(';','')\n",
    "    #sent = sent.replace('.','')\n",
    "    sent = sent.replace(':','')\n",
    "    sent = sent.replace('\"','')\n",
    "    \n",
    "    sent = sent.replace('/','')\n",
    "    sent = sent.replace('\\\\','')\n",
    "    sent = sent.replace('|','')\n",
    "    sent = sent.replace('_','')\n",
    "    sent = sent.replace('@','')\n",
    "    sent = sent.replace('#','')\n",
    "    #sent = sent.replace('$','')\n",
    "    sent = sent.replace('^','')                        \n",
    "    sent = sent.replace('&','')                        \n",
    "    sent = sent.replace('*','')                        \n",
    "    sent = sent.replace('~','')                            \n",
    "    sent = sent.replace('`','')    \n",
    "    sent = sent.replace('+','')                        \n",
    "    sent = sent.replace('-','')    \n",
    "    sent = sent.replace('=','')\n",
    "    sent = sent.replace('<','')\n",
    "    sent = sent.replace('>','')                        \n",
    "    sent = sent.replace('(','')                        \n",
    "    sent = sent.replace(')','')                        \n",
    "    sent = sent.replace('[','')                        \n",
    "    sent = sent.replace(']','')                        \n",
    "    sent = sent.replace('{','')                        \n",
    "    sent = sent.replace('}','')        \n",
    "                        \n",
    "    return sent\n",
    "                        \n",
    "                        \n",
    "                        \n",
    "# Combine and label before train / valid split\n",
    "train_data = pd.read_csv(train_file)\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "# Are the datasets balanced - yes\n",
    "#print ('Checking Dataset Balance')\n",
    "#train_data.hist(column='Review')\n",
    "#test_data.hist(column='Review')\n",
    "\n",
    "\n",
    "# We get the max_len for padding and for the convolution filter definition in the model\n",
    "max_len = 0\n",
    "total_len = 0\n",
    "min_len = 999\n",
    "ls_len = []\n",
    "                        \n",
    "stripped_file_train =  home_dir + 'file_strip_train.csv'   \n",
    "stripped_file_test = home_dir + 'file_strip_test.csv'\n",
    "\n",
    "with open(stripped_file_train,'w') as strip:   \n",
    "    \n",
    "    strip.write('Review' + ',' + 'Description' + '\\n')\n",
    "\n",
    "    for index,row in train_data.iterrows():\n",
    "    \n",
    "        sent_row = strip_characters(row[2])\n",
    "        strip.write(str(row[0]) + ','  + sent_row + '\\n')    \n",
    "        \n",
    "        \n",
    "with open(stripped_file_test,'w') as strip:\n",
    "    \n",
    "    strip.write('Review' + ',' + 'Description' + '\\n')\n",
    "    \n",
    "    for index,row in test_data.iterrows():\n",
    "        \n",
    "        sent_row = strip_characters(row[2])\n",
    "        strip.write(str(row[0]) + ','  + sent_row + '\\n')    \n",
    "\n",
    "        \n",
    "\n",
    "strip_train_data = pd.read_csv(stripped_file_train)\n",
    "X_test = pd.read_csv(stripped_file_test)\n",
    "\n",
    "        \n",
    "label_data = strip_train_data.loc[:,'Review']\n",
    "all_data = strip_train_data\n",
    "all_data = all_data.append(X_test,ignore_index=True)\n",
    "\n",
    "\n",
    "for index, row in all_data.iterrows():\n",
    "    total_len += len(row[1])\n",
    "    ls_len.append(len(row[1]))\n",
    "    if (max_len < len(row[1])):\n",
    "        max_len = len(row[1])\n",
    "    if (min_len > len(row[1])):\n",
    "        min_len = len(row[1])\n",
    "            \n",
    "np_len = np.asarray(ls_len)\n",
    "\n",
    "print ('The longest sentence is of length:' + str(max_len))\n",
    "print ('The shortest sentence is of length:' + str(min_len))\n",
    "print ('The average length is:' + str(total_len/strip_train_data.shape[0]))\n",
    "print ('The 75th percentile is:' + str(np.percentile(np_len,75)))\n",
    "print ('The 90th percentile is:' + str(np.percentile(np_len,90)))\n",
    "\n",
    "all_data = None\n",
    "train_data = None\n",
    "test_data = None\n",
    "\n",
    "# Train and test split (stratified sampling, 70-30 split)\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(strip_train_data,label_data,test_size=0.2,train_size=0.8,random_state=13814,shuffle=True,stratify=label_data)\n",
    "\n",
    "# Verify that stratified sampling worked (histogram distributions must be same)\n",
    "print ('Verify Stratified Sampling')\n",
    "X_train.hist(column=\"Review\") # First histogram is training\n",
    "X_valid.hist(column=\"Review\") # Second histogram is validation\n",
    "\n",
    "\n",
    "'Some more \"global\" variables'\n",
    "cutoff_len = int(np.percentile(np_len,75)) # a graph parameter\n",
    "print(cutoff_len)\n",
    "train_count = X_train.shape[0]\n",
    "valid_count = X_valid.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert each sentence to one-hot character matrix to drive the non-static character embedding matrix creation, and store this character matrix representation in disk cache.\n",
    "\n",
    "Also store in disk cache, the label and a one-hot representation of the label, for the cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the one-hot character vector representation of sentences which will be used as the layer before a\n",
    "# character-embedding matrix\n",
    "def save_numpy_one_hot(X_dataset,unique_char_size,save_dir,one_hot_column_label):\n",
    "    \n",
    "    for index,row in X_dataset.iterrows():\n",
    "        \n",
    "        #sent = row[2]\n",
    "        sent = row[1]\n",
    "        #print (sent)\n",
    "        np_one_hot = np.zeros(shape=(len(sent),unique_char_size),dtype=np.int32)\n",
    "        #np_one_hot = np.zeros(shape=len(sent),dtype=np.int32)\n",
    "        \n",
    "        for i in range(0,len(sent)):\n",
    "            if (sent[i].lower() in one_hot_column_label):\n",
    "                np_one_hot[i][one_hot_column_label[sent[i].lower()]] = 1 # else, it remains 0 encoded.\n",
    "                #np_one_hot[i] = one_hot_column_label[sent[i].lower()] # For character embedding lookup\n",
    "            #else:\n",
    "            #    np_one_hot[i][41] = 1 # unknown character\n",
    "        #print(np_one_hot)\n",
    "            \n",
    "        # Save the one-hot encoding for each sentence \n",
    "        np.save(save_dir + 'inputs/' + 'one_hot_sentence_' + str(index) + '.npy',np_one_hot)\n",
    "        \n",
    "        #Save the label\n",
    "        lbl = int(row[0]) - 1 # Must start with 0-indexing\n",
    "        np.save(save_dir + 'labels/' +  'sentence_label_' + str(index) + '.npy',np.asarray(lbl))\n",
    "        \n",
    "        # Save a one-hot version of the label (for xentropy loss function)\n",
    "        np_one_hot_label = np.zeros(shape=(5),dtype=np.int32)\n",
    "        np_one_hot_label[lbl] = 1\n",
    "        \n",
    "        np.save(save_dir + 'one_hot_labels/' + 'one_hot_sentence_label_' + str(index) + '.npy',np_one_hot_label)\n",
    "        \n",
    "        \n",
    "def save_numpy_one_hot_test(X_dataset,unique_char_size,save_dir,one_hot_column_label):\n",
    "    \n",
    "    for index,row in X_dataset.iterrows():\n",
    "        \n",
    "        sent = row[1]\n",
    "        np_one_hot = np.zeros(shape=(len(sent),unique_char_size),dtype=np.int32)\n",
    "        #np_one_hot = np.zeros(shape=len(sent),dtype=np.int32)\n",
    "        \n",
    "        for i in range(0,len(sent)):\n",
    "            if (sent[i].lower() in one_hot_column_label):\n",
    "                np_one_hot[i][one_hot_column_label[sent[i].lower()]] = 1\n",
    "                #np_one_hot[i] = one_hot_column_label[sent[i].lower()]\n",
    "            #else:\n",
    "            #    np_one_hot[i][41] = 1\n",
    "            \n",
    "        # Save the one-hot encoding for each sentence \n",
    "        np.save(save_dir + 'inputs/' + 'one_hot_sentence_' + str(index) + '.npy',np_one_hot)\n",
    "        \n",
    "        \n",
    "def inp_create_one_hot_char_mat(unique_char_size,train_dir,valid_dir,test_dir):\n",
    "    \n",
    "    save_numpy_one_hot(X_train,unique_char_size,train_dir,one_hot_column_label)\n",
    "    save_numpy_one_hot(X_valid,unique_char_size,valid_dir,one_hot_column_label)\n",
    "    save_numpy_one_hot_test(X_test,unique_char_size,test_dir,one_hot_column_label)\n",
    "    \n",
    "\n",
    "# One  - off folder creation \n",
    "if (not os.path.exists(one_hot_train_dir)):\n",
    "        \n",
    "        print ('Setting up directories and preparing one-hot character vectors')\n",
    "        os.mkdir(one_hot_train_dir)\n",
    "        os.mkdir(one_hot_valid_dir)\n",
    "        os.mkdir(one_hot_test_dir)\n",
    "        os.mkdir(one_hot_train_dir + 'inputs/')\n",
    "        os.mkdir(one_hot_valid_dir + 'inputs/')\n",
    "        os.mkdir(one_hot_test_dir + 'inputs/')\n",
    "        os.mkdir(one_hot_train_dir + 'labels/')\n",
    "        os.mkdir(one_hot_valid_dir + 'labels/')\n",
    "        os.mkdir(one_hot_train_dir + 'one_hot_labels/')\n",
    "        os.mkdir(one_hot_valid_dir + 'one_hot_labels/')\n",
    "        os.mkdir(one_hot_train_dir + 'tensorboard/')\n",
    "        os.mkdir(one_hot_valid_dir + 'tensorboard/')\n",
    "        inp_create_one_hot_char_mat(num_chars_dict,one_hot_train_dir,one_hot_valid_dir,one_hot_test_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build the graph and define the loss and optimizer operations.\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start to build the model\n",
    "# Let's build the graph first\n",
    "def build_graph(cutoff_len):\n",
    "    \n",
    "    # Hyper Parameters with descriptions\n",
    "    num_char_dict = num_chars_dict\n",
    "    char_embedding_n_dim = 32 # The number of columns in the character embedding matrix\n",
    "    \n",
    "    first_window_size = 3 # First window applied to sentence\n",
    "    second_window_size = 4 # Second window\n",
    "    third_window_size = 5 # And so on...\n",
    "    fourth_window_size = 6\n",
    "    \n",
    "    \n",
    "    first_layer_no_filters = 256\n",
    "    second_layer_no_filters = 256\n",
    "    third_layer_no_filters = 256\n",
    "    fourth_layer_no_filters = 256\n",
    "    \n",
    "    first_fc_out = 1024\n",
    "    second_fc_out = 1024\n",
    "    third_fc_out = 1024\n",
    "    # Hyper parameters end \n",
    "    \n",
    "    reuse_flag = tf.AUTO_REUSE\n",
    "    \n",
    "    # Build the graph\n",
    "    with tf.variable_scope('layer_one_char_embedding',reuse=reuse_flag):\n",
    "        \n",
    "        batch_norm_train = tf.placeholder(name=\"batch_norm_train\", dtype=tf.bool)\n",
    "        \n",
    "        '''This layer defines the character embedding matrix, and returns character embedding for batches of sentences'''\n",
    "        \n",
    "        # The input_tensor is of shape [None, cutoff_len, num_char_dict] where cutoff_len is the length limit for the 2-D embedding matrix\n",
    "        #input_tensor = tf.placeholder(dtype=tf.float32,shape=[None,cutoff_len,num_char_dict],name=\"input_tensor\") \n",
    "        input_tensor = tf.placeholder(dtype=tf.int32,shape=[None,cutoff_len],name=\"input_tensor\") \n",
    "        \n",
    "        # Initializer for embedding matrix\n",
    "        l1w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        # The character embedding matrix declared below. The 2nd rank is the number of dimensions representing each character\n",
    "        char_embedding_mat = tf.get_variable(name=\"char_embedding\",dtype=tf.float32,shape=[num_char_dict,char_embedding_n_dim],initializer=l1w_init)\n",
    "        \n",
    "        input_tensor_2D = tf.reshape(input_tensor,shape=[-1,num_char_dict]) \n",
    "        #l1_char_embedding = tf.matmul(input_tensor_2D,char_embedding_mat)\n",
    "        \n",
    "        l1_char_embedding = tf.nn.embedding_lookup(char_embedding_mat,input_tensor,name=\"l1_char_embedding\")\n",
    "        print(l1_char_embedding)\n",
    "        \n",
    "        # reshape char embedding matrix to 3D matrix for the temporal 1D convolution operation\n",
    "        input_tensor_with_embed = tf.reshape(l1_char_embedding,shape=[-1,cutoff_len,char_embedding_n_dim])\n",
    "        \n",
    "        # Some keep probabilities for adding dropout\n",
    "        conv_keep_prob = tf.placeholder(dtype=tf.float32,shape=[],name=\"conv_dropout_prob\")\n",
    "        fc_keep_prob = tf.placeholder(dtype=tf.float32,shape=[],name=\"fc_dropout_prob\")\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('layer_two_kernels',reuse=reuse_flag):\n",
    "        \n",
    "        l2w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        \n",
    "        # 1-D convolution layer\n",
    "        first_conv_layer = tf.layers.conv1d(inputs = input_tensor_with_embed, filters = first_layer_no_filters,kernel_size = first_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"first_conv_layer\")\n",
    "        # Batch normalization\n",
    "        first_conv_batch = tf.contrib.layers.batch_norm(first_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        \n",
    "        # Pooling Layer\n",
    "        #first_sumpool_layer = tf.reduce_sum(first_conv_batch, axis=1,keepdims=False) # Global sumpool 1-D\n",
    "        first_maxpool_layer = tf.reduce_max(first_conv_batch, axis=1,keepdims=False) # Global maxpool 1-D\n",
    "        \n",
    "        # Dropout\n",
    "        first_layer = tf.nn.dropout(first_maxpool_layer,keep_prob = conv_keep_prob )\n",
    "        \n",
    "        \n",
    "        # Repeat for second stack\n",
    "        second_conv_layer = tf.layers.conv1d(inputs = input_tensor_with_embed, filters = second_layer_no_filters,kernel_size = second_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(), name=\"second_conv_layer\")\n",
    "        second_conv_batch = tf.contrib.layers.batch_norm(second_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        #second_sumpool_layer = tf.reduce_sum(second_conv_batch, axis=1,keepdims=False) \n",
    "        second_maxpool_layer = tf.reduce_max(second_conv_batch, axis=1,keepdims=False) \n",
    "        \n",
    "        second_layer = tf.nn.dropout(second_maxpool_layer,keep_prob = conv_keep_prob )\n",
    "        \n",
    "        \n",
    "        #...and third stack...\n",
    "        third_conv_layer = tf.layers.conv1d(inputs = input_tensor_with_embed, filters = third_layer_no_filters,kernel_size = third_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(), name=\"third_conv_layer\")\n",
    "        third_conv_batch = tf.contrib.layers.batch_norm(third_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        #third_sumpool_layer = tf.reduce_sum(third_conv_batch, axis=1,keepdims=False) \n",
    "        third_maxpool_layer = tf.reduce_max(third_conv_batch, axis=1,keepdims=False) \n",
    "        \n",
    "        \n",
    "        third_layer = tf.nn.dropout(third_maxpool_layer,keep_prob = conv_keep_prob )\n",
    "        \n",
    "        \n",
    "        #..and fourth stack..\n",
    "        fourth_conv_layer = tf.layers.conv1d(inputs = input_tensor_with_embed, filters = fourth_layer_no_filters,kernel_size = fourth_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(), name=\"fourth_conv_layer\")\n",
    "        fourth_conv_batch = tf.contrib.layers.batch_norm(fourth_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        #fourth_sumpool_layer = tf.reduce_sum(fourth_conv_batch, axis=1,keepdims=False) # Sums across the 'length' \n",
    "        fourth_maxpool_layer = tf.reduce_sum(fourth_conv_batch, axis=1,keepdims=False) # Sums across the 'length' \n",
    "        \n",
    "        fourth_layer = tf.nn.dropout(fourth_maxpool_layer,keep_prob = conv_keep_prob )\n",
    "        \n",
    "        all_conv = tf.concat(axis=1,values=[first_layer,second_layer,third_layer,fourth_layer],\n",
    "                                            #fifth_conv_dropout,sixth_conv_dropout,seventh_conv_dropout],\n",
    "                                            name=\"all_conv\")\n",
    "        \n",
    "        print ('Stacked Convolution Tensor as below')\n",
    "        print(all_conv)\n",
    "        \n",
    "        \n",
    "    # Now pass it through three FC layers\n",
    "    with tf.variable_scope('layer_three_fc',reuse=reuse_flag):\n",
    "       \n",
    "        l3w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        #l3b_init = tf.random_normal_initializer(mean=0, stddev=0.1, dtype=tf.float32)\n",
    "        l3b_init = tf.zeros_initializer()\n",
    "        \n",
    "        first_fc = tf.contrib.layers.fully_connected(inputs=all_conv,num_outputs=first_fc_out,activation_fn = tf.nn.relu,\n",
    "                                                    weights_initializer = l3w_init,biases_initializer = l3b_init,scope=\"first_fc\")\n",
    "        \n",
    "        first_fc_batch = tf.contrib.layers.batch_norm(first_fc,center = True, scale=False,is_training=batch_norm_train)\n",
    "        first_fc_final = tf.nn.dropout(first_fc_batch,keep_prob = fc_keep_prob) \n",
    "        \n",
    "    '''\n",
    "    with tf.variable_scope('layer_four_fc',reuse=reuse_flag):\n",
    "        l4w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        #l4b_init = tf.random_normal_initializer(mean=0, stddev=0.1, dtype=tf.float32)\n",
    "        l4b_init = tf.zeros_initializer()\n",
    "        \n",
    "        second_fc = tf.contrib.layers.fully_connected(inputs=first_fc_final,num_outputs=second_fc_out,activation_fn = tf.nn.relu,\n",
    "                                                    weights_initializer = l4w_init,biases_initializer = l4b_init,scope=\"second_fc\")\n",
    "        second_fc_batch = tf.contrib.layers.batch_norm(second_fc,center = True, scale=False,is_training=batch_norm_train)\n",
    "        second_fc_final = tf.nn.dropout(second_fc_batch,keep_prob = fc_keep_prob)\n",
    "    '''\n",
    "    # Third FC layer and softmax\n",
    "    with tf.variable_scope('layer_five_fc',reuse=reuse_flag):\n",
    "        l5w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        #l5b_init = tf.random_normal_initializer(mean=0, stddev=0.1, dtype=tf.float32)\n",
    "        l5b_init = tf.zeros_initializer()\n",
    "        \n",
    "        third_fc = tf.contrib.layers.fully_connected(inputs = first_fc_final,num_outputs=num_labels,weights_initializer=l5w_init,\n",
    "                                                    biases_initializer = l5b_init,activation_fn=tf.nn.relu,scope=\"third_fc\")\n",
    "        \n",
    "        third_fc_batch = tf.contrib.layers.batch_norm(third_fc,center=True,scale=False,is_training=batch_norm_train)\n",
    "        third_fc_final = tf.nn.dropout(third_fc_batch,keep_prob = fc_keep_prob)\n",
    "    \n",
    "    with tf.variable_scope('layer_six_softmax',reuse=reuse_flag):\n",
    "        softmax_logits = tf.nn.softmax(logits=third_fc_final,name=\"final_logits_softmax\")\n",
    "        \n",
    "    return input_tensor, batch_norm_train, third_fc_final,softmax_logits, conv_keep_prob,fc_keep_prob\n",
    "        \n",
    "\n",
    "\n",
    "def build_loss_optimizer(logits,softmax_logits,num_labels):\n",
    "    \n",
    "    ''' Loss and Optimizer builder, with Softmax Cross Entropy loss'''\n",
    "    \n",
    "    with tf.variable_scope('cross_entropy'):\n",
    "        \n",
    "        learning_rate_input = tf.placeholder(\n",
    "            tf.float32, [], name='learning_rate_input')\n",
    "        #one_hot_labels = tf.placeholder(\n",
    "        #    name=\"one_hot_labels\",shape=[None,num_labels],dtype=tf.int32)\n",
    "        labels = tf.placeholder(\n",
    "            name=\"labels\",shape=[None],dtype=tf.int64)\n",
    "        \n",
    "        #cross_entropy_mean = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels,logits=softmax_logits)\n",
    "        cross_entropy_mean = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logits,name=\"cross_entropy_mean_sparse\")\n",
    "        \n",
    "        'Toggle between optimizers..'\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_input)\n",
    "        #optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate_input)\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate_input)\n",
    "        \n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # For batch normalization ops update\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            train_step = optimizer.minimize(cross_entropy_mean)\n",
    "        \n",
    "        predicted_indices = tf.argmax(softmax_logits, 1, name=\"predicted_indices\")\n",
    "        correct_prediction = tf.equal(predicted_indices, labels, name='correct_prediction') # Labels are 0-indexed\n",
    "        confusion_matrix = tf.confusion_matrix(\n",
    "            labels, predicted_indices, num_classes=num_labels, name=\"confusion_matrix\")\n",
    "        \n",
    "        # Because the accuracy will be calculated across batch splits, this facilitates resetting the metrics inter-epochs\n",
    "        with tf.variable_scope('streaming_ops'):\n",
    "            accuracy,update_op_acc = tf.metrics.accuracy(labels,predicted_indices)\n",
    "            \n",
    "        metrics_vars = tf.contrib.framework.get_variables('cross_entropy/streaming_ops',collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "        reset_metrics_vars = tf.variables_initializer(metrics_vars) # Running sess.run will reset the streaming metrics within epochs\n",
    "        \n",
    "        # For tensorboard \n",
    "        xent_mean_scalar= tf.summary.scalar('cross_entropy_mean',tf.reduce_mean(cross_entropy_mean))\n",
    "        acc_scalar = tf.summary.scalar('accuracy',accuracy)\n",
    "        \n",
    "        return train_step, confusion_matrix,predicted_indices,cross_entropy_mean, learning_rate_input,labels, reset_metrics_vars, accuracy,update_op_acc, acc_scalar,xent_mean_scalar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative model based off\n",
    "# https://arxiv.org/pdf/1502.01710v5.pdf\n",
    "\n",
    "def build_graph_convnets(cutoff_len):\n",
    "    \n",
    "    num_chars = num_chars_dict\n",
    "    \n",
    "    char_embedding_n_dim = 32\n",
    "    \n",
    "    first_window_size = 7 # First window applied to sentence\n",
    "    second_window_size = 7 # Second window\n",
    "    third_window_size = 3 # And so on...\n",
    "    fourth_window_size = 3\n",
    "    fifth_window_size = 3\n",
    "    sixth_window_size = 3\n",
    "    \n",
    "    # No third, fourth, or fifth pool size, copying the paper's approach \n",
    "    first_pool_size = 3\n",
    "    second_pool_size = 3\n",
    "    sixth_pool_size = 3\n",
    "    \n",
    "    # One channel output for each of the six conv layers\n",
    "    first_layer_no_filters = 256 \n",
    "    second_layer_no_filters = 256\n",
    "    third_layer_no_filters = 256\n",
    "    fourth_layer_no_filters = 256\n",
    "    fifth_layer_no_filters = 256\n",
    "    sixth_layer_no_filters = 256\n",
    "    \n",
    "    first_fc_out = 1024\n",
    "    second_fc_out = 1024\n",
    "    third_fc_out = num_labels \n",
    "    # Hyper parameters end \n",
    "    \n",
    "    reuse_flag = tf.AUTO_REUSE\n",
    "    \n",
    "    # Layer Zero is just to define the input tensors and other 'global' tensors such as dropout probabilities\n",
    "    with tf.variable_scope('layer_zero_convnet',reuse=reuse_flag):\n",
    "        \n",
    "         # This layer defines the character embedding matrix, and returns character embedding for batches of sentences\n",
    "        # tensor: input_tensor is of shape [None, 26] where the first dimension is the batch_size * max_len for that batch\n",
    "        input_tensor = tf.placeholder(dtype=tf.float32,shape=[None,cutoff_len,num_chars_dict],name=\"input_tensor\") # Unroll the first 2 dimensions at the numpy array before feeding\n",
    "        #l0w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        #char_embedding_mat = tf.get_variable(name=\"char_embedding\",dtype=tf.float32,shape=[num_chars_dict,char_embedding_n_dim],initializer=l0w_init)\n",
    "        \n",
    "        # This gets the char embeddings for the batch\n",
    "        # Shape is [batch_size * cutoff_len, char_embedding_n_dim]\n",
    "        #input_tensor_2D = tf.reshape(input_tensor,shape=[-1,num_chars_dict]) # Reshaped to 2D with batch_size * cutoff_len as 1st dim\n",
    "        #l1_char_embedding = tf.matmul(input_tensor_2D,char_embedding_mat)\n",
    "        \n",
    "        #l1_char_embedding = tf.nn.embedding_lookup(char_embedding_mat,input_tensor,name=\"l1_char_embedding\")\n",
    "        #print(l1_char_embedding)\n",
    "        \n",
    "        # reshape input tensor to 3D matrix for the temporal 1D convolution operation\n",
    "        #input_tensor_with_embed = tf.reshape(l1_char_embedding,shape=[-1,cutoff_len,char_embedding_n_dim])\n",
    "        #input_tensor_with_embed = l1_char_embedding\n",
    "        #print(input_tensor_with_embed)\n",
    "        \n",
    "        # The input tensor is already one-hot encoded , with a character dictionary size of 26 characters\n",
    "        #input_tensor = tf.placeholder(dtype=tf.float32,shape=[None,cutoff_len,26],name=\"input_tensor\")\n",
    "        conv_keep_prob = tf.placeholder(dtype=tf.float32,shape=[],name=\"conv_dropout_prob\")\n",
    "        fc_keep_prob = tf.placeholder(dtype=tf.float32,shape=[],name=\"fc_dropout_prob\")\n",
    "        \n",
    "        # Toggle application of either population or sample mean and variance during batch normalization\n",
    "        # is False during validation / testing to allow population statistics to apply\n",
    "        batch_norm_train = tf.placeholder(name=\"batch_norm_train\", dtype=tf.bool)\n",
    "        \n",
    "        #first_embed_batch = tf.contrib.layers.batch_norm(input_tensor_with_embed,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #first_embed_relu = tf.nn.leaky_relu(first_embed_batch,name=\"first_embed_relu\")\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('layer_one_convnet',reuse=reuse_flag):\n",
    "        \n",
    "        #l1w_init = tf.random_normal_initializer(mean=0,stddev=0.02,dtype=tf.float32)\n",
    "        l1w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        \n",
    "        first_conv_layer = tf.layers.conv1d(inputs = input_tensor, filters = first_layer_no_filters,kernel_size = first_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l1w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"first_conv_layer\")\n",
    "        #first_conv_batch = tf.contrib.layers.batch_norm(first_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #first_conv_relu = tf.nn.leaky_relu(first_conv_batch,name=\"first_conv_relu\")\n",
    "        first_maxpool_layer = tf.layers.max_pooling1d(inputs = first_conv_layer, pool_size = first_pool_size, strides = first_pool_size,\n",
    "                                                      padding='valid', data_format='channels_last',name=\"first_maxpool_layer\")\n",
    "\n",
    "        \n",
    "    with tf.variable_scope('layer_two_convnet',reuse=reuse_flag):\n",
    "        \n",
    "        #l2w_init = tf.random_normal_initializer(mean=0,stddev=0.02,dtype=tf.float32)\n",
    "        l2w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        second_conv_layer = tf.layers.conv1d(inputs = first_maxpool_layer, filters = second_layer_no_filters,kernel_size = second_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"second_conv_layer\")\n",
    "        #second_conv_batch = tf.contrib.layers.batch_norm(second_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #second_conv_relu = tf.nn.leaky_relu(second_conv_batch,name=\"second_conv_relu\")\n",
    "        second_maxpool_layer = tf.layers.max_pooling1d(inputs = second_conv_layer, pool_size = second_pool_size, strides = second_pool_size,\n",
    "                                                      padding='valid', data_format='channels_last',name=\"second_maxpool_layer\")\n",
    "\n",
    "\n",
    "        \n",
    "    with tf.variable_scope('layer_three_convnet',reuse=reuse_flag):\n",
    "        #l3w_init = tf.random_normal_initializer(mean=0,stddev=0.02,dtype=tf.float32)\n",
    "        l3w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        third_conv_layer = tf.layers.conv1d(inputs = second_maxpool_layer, filters = third_layer_no_filters,kernel_size = third_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l3w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"third_conv_layer\")\n",
    "        #third_conv_batch = tf.contrib.layers.batch_norm(third_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #third_conv_relu = tf.nn.leaky_relu(third_conv_batch,name=\"third_conv_relu\")\n",
    "        \n",
    "    with tf.variable_scope('layer_four_convnet',reuse=reuse_flag):\n",
    "        #l4w_init = tf.random_normal_initializer(mean=0,stddev=0.02,dtype=tf.float32)\n",
    "        l4w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        fourth_conv_layer = tf.layers.conv1d(inputs = third_conv_layer, filters = fourth_layer_no_filters,kernel_size = fourth_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l4w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"fourth_conv_layer\")\n",
    "        #fourth_conv_batch = tf.contrib.layers.batch_norm(fourth_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #fourth_conv_relu = tf.nn.leaky_relu(fourth_conv_batch,name=\"fourth_conv_relu\")\n",
    "        \n",
    "    with tf.variable_scope('layer_five_convnet',reuse=reuse_flag):\n",
    "        #l5w_init = tf.random_normal_initializer(mean=0,stddev=0.02,dtype=tf.float32)\n",
    "        l5w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        fifth_conv_layer = tf.layers.conv1d(inputs = fourth_conv_layer, filters = fifth_layer_no_filters,kernel_size = fifth_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l5w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"fifth_conv_layer\")\n",
    "        #fifth_conv_batch = tf.contrib.layers.batch_norm(fifth_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #fifth_conv_relu = tf.nn.leaky_relu(fifth_conv_batch,name=\"fifth_conv_relu\")\n",
    "        \n",
    "    with tf.variable_scope('layer_six_convnet',reuse=reuse_flag):\n",
    "        #l6w_init = tf.random_normal_initializer(mean=0,stddev=0.02,dtype=tf.float32)\n",
    "        l6w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        sixth_conv_layer = tf.layers.conv1d(inputs = fifth_conv_layer, filters = sixth_layer_no_filters,kernel_size = sixth_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l6w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"sixth_conv_layer\")\n",
    "        #sixth_conv_batch = tf.contrib.layers.batch_norm(sixth_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #sixth_conv_relu = tf.nn.leaky_relu(sixth_conv_batch,name=\"sixth_conv_relu\")\n",
    "        sixth_maxpool_layer = tf.layers.max_pooling1d(inputs = sixth_conv_layer, pool_size = sixth_pool_size, strides = sixth_pool_size,\n",
    "                                                      padding='valid', data_format='channels_last',name=\"sixth_maxpool_layer\")\n",
    "        \n",
    "        sixth_reshaped_layer = tf.reshape(sixth_maxpool_layer,shape=[-1,sixth_maxpool_layer.get_shape()[1] * sixth_maxpool_layer.get_shape()[2]])\n",
    "        \n",
    "        print (sixth_reshaped_layer)\n",
    "        \n",
    "    \n",
    "    with tf.variable_scope('layer_seven_convnet',reuse=reuse_flag):\n",
    "        #l7w_init = tf.random_normal_initializer(mean=0,stddev=0.02,dtype=tf.float32)\n",
    "        l7w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        l7b_init = tf.zeros_initializer()\n",
    "        \n",
    "        seventh_fc = tf.contrib.layers.fully_connected(inputs=sixth_reshaped_layer,num_outputs=first_fc_out,activation_fn = tf.nn.relu,\n",
    "                                                       weights_initializer = l7w_init,biases_initializer = l7b_init,scope=\"seventh_fc\")\n",
    "        #seventh_fc_batch = tf.contrib.layers.batch_norm(seventh_fc,center = True, scale=True,is_training=batch_norm_train)\n",
    "        #seventh_fc_relu = tf.nn.leaky_relu(seventh_fc_batch,name=\"seventh_fc_relu\")\n",
    "        seventh_fc_final = tf.nn.dropout(seventh_fc,keep_prob = fc_keep_prob)\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('layer_eight_convnet',reuse=reuse_flag):\n",
    "        #l8w_init = tf.random_normal_initializer(mean=0,stddev=0.02,dtype=tf.float32)\n",
    "        l8w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        l8b_init = tf.zeros_initializer()\n",
    "        \n",
    "        eigth_fc = tf.contrib.layers.fully_connected(inputs=seventh_fc_final,num_outputs=second_fc_out,activation_fn = tf.nn.relu,\n",
    "                                                       weights_initializer = l8w_init,biases_initializer = l8b_init,scope=\"eigth_fc\")\n",
    "        #eigth_fc_batch = tf.contrib.layers.batch_norm(eigth_fc,center = True, scale=True,is_training=batch_norm_train)\n",
    "        #eigth_fc_relu = tf.nn.leaky_relu(eigth_fc_batch,name=\"eigth_fc_relu\")\n",
    "        eigth_fc_final = tf.nn.dropout(eigth_fc,keep_prob = fc_keep_prob)\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('layer_ninth_convnet',reuse=reuse_flag):\n",
    "        #l9w_init = tf.random_normal_initializer(mean=0,stddev=0.02,dtype=tf.float32) \n",
    "        l9w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        l9b_init = tf.zeros_initializer()\n",
    "        \n",
    "        ninth_fc = tf.contrib.layers.fully_connected(inputs=eigth_fc_final,num_outputs=num_labels,activation_fn = None,\n",
    "                                                     weights_initializer = l9w_init,biases_initializer = l9b_init,scope=\"ninth_fc\")\n",
    "        \n",
    "\n",
    "        ninth_final_softmax = tf.nn.softmax(ninth_fc,name=\"ninth_final_softmax\")\n",
    "        \n",
    "        return input_tensor, batch_norm_train, ninth_fc,ninth_final_softmax, conv_keep_prob,fc_keep_prob\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the graph is built, randomly batch the inputs from disk cache, and feed them to the graph to train a model.\n",
    "\n",
    "Training takes between 30-45 min on a Tesla M60 GPU from an AWS g3 EC2 instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the training process\n",
      "Building the graph\n",
      "WARNING:tensorflow:From /home/ubuntu/Desktop/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "Tensor(\"layer_six_convnet/Reshape:0\", shape=(?, 4608), dtype=float32)\n",
      "Kicking off the session\n",
      "Preparing randomized validation batches\n",
      "The input directory is:/home/ubuntu/Desktop/nlp_deep/amazon_data_sample/valid/inputs/\n",
      "Starting Training, the Epoch is:1\n",
      "Get a random training batch:1\n",
      "The training cross entropy at step:1\n",
      "1.6110722\n",
      "Get a random training batch:2\n",
      "The training cross entropy at step:2\n",
      "2469.431\n",
      "Get a random training batch:3\n",
      "The training cross entropy at step:3\n",
      "4.6316423\n",
      "Get a random training batch:4\n",
      "The training cross entropy at step:4\n",
      "2.7923472\n",
      "Get a random training batch:5\n",
      "The training cross entropy at step:5\n",
      "1.6192639\n",
      "Get a random training batch:6\n",
      "The training cross entropy at step:6\n",
      "1.7087646\n",
      "Get a random training batch:7\n",
      "The training cross entropy at step:7\n",
      "1.7850251\n",
      "Get a random training batch:8\n",
      "The training cross entropy at step:8\n",
      "1.6560345\n",
      "Get a random training batch:9\n",
      "The training cross entropy at step:9\n",
      "1.6167412\n",
      "Get a random training batch:10\n",
      "The training cross entropy at step:10\n",
      "1.6103845\n",
      "Get a random training batch:11\n",
      "The training cross entropy at step:11\n",
      "1.615829\n",
      "Get a random training batch:12\n",
      "The training cross entropy at step:12\n",
      "1.6076753\n",
      "Get a random training batch:13\n",
      "The training cross entropy at step:13\n",
      "1.6233416\n",
      "Get a random training batch:14\n",
      "The training cross entropy at step:14\n",
      "1.6201574\n",
      "Get a random training batch:15\n",
      "The training cross entropy at step:15\n",
      "1.6190526\n",
      "Get a random training batch:16\n",
      "The training cross entropy at step:16\n",
      "1.6197915\n",
      "Get a random training batch:17\n",
      "The training cross entropy at step:17\n",
      "1.6108885\n",
      "Get a random training batch:18\n",
      "The training cross entropy at step:18\n",
      "1.6122686\n",
      "Get a random training batch:19\n",
      "The training cross entropy at step:19\n",
      "1.6089431\n",
      "Get a random training batch:20\n",
      "The training cross entropy at step:20\n",
      "1.6124852\n",
      "Training Epoch 1 is complete. Running Validation Stats..\n",
      "Starting Training, the Epoch is:2\n",
      "Get a random training batch:1\n",
      "The training cross entropy at step:21\n",
      "1.6101898\n",
      "Get a random training batch:2\n",
      "The training cross entropy at step:22\n",
      "1.6108873\n",
      "Get a random training batch:3\n",
      "The training cross entropy at step:23\n",
      "1.6159631\n",
      "Get a random training batch:4\n",
      "The training cross entropy at step:24\n",
      "1.6094582\n",
      "Get a random training batch:5\n",
      "The training cross entropy at step:25\n",
      "1.612457\n",
      "Get a random training batch:6\n",
      "The training cross entropy at step:26\n",
      "1.614617\n",
      "Get a random training batch:7\n",
      "The training cross entropy at step:27\n",
      "1.6089331\n",
      "Get a random training batch:8\n",
      "The training cross entropy at step:28\n",
      "1.6092336\n",
      "Get a random training batch:9\n",
      "The training cross entropy at step:29\n",
      "1.6108161\n",
      "Get a random training batch:10\n",
      "The training cross entropy at step:30\n",
      "1.6075125\n",
      "Get a random training batch:11\n",
      "The training cross entropy at step:31\n",
      "1.6092637\n",
      "Get a random training batch:12\n",
      "The training cross entropy at step:32\n",
      "1.6101389\n",
      "Get a random training batch:13\n",
      "The training cross entropy at step:33\n",
      "1.6136444\n",
      "Get a random training batch:14\n",
      "The training cross entropy at step:34\n",
      "1.6094759\n",
      "Get a random training batch:15\n",
      "The training cross entropy at step:35\n",
      "1.6106014\n",
      "Get a random training batch:16\n",
      "The training cross entropy at step:36\n",
      "1.6083207\n",
      "Get a random training batch:17\n",
      "The training cross entropy at step:37\n",
      "1.6090931\n",
      "Get a random training batch:18\n",
      "The training cross entropy at step:38\n",
      "1.6068556\n",
      "Get a random training batch:19\n",
      "The training cross entropy at step:39\n",
      "1.6098021\n",
      "Get a random training batch:20\n",
      "The training cross entropy at step:40\n",
      "1.6093357\n",
      "Training Epoch 2 is complete. Running Validation Stats..\n",
      "Starting Training, the Epoch is:3\n",
      "Get a random training batch:1\n",
      "The training cross entropy at step:41\n",
      "1.6096416\n",
      "Get a random training batch:2\n",
      "The training cross entropy at step:42\n",
      "1.6063937\n",
      "Get a random training batch:3\n",
      "The training cross entropy at step:43\n",
      "1.6067756\n",
      "Get a random training batch:4\n",
      "The training cross entropy at step:44\n",
      "1.611814\n",
      "Get a random training batch:5\n",
      "The training cross entropy at step:45\n",
      "1.6101899\n",
      "Get a random training batch:6\n",
      "The training cross entropy at step:46\n",
      "1.6136578\n",
      "Get a random training batch:7\n",
      "The training cross entropy at step:47\n",
      "1.6091975\n",
      "Get a random training batch:8\n",
      "The training cross entropy at step:48\n",
      "1.61039\n",
      "Get a random training batch:9\n",
      "The training cross entropy at step:49\n",
      "1.6096797\n",
      "Get a random training batch:10\n",
      "The training cross entropy at step:50\n",
      "1.606789\n",
      "Get a random training batch:11\n",
      "The training cross entropy at step:51\n",
      "1.612329\n",
      "Get a random training batch:12\n",
      "The training cross entropy at step:52\n",
      "1.6104658\n",
      "Get a random training batch:13\n",
      "The training cross entropy at step:53\n",
      "1.6108571\n",
      "Get a random training batch:14\n",
      "The training cross entropy at step:54\n",
      "1.6110805\n",
      "Get a random training batch:15\n",
      "The training cross entropy at step:55\n",
      "1.6080973\n",
      "Get a random training batch:16\n",
      "The training cross entropy at step:56\n",
      "1.6096089\n",
      "Get a random training batch:17\n",
      "The training cross entropy at step:57\n",
      "1.612223\n",
      "Get a random training batch:18\n",
      "The training cross entropy at step:58\n",
      "1.6086956\n",
      "Get a random training batch:19\n",
      "The training cross entropy at step:59\n",
      "1.6083381\n",
      "Get a random training batch:20\n",
      "The training cross entropy at step:60\n",
      "1.607045\n",
      "Training Epoch 3 is complete. Running Validation Stats..\n",
      "Starting Training, the Epoch is:4\n",
      "Get a random training batch:1\n",
      "The training cross entropy at step:61\n",
      "1.611142\n",
      "Get a random training batch:2\n",
      "The training cross entropy at step:62\n",
      "1.61174\n",
      "Get a random training batch:3\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare batches in the cache\n",
    "# Used for training batch preparation\n",
    "def prepare_randomized_batches(save_dir,batch_size,cutoff_len,file_count):\n",
    "    \n",
    "    # Clean up for rerunning\n",
    "    if (os.path.exists(save_dir + 'inputs/batch/')):\n",
    "        shutil.rmtree(save_dir + 'inputs/batch/')\n",
    "    os.mkdir(save_dir + 'inputs/batch/')\n",
    "        \n",
    "    if (os.path.exists(save_dir + 'labels/batch/')):\n",
    "        shutil.rmtree(save_dir + 'labels/batch/')\n",
    "    os.mkdir(save_dir + 'labels/batch/')\n",
    "        \n",
    "    if (os.path.exists(save_dir + 'one_hot_labels/batch/')):\n",
    "        shutil.rmtree(save_dir + 'one_hot_labels/batch/')\n",
    "    os.mkdir(save_dir + 'one_hot_labels/batch/')\n",
    "    \n",
    "    # For batch preparation\n",
    "    ls_arrays = []\n",
    "    ls_arrays_one_hot_label = []\n",
    "    ls_arrays_label = []\n",
    "    \n",
    "    # A counter\n",
    "    j = 0\n",
    "    \n",
    "    # Randomize the Batch preparation\n",
    "    os.chdir(save_dir + 'inputs/')\n",
    "    \n",
    "    print ('The input directory is:' + save_dir + 'inputs/')\n",
    "    \n",
    "    file_list = glob.glob('*.npy')\n",
    "    random.shuffle(file_list)\n",
    "    \n",
    "    \n",
    "    # Prepare the batches\n",
    "    for file_item in file_list:\n",
    "    \n",
    "        nparr = np.load(save_dir + 'inputs/' + file_item)\n",
    "                \n",
    "        # Make sure it isnt an empty file\n",
    "        if nparr is not None and nparr.shape[0] != 0:\n",
    "                    \n",
    "            # Bottom-Pad  the sentences by adding 0s to the end of the sentence\n",
    "            # Or Truncate sentences that are longer than the cutoff length\n",
    "            \n",
    "            if (nparr.shape[0] <= cutoff_len):\n",
    "                npad = ((0,cutoff_len - nparr.shape[0]),(0,0))\n",
    "                #npad = (0,cutoff_len - nparr.shape[0])\n",
    "                nparr_pad = np.pad(nparr, pad_width=npad, mode='constant', constant_values=0)\n",
    "            else: # Truncate to the cutoff length\n",
    "                #nparr_pad = nparr[:cutoff_len,:]\n",
    "                nparr_pad = nparr[:cutoff_len,:]\n",
    "                \n",
    "            \n",
    "            ls_arrays.append(nparr_pad.tolist())\n",
    "            \n",
    "            one_hot_label = np.load(save_dir + 'one_hot_labels/' + 'one_hot_sentence_label_' + file_item.split('_')[3])\n",
    "            ls_arrays_one_hot_label.append(one_hot_label)\n",
    "            \n",
    "            label = np.load(save_dir + 'labels/' + 'sentence_label_' + file_item.split('_')[3])\n",
    "            ls_arrays_label.append(label)\n",
    "            \n",
    "            j += 1\n",
    "                \n",
    "            # Save batches of the files every batch_size step\n",
    "            if (j % batch_size == 0 or j == file_count):\n",
    "                    \n",
    "                nparr_batch = np.asarray(ls_arrays)\n",
    "                nparr_batch_one_hot_labels = np.asarray(ls_arrays_one_hot_label)\n",
    "                nparr_batch_labels = np.asarray(ls_arrays_label)\n",
    "                    \n",
    "                np.save(save_dir + 'inputs/batch/nparr_batch_' + str(j) + '.npy',nparr_batch)\n",
    "                np.save(save_dir + 'labels/batch/nparr_batch_labels_' + str(j) +  '.npy',nparr_batch_labels)\n",
    "                np.save(save_dir + 'one_hot_labels/batch/nparr_batch_one_hot_labels_' + str(j) +'.npy',nparr_batch_one_hot_labels)\n",
    "                    \n",
    "                ls_arrays = []\n",
    "                ls_arrays_one_hot_label = []\n",
    "                ls_arrays_label = []\n",
    "                \n",
    "                \n",
    "# Gets a random batch of a specified size\n",
    "# This function is not used in the final model\n",
    "# But can be used for mini-batch Gradient Descent \n",
    "def get_a_random_batch(save_dir,batch_size,cutoff_len):\n",
    "\n",
    "        ls_batch_list = [] # Stores the names of all the files randomly selected without replacement\n",
    "    \n",
    "        i = 0\n",
    "        while (i < batch_size):\n",
    "     \n",
    "            rand_choice = random.choice(os.listdir(save_dir + 'inputs/'))\n",
    "            \n",
    "            if rand_choice not in ls_batch_list:\n",
    "                ls_batch_list.append(rand_choice)\n",
    "                i = i + 1\n",
    "        \n",
    "        ls_arrays = []\n",
    "        ls_arrays_one_hot_label = []\n",
    "        ls_arrays_label = []\n",
    "        \n",
    "        for item in ls_batch_list:\n",
    "            \n",
    "            nparr = np.load(save_dir + 'inputs/' + item)\n",
    "            if nparr is not None and nparr.shape[0] != 0:\n",
    "                \n",
    "                #Bottom-Pad  the sentences by adding 0s to the end of the sentence\n",
    "                # Or truncate the sentence upto the length\n",
    "                if (nparr.shape[0] <= cutoff_len):\n",
    "                    npad = ((0,cutoff_len - nparr.shape[0]),(0,0))\n",
    "                    #npad = (0,cutoff_len - nparr.shape[0])\n",
    "                    nparr_pad = np.pad(nparr, pad_width=npad, mode='constant', constant_values=0)\n",
    "                else:\n",
    "                    nparr_pad = nparr[:cutoff_len,:]\n",
    "                    nparr_pad = nparr[:cutoff_len,:]\n",
    "                \n",
    "                ls_arrays.append(nparr_pad.tolist())\n",
    "                \n",
    "                #one_hot_label = np.load(save_dir + 'one_hot_labels/' + 'one_hot_sentence_label_' + item.split('_')[3])\n",
    "                #ls_arrays_one_hot_label.append(one_hot_label)\n",
    "                \n",
    "                label = np.load(save_dir + 'labels/' + 'sentence_label_' + item.split('_')[3])\n",
    "                ls_arrays_label.append(label)\n",
    "                \n",
    "                \n",
    "        nparr_batch = np.asarray(ls_arrays)\n",
    "        nparr_batch_one_hot_labels = np.asarray(ls_arrays_one_hot_label)\n",
    "        nparr_batch_labels = np.asarray(ls_arrays_label)\n",
    "        \n",
    "        return nparr_batch, nparr_batch_labels#, nparr_batch_one_hot_labels\n",
    "              \n",
    "\n",
    "        \n",
    "'''\n",
    "The main training function below. Trains the data using mini-batch gradient descent, \n",
    "and saves checkpoints of the model \n",
    "'''    \n",
    "def execute_training():\n",
    "    \n",
    "    print ('Starting the training process')\n",
    "    \n",
    "    num_epochs = 100\n",
    "    mini_batch_size = 500\n",
    "    batch_size = 500\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    mini_batch_runs = 20\n",
    "    \n",
    "    if (os.path.exists(train_tensorboard_dir)):\n",
    "        shutil.rmtree(train_tensorboard_dir)\n",
    "    os.mkdir(train_tensorboard_dir)\n",
    "    \n",
    "    \n",
    "    if (os.path.exists(valid_tensorboard_dir)):\n",
    "        shutil.rmtree(valid_tensorboard_dir)\n",
    "    os.mkdir(valid_tensorboard_dir)\n",
    "    \n",
    "    print ('Building the graph')\n",
    "    \n",
    "    # Build graph object\n",
    "    with tf.Graph().as_default() as grap:\n",
    "        #input_tensor, batch_norm_train, logits, softmax_logits,conv_keep_prob,fc_keep_prob = build_graph(cutoff_len)\n",
    "        \n",
    "        input_tensor, batch_norm_train, logits, softmax_logits, conv_keep_prob,fc_keep_prob = build_graph_convnets(cutoff_len)\n",
    "            \n",
    "        train_step, confusion_matrix,predicted_indices,cross_entropy_mean, learning_rate_input,labels, reset_metrics_vars, accuracy,update_op_acc, acc_scalar, xent_mean_scalar =  build_loss_optimizer(logits,softmax_logits,num_labels)\n",
    "        \n",
    "            \n",
    "    print ('Kicking off the session')\n",
    "    \n",
    "    # Initiate session for execution\n",
    "    with tf.Session(graph=grap) as sess:\n",
    "        \n",
    "        saver = tf.train.Saver() # For saving checkpoints for test inference \n",
    "        \n",
    "        # Initialize\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        sess.run(reset_metrics_vars)\n",
    "        \n",
    "        train_writer = tf.summary.FileWriter(train_tensorboard_dir,sess.graph)\n",
    "        valid_writer = tf.summary.FileWriter(valid_tensorboard_dir)\n",
    "        \n",
    "        # Prepares randomized batches in the /inputs/batch folder\n",
    "        #prepare_randomized_batches(one_hot_train_dir,train_batch_size,cutoff_len,train_count)\n",
    "        \n",
    "        print ('Preparing randomized validation batches')\n",
    "        prepare_randomized_batches(one_hot_valid_dir,batch_size,cutoff_len,valid_count)\n",
    "        \n",
    "        xent_counter = 0\n",
    "        _guid = uuid.uuid4()\n",
    "        \n",
    "        for i in range(0,num_epochs):\n",
    "        \n",
    "            print ('Starting Training, the Epoch is:' + str(i + 1))\n",
    "            \n",
    "            # Switch to the /inputs/batch folder and fetch all the batch files \n",
    "            # Which will be passed to the training mechanism\n",
    "            #os.chdir(one_hot_train_dir + 'inputs/batch/')\n",
    "            #train_batch_files = glob.glob('*.npy')\n",
    "            #random.shuffle(train_batch_files)\n",
    "            \n",
    "            train_conf_matrix = None\n",
    "            \n",
    "            #for file in train_batch_files:\n",
    "            for j in range(0,mini_batch_runs): # 20 mini batch runs per epoch\n",
    "            \n",
    "                print ('Get a random training batch:'  + str(j + 1))\n",
    "                train_batch, train_batch_labels = get_a_random_batch(one_hot_train_dir,mini_batch_size,cutoff_len)\n",
    "                \n",
    "                #train_batch = np.load(one_hot_train_dir + 'inputs/batch/' + file)\n",
    "                #train_labels = np.load(one_hot_train_dir + 'labels/batch/nparr_batch_labels_' + file.split('_')[2])\n",
    "                #train_one_hot_labels = np.load(one_hot_train_dir + 'one_hot_labels/batch/nparr_batch_one_hot_labels_' + file.split('_')[2])\n",
    "                \n",
    "                # Kick off the training\n",
    "                _,conf_matrix,xent_mean,xent_scalar,_,logi = sess.run(\n",
    "                [train_step,confusion_matrix, cross_entropy_mean,xent_mean_scalar,update_op_acc,logits],\n",
    "                feed_dict = {input_tensor : train_batch,\n",
    "                             labels: train_batch_labels,\n",
    "                             #one_hot_labels : None,\n",
    "                             learning_rate_input : learning_rate,\n",
    "                             batch_norm_train : True,\n",
    "                             conv_keep_prob : 1.0,\n",
    "                             fc_keep_prob : 0.6\n",
    "                    })\n",
    "                \n",
    "                if (train_conf_matrix is None):\n",
    "                    train_conf_matrix = conf_matrix\n",
    "                else:\n",
    "                    train_conf_matrix += conf_matrix\n",
    "                \n",
    "                xent_counter += 1\n",
    "                train_writer.add_summary(xent_scalar,xent_counter)\n",
    "                print ('The training cross entropy at step:' + str(xent_counter))\n",
    "                print (np.mean(xent_mean))\n",
    "                \n",
    "                \n",
    "            # Print confusion matrix out\n",
    "            with open (log_dir + str(_guid) + '.txt','a') as eg:\n",
    "                eg.write('Training Confusion Matrix:' + '\\n' + str(train_conf_matrix))\n",
    "                true_pos = np.sum(np.diag(train_conf_matrix))\n",
    "                all_pos = np.sum(train_conf_matrix)\n",
    "                eg.write('\\n' + 'Training Accuracy is: ' + str(float(true_pos / all_pos)) + '\\n')  # Another way to get accuracy!\n",
    "            \n",
    "            _,scalar = sess.run([accuracy,acc_scalar])\n",
    "            train_writer.add_summary(scalar,i)\n",
    "            \n",
    "            sess.run(reset_metrics_vars)\n",
    "            \n",
    "            \n",
    "            # Save model every 10 epochs\n",
    "            if ((i + 1) % 10 == 0):\n",
    "                print ('Saving checkpoint after epoch:' + str(i + 1))\n",
    "                saver.save(sess=sess,save_path=checkpoint_dir + 'Char_Sent_Classification_CNN.ckpt',global_step = i )\n",
    "                \n",
    "            \n",
    "            # Run on validation data to check validation accuracy\n",
    "            print ('Training Epoch ' + str(i + 1) + ' is complete. Running Validation Stats..')\n",
    "            \n",
    "            os.chdir(one_hot_valid_dir + 'inputs/batch/')\n",
    "            valid_batch_files = glob.glob('*.npy')\n",
    "            random.shuffle(valid_batch_files)\n",
    "            \n",
    "            valid_conf_matrix = None\n",
    "            \n",
    "            for file in valid_batch_files:\n",
    "                \n",
    "                valid_batch = np.load(one_hot_valid_dir + 'inputs/batch/' + file)\n",
    "                valid_labels = np.load(one_hot_valid_dir + 'labels/batch/nparr_batch_labels_' + file.split('_')[2])\n",
    "                #valid_one_hot_labels = np.load(one_hot_valid_dir + 'one_hot_labels/batch/nparr_batch_one_hot_labels_' + file.split('_')[2])\n",
    "               \n",
    "                \n",
    "                val_conf_matrix,_ = sess.run(\n",
    "                [confusion_matrix,update_op_acc],\n",
    "                feed_dict = {input_tensor : valid_batch,\n",
    "                            labels: valid_labels,\n",
    "                            #one_hot_labels : None,\n",
    "                            batch_norm_train : False,\n",
    "                            conv_keep_prob : 1.0,\n",
    "                            fc_keep_prob : 1.0\n",
    "                         \n",
    "                })\n",
    "                \n",
    "                if (valid_conf_matrix is None):\n",
    "                    valid_conf_matrix = val_conf_matrix\n",
    "                else:\n",
    "                    valid_conf_matrix += val_conf_matrix\n",
    "                    \n",
    "            \n",
    "            val_acc,val_summary = sess.run([accuracy,acc_scalar])\n",
    "            valid_writer.add_summary(val_summary,i)\n",
    "            \n",
    "            sess.run(reset_metrics_vars)\n",
    "            \n",
    "            with open (log_dir + str(_guid) + '.txt','a') as eg:\n",
    "                eg.write('Validation Confusion Matrix:' + '\\n' + str(valid_conf_matrix))\n",
    "                true_pos = np.sum(np.diag(valid_conf_matrix))\n",
    "                all_pos = np.sum(valid_conf_matrix)\n",
    "                eg.write('\\n' + 'Validation Accuracy is: ' + str(float(true_pos / all_pos)) + '\\n') \n",
    "\n",
    "            \n",
    "execute_training()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the saved checkpoints to do inference on the batch of test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(cutoff_len):\n",
    "    \n",
    "    with tf.Graph().as_default() as grap:\n",
    "        input_tensor, batch_norm_train, logits, softmax_logits,conv_keep_prob,fc_keep_prob = build_graph(cutoff_len)\n",
    "        \n",
    "    with open(one_hot_test_dir + 'ytest.txt','w') as predfile:\n",
    "    \n",
    "        with tf.Session(graph=grap) as sess:\n",
    "        \n",
    "            init = tf.global_variables_initializer()\n",
    "            sess = tf.Session()\n",
    "            sess.run(init)\n",
    "        \n",
    "            saver = tf.train.Saver()\n",
    "        \n",
    "            checkpoint_file_path = '/home/ubuntu/Desktop/challenge/offline_challenge_to_send/offline_challenge_to_send/checkpoints/Char_Sent_Classification_CNN.ckpt-99'\n",
    "\n",
    "            print ('Loading checkpoint file:' + checkpoint_file_path)\n",
    "            saver.restore(sess,checkpoint_file_path)\n",
    "\n",
    "            os.chdir(one_hot_test_dir + 'inputs/')\n",
    "            test_files = glob.glob('*.npy')\n",
    "               \n",
    "            for file in test_files:\n",
    "            \n",
    "                _idx = file.split('_')[3].replace('.npy','')\n",
    "            \n",
    "            \n",
    "                nparr = np.load(one_hot_test_dir + 'inputs/' + file)\n",
    "                npad = ((0,cutoff_len - nparr.shape[0]),(0,0))\n",
    "                nparr_pad = np.pad(nparr, pad_width=npad, mode='constant', constant_values=0)\n",
    "            \n",
    "                nparr_2 = np.expand_dims(nparr_pad,axis=0)\n",
    "             \n",
    "                predictions,soft = sess.run(\n",
    "                [\n",
    "                    logits,softmax_logits\n",
    "                ],feed_dict = {input_tensor : nparr_2,\n",
    "                                batch_norm_train : False,\n",
    "                                conv_keep_prob : 1.0,\n",
    "                                fc_keep_prob : 1.0})\n",
    "                \n",
    "                predicted_index = tf.argmax(soft, axis=1)\n",
    "                \n",
    "                predfile.write(str(_idx,) + ',' + str(predicted_index.eval(session=sess)[0]) + '\\n')\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "#inference(cutoff_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APPENDIX below: \n",
    "\n",
    "This is an alternative model based off Text Understanding From Scratch by Xiang ZHang and Yann LeCun. It is not used in the final model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
