{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Desktop/tensorflow-gpu/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/Desktop/tensorflow-gpu/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os, random , math, shutil, glob, uuid\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Initialize all the Global Variables for Train, Validation, and Test Data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Below are all the \"global\" constants used in the model (not hyperparameters or graph parameters)'''\n",
    "\n",
    "\n",
    "\n",
    "home_dir = '/home/ubuntu/Desktop/nlp_deep/amazon_data_sample/'\n",
    "one_hot_train_dir = home_dir + 'train/'\n",
    "one_hot_valid_dir = home_dir + 'valid/'\n",
    "one_hot_test_dir = home_dir + 'test/'\n",
    "train_file = home_dir + 'train_sample.csv'\n",
    "test_file = home_dir + 'test_sample.csv'\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "home_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/'\n",
    "one_hot_train_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/train/'\n",
    "one_hot_valid_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/valid/'\n",
    "one_hot_test_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/test/'\n",
    "train_file = home_dir + 'amazon_review_full_csv/train_sample.csv'\n",
    "test_file = home_dir + 'amazon_review_full_csv/test_sample.csv'\n",
    "'''\n",
    "\n",
    "\n",
    "# Stores all the checkpoint models\n",
    "checkpoint_dir = '/home/ubuntu/Desktop/nlp_deep/checkpoints/'\n",
    "#checkpoint_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/checkpoints/'\n",
    "\n",
    "\n",
    "\n",
    "# Index denotes the column placement in the character embedding matrix\n",
    "# This is used to create the one-hot character vector of sentence inputs\n",
    "\n",
    "'''\n",
    "one_hot_column_label = {'a':68,'b':67,'c':66,'d':65,'e':64,'f':63,'g':62,'h':61,'i':60,'j':59,'k':58,'l':57,'m':56,'n':55,'o':54,\n",
    "                        'p':53,'q':52,'r':51,'s':50,'t':49,'u':48,'v':47,'w':46,'x':45,'y':44,'z':43,'0':42,'1':41,'2':40,'3':39,'4':38,'5':37,'6':36,'7':35,\n",
    "                        '8':34,'9':33,'-':32,',':31,';':30,'.':29,'!':28,'?':27,':':26,'\"':25,'\\'':24,'/':23,'\\\\':22,'|':21,'_':20,\n",
    "                        '@':19,'#':18,'$':17,'%':16,'^':15,'&':14,'*':13,'~':12,'`':11,'+':10,'-':9,'=':8,'<':7,'>':6,'(':5,')':4,'[':3,\n",
    "                        ']':2,'{':1,'}':0}\n",
    "'''\n",
    "\n",
    "one_hot_column_label = {'a':42,'b':41,'c':40,'d':39,'e':38,'f':37,'g':36,'h':35,'i':34,'j':33,'k':32,'l':31,'m':30,'n':29,'o':28,\n",
    "                        'p':27,'q':26,'r':25,'s':24,'t':23,'u':22,'v':21,'w':20,'x':19,'y':18,'z':17,'0':16,'1':15,'2':14,'3':13,'4':12,'5':11,'6':10,'7':9,\n",
    "                        '8':8,'9':7,'-':6,'#':5,'.':4,'!':3,'?':2,':':1,';':0}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "one_hot_column_label = {'a':0,'b':1,'c':2,'d':3,'e':4,'f':5,'g':6,'h':7,'i':8,'j':9,'k':10,'l':11,'m':12,'n':13,'o':14,\n",
    "                        'p':15,'q':16,'r':17,'s':18,'t':19,'u':20,'v':21,'w':22,'x':23,'y':24,'z':25,'A':26,'B':27,'C':28,'D':29,'E':30,'F':31,\n",
    "                        'G':32,'H':33,'I':34,'J':35,'K':36,'L':37,'M':38,'N':39,'O':40,'P':41,'Q':42,'R':43,'S':44,'T':45,\n",
    "                        'U':46,'V':47,'W':48,'X':49,'Y':50,'Z':51,'0':52,'1':53,'2':54,'3':55,'4':56,'5':57,'6':58,'7':59,\n",
    "                        '8':60,'9':61,'-':62,',':63,';':64,'.':65,'!':66,'?':67,':':68,'\"':69,'\\'':70,'/':71,'\\\\':72,'|':73,'_':74,\n",
    "                        '@':75,'#':76,'$':77,'%':78,'^':79,'&':80,'*':81,'~':82,'`':83,'+':84,'-':85,'=':86,'<':87,'>':88,'(':89,')':90,'[':91,\n",
    "                        ']':92,'{':93,'}':94}\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "# For 1-1 mapping of character to letter encoding\n",
    "one_hot_column_label = {'a':1,'b':2,'c':3,'d':4,'e':5,'f':6,'g':7,'h':8,'i':9,'j':10,'k':11,'l':12,'m':13,'n':14,'o':15,\n",
    "                        'p':16,'q':17,'r':18,'s':19,'t':20,'u':21,'v':22,'w':23,'x':24,'y':25,'z':26,'0':27,'1':28,'2':29,'3':30,'4':31,'5':32,'6':33,'7':34,\n",
    "                        '8':35,'9':36,'?':37,'!':38,' ':39,'$':40,'.':41}\n",
    "'''\n",
    "'''\n",
    "one_hot_column_label = {'a':0,'b':1,'c':2,'d':3,'e':4,'f':5,'g':6,'h':7,'i':8,'j':9,'k':10,'l':11,'m':12,'n':13,'o':14,\n",
    "                        'p':15,'q':16,'r':17,'s':18,'t':19,'u':20,'v':21,'w':22,'x':23,'y':24,'z':25,'0':26,'1':27,'2':28,'3':29,'4':30,'5':31,'6':32,'7':33,\n",
    "                        '8':34,'9':35,'?':36,'!':37,'$':38,'.':39,' ':40}\n",
    "\n",
    "'''\n",
    "\n",
    "num_chars_dict = 43\n",
    "\n",
    "# Number of labels to classify\n",
    "num_labels = 5\n",
    "\n",
    "# Tensorboard directories\n",
    "train_tensorboard_dir = home_dir + 'train/tensorboard/'\n",
    "valid_tensorboard_dir = home_dir + 'valid/tensorboard/'\n",
    "\n",
    "log_dir = '/home/ubuntu/Desktop/nlp_deep/Char_CNN_log/'\n",
    "#log_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/log/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a 80-20 train-validation split. Get the length of the longest sentence, which will be used to build the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "The longest sentence is of length:1003\n",
      "The shortest sentence is of length:9\n",
      "The average length is:428.683941052217\n",
      "The 75th percentile is:584.0\n",
      "The 90th percentile is:778.0\n",
      "Verify Stratified Sampling\n",
      "584\n",
      "The cutoff length and counts\n",
      "584\n",
      "623999\n",
      "26000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def strip_characters(sent):\n",
    "    \n",
    "    # Strip unnecessary characters from the review\n",
    "    \n",
    "    #sent = sent.replace('-','')\n",
    "    sent = sent.replace(',','')\n",
    "    #sent = sent.replace(';','')\n",
    "    #sent = sent.replace('.','')\n",
    "    #sent = sent.replace(':','')\n",
    "    sent = sent.replace('\"','')\n",
    "    \n",
    "    sent = sent.replace('/','')\n",
    "    sent = sent.replace('\\\\','')\n",
    "    sent = sent.replace('|','')\n",
    "    sent = sent.replace('_','')\n",
    "    sent = sent.replace('@','')\n",
    "    #sent = sent.replace('#','')\n",
    "    sent = sent.replace('$','')\n",
    "    sent = sent.replace('^','')                        \n",
    "    sent = sent.replace('&','')                        \n",
    "    sent = sent.replace('*','')                        \n",
    "    sent = sent.replace('~','')                            \n",
    "    sent = sent.replace('`','')    \n",
    "    sent = sent.replace('+','')                        \n",
    "    sent = sent.replace('-','')    \n",
    "    sent = sent.replace('=','')\n",
    "    sent = sent.replace('<','')\n",
    "    sent = sent.replace('>','')                        \n",
    "    sent = sent.replace('(','')                        \n",
    "    sent = sent.replace(')','')                        \n",
    "    sent = sent.replace('[','')                        \n",
    "    sent = sent.replace(']','')                        \n",
    "    sent = sent.replace('{','')                        \n",
    "    sent = sent.replace('}','')        \n",
    "    \n",
    "    \n",
    "    # remove the stop words\n",
    "    stop_words = list(get_stop_words('en'))\n",
    "    nltk_words = list(stopwords.words('english'))\n",
    "    stop_words.extend(nltk_words)\n",
    "    \n",
    "    #print (sent)\n",
    "    #sent_words = sent.split(' ')\n",
    "    #sent_stopped_words = [w for w in sent_words if not w in stop_words]\n",
    "    #stopped_sent = ' '.join(sent_stopped_words)\n",
    "    #print (stopped_sent)\n",
    "    \n",
    "                        \n",
    "    return sent\n",
    "                        \n",
    "def main():                        \n",
    "    nltk.download('stopwords')                        \n",
    "    # Combine and label before train / valid split\n",
    "    train_data = pd.read_csv(train_file)\n",
    "    test_data = pd.read_csv(test_file)\n",
    "\n",
    "    # Are the datasets balanced - yes\n",
    "    #print ('Checking Dataset Balance')\n",
    "    #train_data.hist(column='Review')\n",
    "    #test_data.hist(column='Review')\n",
    "\n",
    "\n",
    "    # We get the max_len for padding and for the convolution filter definition in the model\n",
    "    max_len = 0\n",
    "    total_len = 0\n",
    "    min_len = 999\n",
    "    ls_len = []\n",
    "\n",
    "\n",
    "    stripped_file_train =  home_dir + 'file_strip_train.csv'   \n",
    "    stripped_file_test = home_dir + 'file_strip_test.csv'\n",
    "\n",
    "    with open(stripped_file_train,'w') as strip:   \n",
    "    \n",
    "        strip.write('Review' + ',' + 'Description' + '\\n')\n",
    "\n",
    "        for index,row in train_data.iterrows():\n",
    "    \n",
    "            sent_row = strip_characters(row[2])\n",
    "            strip.write(str(row[0]) + ','  + sent_row + '\\n')    \n",
    "        \n",
    "        \n",
    "    with open(stripped_file_test,'w') as strip:\n",
    "     \n",
    "        strip.write('Review' + ',' + 'Description' + '\\n')\n",
    "    \n",
    "        for index,row in test_data.iterrows():\n",
    "        \n",
    "            sent_row = strip_characters(row[2])\n",
    "            strip.write(str(row[0]) + ','  + sent_row + '\\n')    \n",
    "\n",
    "        \n",
    "\n",
    "    strip_train_data = pd.read_csv(stripped_file_train)\n",
    "    X_test = pd.read_csv(stripped_file_test)\n",
    "\n",
    "        \n",
    "    label_data = strip_train_data.loc[:,'Review']\n",
    "    all_data = strip_train_data\n",
    "    all_data = all_data.append(X_test,ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "    for index, row in all_data.iterrows():\n",
    "        total_len += len(row[1])\n",
    "        ls_len.append(len(row[1]))\n",
    "        if (max_len < len(row[1])):\n",
    "            max_len = len(row[1])\n",
    "        if (min_len > len(row[1])):\n",
    "            min_len = len(row[1])\n",
    "            \n",
    "    np_len = np.asarray(ls_len)\n",
    "\n",
    "    print ('The longest sentence is of length:' + str(max_len))\n",
    "    print ('The shortest sentence is of length:' + str(min_len))\n",
    "    print ('The average length is:' + str(total_len/strip_train_data.shape[0]))\n",
    "    print ('The 75th percentile is:' + str(np.percentile(np_len,75)))\n",
    "    print ('The 90th percentile is:' + str(np.percentile(np_len,90)))\n",
    "\n",
    "\n",
    "\n",
    "    # Train and test split (stratified sampling, 70-30 split)\n",
    "    X_train,X_valid,y_train,y_valid = train_test_split(strip_train_data,label_data,test_size=0.04,train_size=0.96,random_state=13814,shuffle=True,stratify=label_data)\n",
    " \n",
    "    # Verify that stratified sampling worked (histogram distributions must be same)\n",
    "    print ('Verify Stratified Sampling')\n",
    "    X_train.hist(column=\"Review\") # First histogram is training\n",
    "    X_valid.hist(column=\"Review\") # Second histogram is validation\n",
    "\n",
    "\n",
    "    'Some more \"global\" variables'\n",
    "    cutoff_len = int(np.percentile(np_len,75)) # a graph parameter\n",
    "    print(cutoff_len)\n",
    "    train_count = X_train.shape[0]\n",
    "    valid_count = X_valid.shape[0]\n",
    "    \n",
    "    return cutoff_len, train_count, valid_count, X_train, X_valid\n",
    "    \n",
    "    \n",
    "cutoff_len, train_count, valid_count, X_train, X_valid = main()\n",
    "print ('The cutoff length and counts')\n",
    "print (str(cutoff_len))\n",
    "print (str(train_count))       \n",
    "print (str(valid_count))       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert each sentence to one-hot character matrix to drive the non-static character embedding matrix creation, and store this character matrix representation in disk cache.\n",
    "\n",
    "Also store in disk cache, the label and a one-hot representation of the label, for the cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the one-hot character vector representation of sentences which will be used as the layer before a\n",
    "# character-embedding matrix\n",
    "def save_numpy_one_hot(X_dataset,unique_char_size,save_dir,one_hot_column_label):\n",
    "    \n",
    "    for index,row in X_dataset.iterrows():\n",
    "        \n",
    "        #sent = row[2]\n",
    "        sent = row[1]\n",
    "        #print (sent)\n",
    "        np_one_hot = np.zeros(shape=(len(sent),unique_char_size),dtype=np.int32)\n",
    "        #np_one_hot = np.zeros(shape=len(sent),dtype=np.int32)\n",
    "        \n",
    "        for i in range(0,len(sent)):\n",
    "            if (sent[i].lower() in one_hot_column_label):\n",
    "                np_one_hot[i][one_hot_column_label[sent[i].lower()]] = 1 # else, it remains 0 encoded.\n",
    "                #np_one_hot[i] = one_hot_column_label[sent[i].lower()] # For character embedding lookup\n",
    "            #else:\n",
    "            #    np_one_hot[i][41] = 1 # unknown character\n",
    "        #print(np_one_hot)\n",
    "            \n",
    "        # Save the one-hot encoding for each sentence \n",
    "        np.save(save_dir + 'inputs/' + 'one_hot_sentence_' + str(index) + '.npy',np_one_hot)\n",
    "        \n",
    "        #Save the label\n",
    "        lbl = int(row[0]) - 1 # Must start with 0-indexing\n",
    "        np.save(save_dir + 'labels/' +  'sentence_label_' + str(index) + '.npy',np.asarray(lbl))\n",
    "        \n",
    "        # Save a one-hot version of the label (for xentropy loss function)\n",
    "        np_one_hot_label = np.zeros(shape=(5),dtype=np.int32)\n",
    "        np_one_hot_label[lbl] = 1\n",
    "        \n",
    "        np.save(save_dir + 'one_hot_labels/' + 'one_hot_sentence_label_' + str(index) + '.npy',np_one_hot_label)\n",
    "        \n",
    "        \n",
    "def save_numpy_one_hot_test(X_dataset,unique_char_size,save_dir,one_hot_column_label):\n",
    "    \n",
    "    for index,row in X_dataset.iterrows():\n",
    "        \n",
    "        sent = row[1]\n",
    "        np_one_hot = np.zeros(shape=(len(sent),unique_char_size),dtype=np.int32)\n",
    "        #np_one_hot = np.zeros(shape=len(sent),dtype=np.int32)\n",
    "        \n",
    "        for i in range(0,len(sent)):\n",
    "            if (sent[i].lower() in one_hot_column_label):\n",
    "                np_one_hot[i][one_hot_column_label[sent[i].lower()]] = 1\n",
    "                #np_one_hot[i] = one_hot_column_label[sent[i].lower()]\n",
    "            #else:\n",
    "            #    np_one_hot[i][41] = 1\n",
    "            \n",
    "        # Save the one-hot encoding for each sentence \n",
    "        np.save(save_dir + 'inputs/' + 'one_hot_sentence_' + str(index) + '.npy',np_one_hot)\n",
    "        \n",
    "        \n",
    "def inp_create_one_hot_char_mat(unique_char_size,train_dir,valid_dir,test_dir):\n",
    "    \n",
    "    save_numpy_one_hot(X_train,unique_char_size,train_dir,one_hot_column_label)\n",
    "    save_numpy_one_hot(X_valid,unique_char_size,valid_dir,one_hot_column_label)\n",
    "    #save_numpy_one_hot_test(X_test,unique_char_size,test_dir,one_hot_column_label)\n",
    "    \n",
    "\n",
    "# One  - off folder creation \n",
    "if (not os.path.exists(one_hot_train_dir)):\n",
    "        \n",
    "        print ('Setting up directories and preparing one-hot character vectors')\n",
    "        print ('Number chars:' + str(num_chars_dict))\n",
    "        os.mkdir(one_hot_train_dir)\n",
    "        os.mkdir(one_hot_valid_dir)\n",
    "        os.mkdir(one_hot_test_dir)\n",
    "        os.mkdir(one_hot_train_dir + 'inputs/')\n",
    "        os.mkdir(one_hot_valid_dir + 'inputs/')\n",
    "        os.mkdir(one_hot_test_dir + 'inputs/')\n",
    "        os.mkdir(one_hot_train_dir + 'labels/')\n",
    "        os.mkdir(one_hot_valid_dir + 'labels/')\n",
    "        os.mkdir(one_hot_train_dir + 'one_hot_labels/')\n",
    "        os.mkdir(one_hot_valid_dir + 'one_hot_labels/')\n",
    "        os.mkdir(one_hot_train_dir + 'tensorboard/')\n",
    "        os.mkdir(one_hot_valid_dir + 'tensorboard/')\n",
    "        inp_create_one_hot_char_mat(num_chars_dict,one_hot_train_dir,one_hot_valid_dir,one_hot_test_dir)\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build the graph and define the loss and optimizer operations.\n",
    "\n",
    "The below function is not used - please see build_graph_convnet function below\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start to build the model\n",
    "# Let's build the graph first\n",
    "\n",
    "X_train = None\n",
    "X_valid = None\n",
    "\n",
    "# This function is Not Used in the main model - please see build_graph_convnets below\n",
    "def build_graph(cutoff_len):\n",
    "    \n",
    "    # Hyper Parameters with descriptions\n",
    "    num_char_dict = num_chars_dict\n",
    "    char_embedding_n_dim = 256 # The number of columns in the character embedding matrix\n",
    "    \n",
    "    first_window_size = 3 # First window applied to sentence\n",
    "    second_window_size = 4 # Second window\n",
    "    third_window_size = 5 # And so on...\n",
    "    fourth_window_size = 6\n",
    "    \n",
    "    \n",
    "    first_layer_no_filters = 256\n",
    "    second_layer_no_filters = 256\n",
    "    third_layer_no_filters = 256\n",
    "    fourth_layer_no_filters = 256\n",
    "    \n",
    "    first_fc_out = 1024\n",
    "    second_fc_out = 1024\n",
    "    \n",
    "    # Hyper parameters end \n",
    "    \n",
    "    reuse_flag = tf.AUTO_REUSE\n",
    "    \n",
    "    # Build the graph\n",
    "    with tf.variable_scope('layer_one_char_embedding',reuse=reuse_flag):\n",
    "        \n",
    "        batch_norm_train = tf.placeholder(name=\"batch_norm_train\", dtype=tf.bool)\n",
    "        \n",
    "        '''This layer defines the character embedding matrix, and returns character embedding for batches of sentences'''\n",
    "        \n",
    "        # The input_tensor is of shape [None, cutoff_len, num_char_dict] where cutoff_len is the length limit for the 2-D embedding matrix\n",
    "        #input_tensor = tf.placeholder(dtype=tf.float32,shape=[None,cutoff_len,num_char_dict],name=\"input_tensor\") \n",
    "        input_tensor = tf.placeholder(dtype=tf.int32,shape=[None,cutoff_len],name=\"input_tensor\") \n",
    "        \n",
    "        # Initializer for embedding matrix\n",
    "        l1w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        # The character embedding matrix declared below. The 2nd rank is the number of dimensions representing each character\n",
    "        char_embedding_mat = tf.get_variable(name=\"char_embedding\",dtype=tf.float32,shape=[num_char_dict,char_embedding_n_dim],initializer=l1w_init)\n",
    "        \n",
    "        input_tensor_2D = tf.reshape(input_tensor,shape=[-1,num_char_dict]) \n",
    "        #l1_char_embedding = tf.matmul(input_tensor_2D,char_embedding_mat)\n",
    "        \n",
    "        l1_char_embedding = tf.nn.embedding_lookup(char_embedding_mat,input_tensor,name=\"l1_char_embedding\")\n",
    "        print(l1_char_embedding)\n",
    "        \n",
    "        # reshape char embedding matrix to 3D matrix for the temporal 1D convolution operation\n",
    "        input_tensor_with_embed = tf.reshape(l1_char_embedding,shape=[-1,cutoff_len,char_embedding_n_dim])\n",
    "        \n",
    "        # Some keep probabilities for adding dropout\n",
    "        conv_keep_prob = tf.placeholder(dtype=tf.float32,shape=[],name=\"conv_dropout_prob\")\n",
    "        fc_keep_prob = tf.placeholder(dtype=tf.float32,shape=[],name=\"fc_dropout_prob\")\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('layer_two_kernels',reuse=reuse_flag):\n",
    "        \n",
    "        l2w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        \n",
    "        # 1-D convolution layer\n",
    "        first_conv_layer = tf.layers.conv1d(inputs = input_tensor_with_embed, filters = first_layer_no_filters,kernel_size = first_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"first_conv_layer\")\n",
    "        # Batch normalization\n",
    "        first_conv_batch = tf.contrib.layers.batch_norm(first_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        \n",
    "        # Pooling Layer\n",
    "        #first_sumpool_layer = tf.reduce_sum(first_conv_batch, axis=1,keepdims=False) # Global sumpool 1-D\n",
    "        first_maxpool_layer = tf.reduce_max(first_conv_batch, axis=1,keepdims=False) # Global maxpool 1-D\n",
    "        \n",
    "        # Dropout\n",
    "        first_layer = tf.nn.dropout(first_maxpool_layer,keep_prob = conv_keep_prob )\n",
    "        \n",
    "        \n",
    "        # Repeat for second stack\n",
    "        second_conv_layer = tf.layers.conv1d(inputs = input_tensor_with_embed, filters = second_layer_no_filters,kernel_size = second_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(), name=\"second_conv_layer\")\n",
    "        second_conv_batch = tf.contrib.layers.batch_norm(second_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        #second_sumpool_layer = tf.reduce_sum(second_conv_batch, axis=1,keepdims=False) \n",
    "        second_maxpool_layer = tf.reduce_max(second_conv_batch, axis=1,keepdims=False) \n",
    "        \n",
    "        second_layer = tf.nn.dropout(second_maxpool_layer,keep_prob = conv_keep_prob )\n",
    "        \n",
    "        \n",
    "        #...and third stack...\n",
    "        third_conv_layer = tf.layers.conv1d(inputs = input_tensor_with_embed, filters = third_layer_no_filters,kernel_size = third_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(), name=\"third_conv_layer\")\n",
    "        third_conv_batch = tf.contrib.layers.batch_norm(third_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        #third_sumpool_layer = tf.reduce_sum(third_conv_batch, axis=1,keepdims=False) \n",
    "        third_maxpool_layer = tf.reduce_max(third_conv_batch, axis=1,keepdims=False) \n",
    "        \n",
    "        \n",
    "        third_layer = tf.nn.dropout(third_maxpool_layer,keep_prob = conv_keep_prob )\n",
    "        \n",
    "        \n",
    "        #..and fourth stack..\n",
    "        fourth_conv_layer = tf.layers.conv1d(inputs = input_tensor_with_embed, filters = fourth_layer_no_filters,kernel_size = fourth_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(), name=\"fourth_conv_layer\")\n",
    "        fourth_conv_batch = tf.contrib.layers.batch_norm(fourth_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        #fourth_sumpool_layer = tf.reduce_sum(fourth_conv_batch, axis=1,keepdims=False) # Sums across the 'length' \n",
    "        fourth_maxpool_layer = tf.reduce_sum(fourth_conv_batch, axis=1,keepdims=False) # Sums across the 'length' \n",
    "        \n",
    "        fourth_layer = tf.nn.dropout(fourth_maxpool_layer,keep_prob = conv_keep_prob )\n",
    "        \n",
    "        all_conv = tf.concat(axis=1,values=[first_layer,second_layer,third_layer,fourth_layer],\n",
    "                                            #fifth_conv_dropout,sixth_conv_dropout,seventh_conv_dropout],\n",
    "                                            name=\"all_conv\")\n",
    "        \n",
    "        print ('Stacked Convolution Tensor as below')\n",
    "        print(all_conv)\n",
    "        \n",
    "        \n",
    "    # Now pass it through three FC layers\n",
    "    with tf.variable_scope('layer_three_fc',reuse=reuse_flag):\n",
    "       \n",
    "        l3w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        #l3b_init = tf.random_normal_initializer(mean=0, stddev=0.1, dtype=tf.float32)\n",
    "        l3b_init = tf.zeros_initializer()\n",
    "        \n",
    "        first_fc = tf.contrib.layers.fully_connected(inputs=all_conv,num_outputs=first_fc_out,activation_fn = tf.nn.relu,\n",
    "                                                    weights_initializer = l3w_init,biases_initializer = l3b_init,scope=\"first_fc\")\n",
    "        \n",
    "        first_fc_batch = tf.contrib.layers.batch_norm(first_fc,center = True, scale=False,is_training=batch_norm_train)\n",
    "        first_fc_final = tf.nn.dropout(first_fc_batch,keep_prob = fc_keep_prob) \n",
    "        \n",
    "    '''\n",
    "    with tf.variable_scope('layer_four_fc',reuse=reuse_flag):\n",
    "        l4w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        #l4b_init = tf.random_normal_initializer(mean=0, stddev=0.1, dtype=tf.float32)\n",
    "        l4b_init = tf.zeros_initializer()\n",
    "        \n",
    "        second_fc = tf.contrib.layers.fully_connected(inputs=first_fc_final,num_outputs=second_fc_out,activation_fn = tf.nn.relu,\n",
    "                                                    weights_initializer = l4w_init,biases_initializer = l4b_init,scope=\"second_fc\")\n",
    "        second_fc_batch = tf.contrib.layers.batch_norm(second_fc,center = True, scale=False,is_training=batch_norm_train)\n",
    "        second_fc_final = tf.nn.dropout(second_fc_batch,keep_prob = fc_keep_prob)\n",
    "    '''\n",
    "    # Third FC layer and softmax\n",
    "    with tf.variable_scope('layer_five_fc',reuse=reuse_flag):\n",
    "        l5w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        #l5b_init = tf.random_normal_initializer(mean=0, stddev=0.1, dtype=tf.float32)\n",
    "        l5b_init = tf.zeros_initializer()\n",
    "        \n",
    "        third_fc = tf.contrib.layers.fully_connected(inputs = first_fc_final,num_outputs=num_labels,weights_initializer=l5w_init,\n",
    "                                                    biases_initializer = l5b_init,activation_fn=tf.nn.relu,scope=\"third_fc\")\n",
    "        \n",
    "        third_fc_batch = tf.contrib.layers.batch_norm(third_fc,center=True,scale=False,is_training=batch_norm_train)\n",
    "        third_fc_final = tf.nn.dropout(third_fc_batch,keep_prob = fc_keep_prob)\n",
    "    \n",
    "    with tf.variable_scope('layer_six_softmax',reuse=reuse_flag):\n",
    "        softmax_logits = tf.nn.softmax(logits=third_fc_final,name=\"final_logits_softmax\")\n",
    "        \n",
    "    return input_tensor, batch_norm_train, third_fc_final,softmax_logits, conv_keep_prob,fc_keep_prob\n",
    "        \n",
    "\n",
    "\n",
    "def build_loss_optimizer(logits,softmax_logits,num_labels):\n",
    "    \n",
    "    ''' Loss and Optimizer builder, with Softmax Cross Entropy loss'''\n",
    "    \n",
    "    with tf.variable_scope('cross_entropy'):\n",
    "        \n",
    "        learning_rate_input = tf.placeholder(\n",
    "            tf.float32, [], name='learning_rate_input')\n",
    "        one_hot_labels = tf.placeholder(\n",
    "            name=\"one_hot_labels\",shape=[None,num_labels],dtype=tf.int32)\n",
    "        labels = tf.placeholder(\n",
    "            name=\"labels\",shape=[None],dtype=tf.int64)\n",
    "        \n",
    "        #cross_entropy_mean = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits=logits)\n",
    "        #cross_entropy_mean = tf.nn.softmax_cross_entropy_with_logits_v2(labels = one_hot_labels,logits=logits,name=\"cross_entropy_mean_v2\")\n",
    "        cross_entropy_mean = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logits,name=\"cross_entropy_mean_sparse\")\n",
    "        loss = tf.reduce_mean(cross_entropy_mean,name=\"cross_entropy_loss\")\n",
    "        \n",
    "        'Toggle between optimizers..'\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_input)\n",
    "        #optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate_input)\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate_input)\n",
    "        \n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # For batch normalization ops update\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            train_step = optimizer.minimize(loss)\n",
    "        \n",
    "        predicted_indices = tf.argmax(softmax_logits, 1, name=\"predicted_indices\")\n",
    "        correct_prediction = tf.equal(predicted_indices, labels, name='correct_prediction') # Labels are 0-indexed\n",
    "        confusion_matrix = tf.confusion_matrix(\n",
    "            labels, predicted_indices, num_classes=num_labels, name=\"confusion_matrix\")\n",
    "        \n",
    "        '''\n",
    "        # Because the accuracy will be calculated across batch splits, this facilitates resetting the metrics inter-epochs\n",
    "        with tf.variable_scope('streaming_ops'):\n",
    "            accuracy,update_op_acc = tf.metrics.accuracy(labels,predicted_indices)\n",
    "            \n",
    "        metrics_vars = tf.contrib.framework.get_variables('cross_entropy/streaming_ops',collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "        reset_metrics_vars = tf.variables_initializer(metrics_vars) # Running sess.run will reset the streaming metrics within epochs\n",
    "        \n",
    "        # For tensorboard \n",
    "        xent_mean_scalar= tf.summary.scalar('cross_entropy_mean',tf.reduce_mean(cross_entropy_mean))\n",
    "        acc_scalar = tf.summary.scalar('accuracy',accuracy)\n",
    "        '''\n",
    "        \n",
    "        return train_step, confusion_matrix,predicted_indices,cross_entropy_mean, learning_rate_input,labels, one_hot_labels,loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative model based off\n",
    "# https://arxiv.org/pdf/1502.01710v5.pdf\n",
    "\n",
    "def build_graph_convnets(cutoff_len):\n",
    "    \n",
    "    num_chars = num_chars_dict\n",
    "    \n",
    "    char_embedding_n_dim = 256\n",
    "    \n",
    "    first_window_size = 7 # First window applied to sentence\n",
    "    second_window_size = 7 # Second window\n",
    "    third_window_size = 3 # And so on...\n",
    "    fourth_window_size = 3\n",
    "    fifth_window_size = 3\n",
    "    sixth_window_size = 3\n",
    "    \n",
    "    # No third, fourth, or fifth pool size, copying the paper's approach \n",
    "    first_pool_size = 3\n",
    "    second_pool_size = 3\n",
    "    sixth_pool_size = 3\n",
    "    \n",
    "    # One channel output for each of the six conv layers\n",
    "    first_layer_no_filters = 1024 \n",
    "    second_layer_no_filters = 1024\n",
    "    third_layer_no_filters = 1024\n",
    "    fourth_layer_no_filters = 1024\n",
    "    fifth_layer_no_filters = 1024\n",
    "    sixth_layer_no_filters = 1024\n",
    "    \n",
    "    first_fc_out = 2048\n",
    "    second_fc_out = 2048\n",
    "    third_fc_out = num_labels \n",
    "    # Hyper parameters end \n",
    "    \n",
    "    reuse_flag = tf.AUTO_REUSE\n",
    "    \n",
    "    # Layer Zero is just to define the input tensors and other 'global' tensors such as dropout probabilities\n",
    "    with tf.variable_scope('layer_zero_convnet',reuse=reuse_flag):\n",
    "        \n",
    "         # This layer defines the character embedding matrix, and returns character embedding for batches of sentences\n",
    "        # tensor: input_tensor is of shape [None, 26] where the first dimension is the batch_size * max_len for that batch\n",
    "        #input_tensor = tf.placeholder(dtype=tf.float32,shape=[None,cutoff_len,num_chars_dict],name=\"input_tensor\") # Unroll the first 2 dimensions at the numpy array before feeding\n",
    "        input_tensor = tf.placeholder(dtype=tf.float32,shape=[None,cutoff_len,num_chars_dict],name=\"input_tensor\") # Unroll the first 2 dimensions at the numpy array before feeding\n",
    "        conv_keep_prob = tf.placeholder(dtype=tf.float32,shape=[],name=\"conv_dropout_prob\")\n",
    "        fc_keep_prob = tf.placeholder(dtype=tf.float32,shape=[],name=\"fc_dropout_prob\")\n",
    "        batch_norm_train = tf.placeholder(name=\"batch_norm_train\", dtype=tf.bool)\n",
    "        print (input_tensor)\n",
    "        \n",
    "        #l0w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        #l0b_init = tf.zeros_initializer()\n",
    "        #char_embedding_mat = tf.get_variable(name=\"char_embedding\",dtype=tf.float32,shape=[num_chars_dict,char_embedding_n_dim],initializer=l0w_init,trainable=True)\n",
    "        #char_emb_bias = tf.get_variable(name=\"char_embedding_bias\",dtype=tf.float32,shape=[char_embedding_n_dim],initializer = l0b_init)\n",
    "        \n",
    "        # This gets the char embeddings for the batch\n",
    "        # Shape is [batch_size * cutoff_len, char_embedding_n_dim]\n",
    "        #input_tensor_2D = tf.reshape(input_tensor,shape=[-1,num_chars_dict]) # Reshaped to 2D with batch_size * cutoff_len as 1st dim\n",
    "        #l1_char_embedding = tf.matmul(input_tensor_2D,char_embedding_mat) + char_emb_bias\n",
    "        \n",
    "        #l1_char_embedding = tf.nn.embedding_lookup(char_embedding_mat,input_tensor,name=\"l1_char_embedding\")\n",
    "        #print(l1_char_embedding)\n",
    "        \n",
    "        # reshape input tensor to 3D matrix for the temporal 1D convolution operation\n",
    "        #input_tensor_with_embed = tf.reshape(l1_char_embedding,shape=[-1,cutoff_len,char_embedding_n_dim])\n",
    "        #input_tensor_with_embed = l1_char_embedding\n",
    "        #input_tensor_embed_relu = tf.nn.relu(input_tensor_with_embed,name=\"input_tensor_embed_relu\")\n",
    "        #input_tensor_batch = tf.contrib.layers.batch_norm(input_tensor_embed_relu,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #print(input_tensor_batch)\n",
    "        \n",
    "        # The input tensor is already one-hot encoded , with a character dictionary size of 26 characters\n",
    "        #input_tensor = tf.placeholder(dtype=tf.float32,shape=[None,cutoff_len,26],name=\"input_tensor\")\n",
    "        \n",
    "        \n",
    "        # Toggle application of either population or sample mean and variance during batch normalization\n",
    "        # is False during validation / testing to allow population statistics to apply\n",
    "        \n",
    "        \n",
    "        #first_embed_batch = tf.contrib.layers.batch_norm(input_tensor_with_embed,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #first_embed_relu = tf.nn.leaky_relu(first_embed_batch,name=\"first_embed_relu\")\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('layer_one_convnet',reuse=reuse_flag):\n",
    "        \n",
    "        #l1w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l1w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        \n",
    "        first_conv_layer = tf.layers.conv1d(inputs = input_tensor, filters = first_layer_no_filters,kernel_size = first_window_size,strides=1,padding='valid',\n",
    "                                            activation=tf.nn.relu, use_bias=True, kernel_initializer= l1w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"first_conv_layer\")\n",
    "        first_conv_batch = tf.contrib.layers.batch_norm(first_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #first_conv_relu = tf.nn.leaky_relu(first_conv_batch,name=\"first_conv_relu\")\n",
    "        first_maxpool_layer = tf.layers.max_pooling1d(inputs = first_conv_batch, pool_size = first_pool_size, strides=first_pool_size,\n",
    "                                                      padding='valid',name=\"first_maxpool_layer\")\n",
    "        \n",
    "\n",
    "        \n",
    "    with tf.variable_scope('layer_two_convnet',reuse=reuse_flag):\n",
    "        \n",
    "        #l2w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l2w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        second_conv_layer = tf.layers.conv1d(inputs = first_maxpool_layer, filters = second_layer_no_filters,kernel_size = second_window_size,strides=1,padding='valid'\n",
    "                                            ,activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"second_conv_layer\")\n",
    "        second_conv_batch = tf.contrib.layers.batch_norm(second_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #second_conv_relu = tf.nn.leaky_relu(second_conv_batch,name=\"second_conv_relu\")\n",
    "        second_maxpool_layer = tf.layers.max_pooling1d(inputs = second_conv_batch, pool_size = second_pool_size, strides= second_pool_size,\n",
    "                                                      padding='valid',name=\"second_maxpool_layer\")\n",
    "\n",
    "\n",
    "        \n",
    "    with tf.variable_scope('layer_three_convnet',reuse=reuse_flag):\n",
    "        #l3w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l3w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        third_conv_layer = tf.layers.conv1d(inputs = second_maxpool_layer, filters = third_layer_no_filters,kernel_size = third_window_size,strides=1,padding='valid',\n",
    "                                            activation=tf.nn.relu, use_bias=True, kernel_initializer= l3w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"third_conv_layer\")\n",
    "        third_conv_batch = tf.contrib.layers.batch_norm(third_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #third_conv_relu = tf.nn.leaky_relu(third_conv_batch,name=\"third_conv_relu\")\n",
    "        \n",
    "    with tf.variable_scope('layer_four_convnet',reuse=reuse_flag):\n",
    "        #l4w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l4w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        fourth_conv_layer = tf.layers.conv1d(inputs = third_conv_batch, filters = fourth_layer_no_filters,kernel_size = fourth_window_size,strides=1,padding='valid',\n",
    "                                            activation=tf.nn.relu, use_bias=True, kernel_initializer= l4w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"fourth_conv_layer\")\n",
    "        fourth_conv_batch = tf.contrib.layers.batch_norm(fourth_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #fourth_conv_relu = tf.nn.leaky_relu(fourth_conv_batch,name=\"fourth_conv_relu\")\n",
    "        \n",
    "    with tf.variable_scope('layer_five_convnet',reuse=reuse_flag):\n",
    "        #l5w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l5w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        fifth_conv_layer = tf.layers.conv1d(inputs = fourth_conv_batch, filters = fifth_layer_no_filters,kernel_size = fifth_window_size,strides=1,padding='valid',\n",
    "                                            activation=tf.nn.relu, use_bias=True, kernel_initializer= l5w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"fifth_conv_layer\")\n",
    "        fifth_conv_batch = tf.contrib.layers.batch_norm(fifth_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #fifth_conv_relu = tf.nn.leaky_relu(fifth_conv_batch,name=\"fifth_conv_relu\")\n",
    "        \n",
    "    with tf.variable_scope('layer_six_convnet',reuse=reuse_flag):\n",
    "        #l6w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l6w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        sixth_conv_layer = tf.layers.conv1d(inputs = fifth_conv_batch, filters = sixth_layer_no_filters,kernel_size = sixth_window_size,strides=1,padding='valid',\n",
    "                                            activation=tf.nn.relu, use_bias=True, kernel_initializer= l6w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"sixth_conv_layer\")\n",
    "        sixth_conv_batch = tf.contrib.layers.batch_norm(sixth_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #sixth_conv_relu = tf.nn.leaky_relu(sixth_conv_batch,name=\"sixth_conv_relu\")\n",
    "        sixth_maxpool_layer = tf.layers.max_pooling1d(inputs = sixth_conv_batch, pool_size = sixth_pool_size, strides=sixth_pool_size,\n",
    "                                                      padding='valid',name=\"sixth_maxpool_layer\")\n",
    "        \n",
    "        #sixth_reshaped_layer = tf.reshape(sixth_maxpool_layer,shape=[-1,sixth_maxpool_layer.get_shape()[1] * sixth_maxpool_layer.get_shape()[2]])\n",
    "        sixth_reshaped_layer = tf.contrib.layers.flatten(sixth_maxpool_layer) \n",
    "        print (sixth_reshaped_layer)\n",
    "        \n",
    "    \n",
    "    with tf.variable_scope('layer_seven_convnet',reuse=reuse_flag):\n",
    "        #l7w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l7w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        l7b_init = tf.zeros_initializer()\n",
    "        \n",
    "        seventh_fc = tf.contrib.layers.fully_connected(inputs=sixth_reshaped_layer,num_outputs=first_fc_out,activation_fn = tf.nn.relu,\n",
    "                                                       weights_initializer = l7w_init,biases_initializer = l7b_init,scope=\"seventh_fc\")\n",
    "        seventh_fc_batch = tf.contrib.layers.batch_norm(seventh_fc,center = True, scale=True,is_training=batch_norm_train)\n",
    "        #seventh_fc_relu = tf.nn.leaky_relu(seventh_fc_batch,name=\"seventh_fc_relu\")\n",
    "        seventh_fc_final = tf.nn.dropout(seventh_fc_batch,keep_prob = fc_keep_prob)\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('layer_eight_convnet',reuse=reuse_flag):\n",
    "        #l8w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l8w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        l8b_init = tf.zeros_initializer()\n",
    "        \n",
    "        eigth_fc = tf.contrib.layers.fully_connected(inputs=seventh_fc_final,num_outputs=second_fc_out,activation_fn = tf.nn.relu,\n",
    "                                                       weights_initializer = l8w_init,biases_initializer = l8b_init,scope=\"eigth_fc\")\n",
    "        eigth_fc_batch = tf.contrib.layers.batch_norm(eigth_fc,center = True, scale=True,is_training=batch_norm_train)\n",
    "        #eigth_fc_relu = tf.nn.leaky_relu(eigth_fc_batch,name=\"eigth_fc_relu\")\n",
    "        eigth_fc_final = tf.nn.dropout(eigth_fc_batch,keep_prob = fc_keep_prob)\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('layer_ninth_convnet',reuse=reuse_flag):\n",
    "        #l9w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32) \n",
    "        l9w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        l9b_init = tf.zeros_initializer()\n",
    "        \n",
    "        ninth_fc = tf.contrib.layers.fully_connected(inputs=eigth_fc_final,num_outputs=num_labels,activation_fn = None,\n",
    "                                                     weights_initializer = l9w_init,biases_initializer = l9b_init,scope=\"ninth_fc\")\n",
    "        #ninth_fc_batch = tf.contrib.layers.batch_norm(ninth_fc,center = True, scale=True,is_training=batch_norm_train)\n",
    "\n",
    "        ninth_final_softmax = tf.nn.softmax(ninth_fc,name=\"ninth_final_softmax\")\n",
    "        \n",
    "        return input_tensor, batch_norm_train, ninth_fc,ninth_final_softmax, conv_keep_prob,fc_keep_prob\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the graph is built, randomly batch the inputs from disk cache, and feed them to the graph to train a model.\n",
    "\n",
    "Training takes between 30-45 min on a Tesla M60 GPU from an AWS g3 EC2 instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the training process\n",
      "Building the graph\n",
      "Tensor(\"layer_zero_convnet/input_tensor:0\", shape=(?, 584, 43), dtype=float32)\n",
      "WARNING:tensorflow:From /home/ubuntu/Desktop/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "Tensor(\"layer_six_convnet/Flatten/flatten/Reshape:0\", shape=(?, 18432), dtype=float32)\n",
      "Kicking off the session\n",
      "Preparing randomized validation batches\n",
      "The input directory is:/home/ubuntu/Desktop/nlp_deep/amazon_data_sample/valid/inputs/\n",
      "breaking afetr processing files:20000\n",
      "one_hot_sentence_359954.npy\n",
      "623999\n",
      "Starting Training, the Epoch is:1\n",
      "Starting Training, the Epoch is:2\n",
      "Starting Training, the Epoch is:3\n",
      "Starting Training, the Epoch is:4\n",
      "Starting Training, the Epoch is:5\n",
      "Starting Training, the Epoch is:6\n",
      "Starting Training, the Epoch is:7\n",
      "Starting Training, the Epoch is:8\n",
      "Starting Training, the Epoch is:9\n",
      "Starting Training, the Epoch is:10\n",
      "Starting Training, the Epoch is:11\n",
      "Starting Training, the Epoch is:12\n",
      "Starting Training, the Epoch is:13\n",
      "Starting Training, the Epoch is:14\n",
      "Starting Training, the Epoch is:15\n",
      "Starting Training, the Epoch is:16\n",
      "Starting Training, the Epoch is:17\n",
      "Starting Training, the Epoch is:18\n",
      "Starting Training, the Epoch is:19\n",
      "Starting Training, the Epoch is:20\n",
      "Starting Training, the Epoch is:21\n",
      "Starting Training, the Epoch is:22\n",
      "Starting Training, the Epoch is:23\n",
      "Starting Training, the Epoch is:24\n",
      "Starting Training, the Epoch is:25\n",
      "Starting Training, the Epoch is:26\n",
      "Starting Training, the Epoch is:27\n",
      "Starting Training, the Epoch is:28\n",
      "Starting Training, the Epoch is:29\n",
      "Starting Training, the Epoch is:30\n",
      "Starting Training, the Epoch is:31\n",
      "Starting Training, the Epoch is:32\n",
      "Starting Training, the Epoch is:33\n",
      "Starting Training, the Epoch is:34\n",
      "Starting Training, the Epoch is:35\n",
      "Starting Training, the Epoch is:36\n",
      "Starting Training, the Epoch is:37\n",
      "Starting Training, the Epoch is:38\n",
      "Starting Training, the Epoch is:39\n",
      "Starting Training, the Epoch is:40\n",
      "Starting Training, the Epoch is:41\n",
      "Starting Training, the Epoch is:42\n",
      "Starting Training, the Epoch is:43\n",
      "Starting Training, the Epoch is:44\n",
      "Starting Training, the Epoch is:45\n",
      "Starting Training, the Epoch is:46\n",
      "Starting Training, the Epoch is:47\n",
      "Starting Training, the Epoch is:48\n",
      "Starting Training, the Epoch is:49\n",
      "Starting Training, the Epoch is:50\n",
      "Starting Training, the Epoch is:51\n",
      "Starting Training, the Epoch is:52\n",
      "Starting Training, the Epoch is:53\n",
      "Starting Training, the Epoch is:54\n",
      "Starting Training, the Epoch is:55\n",
      "Starting Training, the Epoch is:56\n",
      "Starting Training, the Epoch is:57\n",
      "Starting Training, the Epoch is:58\n",
      "Starting Training, the Epoch is:59\n",
      "Starting Training, the Epoch is:60\n",
      "Starting Training, the Epoch is:61\n",
      "Starting Training, the Epoch is:62\n",
      "Starting Training, the Epoch is:63\n",
      "Starting Training, the Epoch is:64\n",
      "Starting Training, the Epoch is:65\n",
      "Starting Training, the Epoch is:66\n",
      "Starting Training, the Epoch is:67\n",
      "Starting Training, the Epoch is:68\n",
      "Starting Training, the Epoch is:69\n",
      "Starting Training, the Epoch is:70\n",
      "Starting Training, the Epoch is:71\n",
      "Starting Training, the Epoch is:72\n",
      "Starting Training, the Epoch is:73\n",
      "Starting Training, the Epoch is:74\n",
      "Starting Training, the Epoch is:75\n",
      "Starting Training, the Epoch is:76\n",
      "Starting Training, the Epoch is:77\n",
      "Starting Training, the Epoch is:78\n",
      "Starting Training, the Epoch is:79\n",
      "Starting Training, the Epoch is:80\n",
      "Starting Training, the Epoch is:81\n",
      "Starting Training, the Epoch is:82\n",
      "Starting Training, the Epoch is:83\n",
      "Starting Training, the Epoch is:84\n",
      "Starting Training, the Epoch is:85\n",
      "Starting Training, the Epoch is:86\n",
      "Starting Training, the Epoch is:87\n",
      "Starting Training, the Epoch is:88\n",
      "Starting Training, the Epoch is:89\n",
      "Starting Training, the Epoch is:90\n",
      "Starting Training, the Epoch is:91\n",
      "Starting Training, the Epoch is:92\n",
      "Starting Training, the Epoch is:93\n",
      "Starting Training, the Epoch is:94\n",
      "Starting Training, the Epoch is:95\n",
      "Starting Training, the Epoch is:96\n",
      "Starting Training, the Epoch is:97\n",
      "Starting Training, the Epoch is:98\n",
      "Starting Training, the Epoch is:99\n",
      "Starting Training, the Epoch is:100\n",
      "Starting Training, the Epoch is:101\n",
      "Starting Training, the Epoch is:102\n",
      "Starting Training, the Epoch is:103\n",
      "Starting Training, the Epoch is:104\n",
      "Starting Training, the Epoch is:105\n",
      "Starting Training, the Epoch is:106\n",
      "Starting Training, the Epoch is:107\n",
      "Starting Training, the Epoch is:108\n",
      "Starting Training, the Epoch is:109\n",
      "Starting Training, the Epoch is:110\n",
      "Starting Training, the Epoch is:111\n",
      "Starting Training, the Epoch is:112\n",
      "Starting Training, the Epoch is:113\n",
      "Starting Training, the Epoch is:114\n",
      "Starting Training, the Epoch is:115\n",
      "Starting Training, the Epoch is:116\n",
      "Starting Training, the Epoch is:117\n",
      "Starting Training, the Epoch is:118\n",
      "Starting Training, the Epoch is:119\n",
      "Starting Training, the Epoch is:120\n",
      "Starting Training, the Epoch is:121\n",
      "Starting Training, the Epoch is:122\n",
      "Starting Training, the Epoch is:123\n",
      "Starting Training, the Epoch is:124\n",
      "Starting Training, the Epoch is:125\n",
      "Starting Training, the Epoch is:126\n",
      "Starting Training, the Epoch is:127\n",
      "Starting Training, the Epoch is:128\n",
      "Starting Training, the Epoch is:129\n",
      "Starting Training, the Epoch is:130\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare batches in the cache\n",
    "# Used for training batch preparation\n",
    "def prepare_randomized_batches(save_dir,batch_size,cutoff_len,file_count):\n",
    "    \n",
    "    # Clean up for rerunning\n",
    "    if (os.path.exists(save_dir + 'inputs/batch/')):\n",
    "        shutil.rmtree(save_dir + 'inputs/batch/')\n",
    "    os.mkdir(save_dir + 'inputs/batch/')\n",
    "        \n",
    "    if (os.path.exists(save_dir + 'labels/batch/')):\n",
    "        shutil.rmtree(save_dir + 'labels/batch/')\n",
    "    os.mkdir(save_dir + 'labels/batch/')\n",
    "        \n",
    "    if (os.path.exists(save_dir + 'one_hot_labels/batch/')):\n",
    "        shutil.rmtree(save_dir + 'one_hot_labels/batch/')\n",
    "    os.mkdir(save_dir + 'one_hot_labels/batch/')\n",
    "    \n",
    "    # For batch preparation\n",
    "    ls_arrays = []\n",
    "    ls_arrays_one_hot_label = []\n",
    "    ls_arrays_label = []\n",
    "    \n",
    "    # A counter\n",
    "    j = 0\n",
    "    \n",
    "    # Randomize the Batch preparation\n",
    "    os.chdir(save_dir + 'inputs/')\n",
    "    \n",
    "    print ('The input directory is:' + save_dir + 'inputs/')\n",
    "    \n",
    "    file_list = glob.glob('*.npy')\n",
    "    random.shuffle(file_list)\n",
    "    \n",
    "    \n",
    "    # Prepare the batches\n",
    "    for file_item in file_list:\n",
    "    \n",
    "        nparr = np.load(save_dir + 'inputs/' + file_item)\n",
    "                \n",
    "        # Make sure it isnt an empty file\n",
    "        if nparr is not None and nparr.shape[0] != 0:\n",
    "                    \n",
    "            # Bottom-Pad  the sentences by adding 0s to the end of the sentence\n",
    "            # Or Truncate sentences that are longer than the cutoff length\n",
    "            \n",
    "            if (nparr.shape[0] <= cutoff_len):\n",
    "                npad = ((0,cutoff_len - nparr.shape[0]),(0,0))\n",
    "                #npad = (0,cutoff_len - nparr.shape[0])\n",
    "                nparr_pad = np.pad(nparr, pad_width=npad, mode='constant', constant_values=0)\n",
    "            else: # Truncate to the cutoff length\n",
    "                #nparr_pad = nparr[:cutoff_len,:]\n",
    "                nparr_pad = nparr[:cutoff_len,:]\n",
    "            \n",
    "            \n",
    "            ls_arrays.append(nparr_pad.tolist())\n",
    "            \n",
    "            one_hot_label = np.load(save_dir + 'one_hot_labels/' + 'one_hot_sentence_label_' + file_item.split('_')[3])\n",
    "            ls_arrays_one_hot_label.append(one_hot_label.tolist())\n",
    "            \n",
    "            label = np.load(save_dir + 'labels/' + 'sentence_label_' + file_item.split('_')[3])\n",
    "            ls_arrays_label.append(label.tolist())\n",
    "            \n",
    "            j += 1\n",
    "                \n",
    "            # Save batches of the files every batch_size step\n",
    "            if (j % batch_size == 0 or j == file_count):\n",
    "                    \n",
    "                nparr_batch = np.asarray(ls_arrays)\n",
    "                nparr_batch_one_hot_labels = np.asarray(ls_arrays_one_hot_label)\n",
    "                nparr_batch_labels = np.asarray(ls_arrays_label)\n",
    "                    \n",
    "                np.save(save_dir + 'inputs/batch/nparr_batch_' + str(j) + '.npy',nparr_batch)\n",
    "                np.save(save_dir + 'labels/batch/nparr_batch_labels_' + str(j) +  '.npy',nparr_batch_labels)\n",
    "                np.save(save_dir + 'one_hot_labels/batch/nparr_batch_one_hot_labels_' + str(j) +'.npy',nparr_batch_one_hot_labels)\n",
    "                    \n",
    "                ls_arrays = []\n",
    "                ls_arrays_one_hot_label = []\n",
    "                ls_arrays_label = []\n",
    "                \n",
    "            if (j == file_count):\n",
    "                print ('breaking afetr processing files:' + str(j))\n",
    "                break\n",
    "                \n",
    "                \n",
    "# Gets a random batch of a specified size\n",
    "# This function is not used in the final model\n",
    "# But can be used for mini-batch Gradient Descent \n",
    "def get_a_random_batch(save_dir,batch_size,cutoff_len, file_list):\n",
    "\n",
    "        ls_batch_list = [] # Stores the names of all the files randomly selected without replacement\n",
    "        \n",
    "        '''\n",
    "        i = 0\n",
    "        while (i < batch_size):\n",
    "            \n",
    "            rand_choice = random.choice(os.listdir(save_dir + 'inputs/'))\n",
    "            if rand_choice not in ls_batch_list:\n",
    "                ls_batch_list.append(rand_choice)\n",
    "                i = i + 1\n",
    "        '''\n",
    "        ls_batch_list = random.sample(file_list,batch_size)\n",
    "        \n",
    "        ls_arrays = []\n",
    "        ls_arrays_one_hot_label = []\n",
    "        ls_arrays_label = []\n",
    "        \n",
    "        #print ('The list length is:' + str(len(ls_batch_list)))\n",
    "        \n",
    "        for item in ls_batch_list:\n",
    "            \n",
    "            nparr = np.load(save_dir + 'inputs/' + item)\n",
    "            if nparr is not None and nparr.shape[0] != 0:\n",
    "                \n",
    "                #Bottom-Pad  the sentences by adding 0s to the end of the sentence\n",
    "                # Or truncate the sentence upto the length\n",
    "                if (nparr.shape[0] <= cutoff_len):\n",
    "                    npad = ((0,cutoff_len - nparr.shape[0]),(0,0))\n",
    "                    #npad = (0,cutoff_len - nparr.shape[0])\n",
    "                    nparr_pad = np.pad(nparr, pad_width=npad, mode='constant', constant_values=0)\n",
    "                else:\n",
    "                    nparr_pad = nparr[:cutoff_len,:]\n",
    "                \n",
    "                ls_arrays.append(nparr_pad.tolist())\n",
    "                \n",
    "                one_hot_label = np.load(save_dir + 'one_hot_labels/' + 'one_hot_sentence_label_' + item.split('_')[3])\n",
    "                ls_arrays_one_hot_label.append(one_hot_label.tolist())\n",
    "                \n",
    "                label = np.load(save_dir + 'labels/' + 'sentence_label_' + item.split('_')[3])\n",
    "                ls_arrays_label.append(label.tolist())\n",
    "                \n",
    "                \n",
    "        nparr_batch = np.asarray(ls_arrays)\n",
    "        nparr_batch_one_hot_labels = np.asarray(ls_arrays_one_hot_label)\n",
    "        nparr_batch_labels = np.asarray(ls_arrays_label)\n",
    "        \n",
    "        return nparr_batch, nparr_batch_labels, nparr_batch_one_hot_labels\n",
    "              \n",
    "\n",
    "        \n",
    "'''\n",
    "The main training function below. Trains the data using mini-batch gradient descent, \n",
    "and saves checkpoints of the model \n",
    "'''    \n",
    "def execute_training():\n",
    "    \n",
    "    print ('Starting the training process')\n",
    "    \n",
    "    num_epochs = 5000\n",
    "    mini_batch_size = 200\n",
    "    batch_size = 200\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    mini_batch_runs = 100\n",
    "    \n",
    "    if (os.path.exists(train_tensorboard_dir)):\n",
    "        shutil.rmtree(train_tensorboard_dir)\n",
    "    os.mkdir(train_tensorboard_dir)\n",
    "    \n",
    "    \n",
    "    if (os.path.exists(valid_tensorboard_dir)):\n",
    "        shutil.rmtree(valid_tensorboard_dir)\n",
    "    os.mkdir(valid_tensorboard_dir)\n",
    "    \n",
    "    print ('Building the graph')\n",
    "    \n",
    "    # Build graph object\n",
    "    with tf.Graph().as_default() as grap:\n",
    "        #input_tensor, batch_norm_train, logits, softmax_logits,conv_keep_prob,fc_keep_prob = build_graph(cutoff_len)\n",
    "        \n",
    "        input_tensor, batch_norm_train, logits, softmax_logits, conv_keep_prob,fc_keep_prob = build_graph_convnets(cutoff_len)\n",
    "            \n",
    "        train_step, confusion_matrix,predicted_indices,cross_entropy_mean, learning_rate_input,labels,one_hot_labels,loss =  build_loss_optimizer(logits,softmax_logits,num_labels)\n",
    "        \n",
    "            \n",
    "    print ('Kicking off the session')\n",
    "    \n",
    "    # Initiate session for execution\n",
    "    with tf.Session(graph=grap) as sess:\n",
    "        \n",
    "        saver = tf.train.Saver() # For saving checkpoints for test inference \n",
    "        \n",
    "        # Initialize\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        #sess.run(reset_metrics_vars)\n",
    "        \n",
    "        train_writer = tf.summary.FileWriter(train_tensorboard_dir,sess.graph)\n",
    "        valid_writer = tf.summary.FileWriter(valid_tensorboard_dir)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Prepares randomized batches in the /inputs/batch folder\n",
    "        #prepare_randomized_batches(one_hot_train_dir,train_batch_size,cutoff_len,train_count)\n",
    "        \n",
    "        print ('Preparing randomized validation batches')\n",
    "        valid_count = 20000\n",
    "        prepare_randomized_batches(one_hot_valid_dir,batch_size,cutoff_len,valid_count)\n",
    "        \n",
    "        xent_counter = 0\n",
    "        \n",
    "        _guid = uuid.uuid4()\n",
    "        \n",
    "        with open (home_dir + 'train_npy.txt','r') as d:\n",
    "            train_file_list = [line.replace('\\n','') for line in d]\n",
    "            print (train_file_list[0])\n",
    "            print (len(train_file_list))\n",
    "        \n",
    "        for i in range(0,num_epochs):\n",
    "            \n",
    "            print ('Starting Training, the Epoch is:' + str(i + 1))\n",
    "            xent_summary = 0\n",
    "            \n",
    "            # Switch to the /inputs/batch folder and fetch all the batch files \n",
    "            # Which will be passed to the training mechanism\n",
    "            #os.chdir(one_hot_train_dir + 'inputs/batch/')\n",
    "            #train_batch_files = glob.glob('*.npy')\n",
    "            #random.shuffle(train_batch_files)\n",
    "            \n",
    "            train_conf_matrix = None\n",
    "            \n",
    "            #for file in train_batch_files:\n",
    "            for j in range(0,mini_batch_runs): #mini batch runs per epoch\n",
    "                \n",
    "                \n",
    "                \n",
    "                #print ('Get a random training batch:'  + str(j + 1))\n",
    "                train_batch, train_batch_labels, train_batch_one_hot = get_a_random_batch(one_hot_train_dir,mini_batch_size,cutoff_len,train_file_list)\n",
    "                #print (train_batch.shape)\n",
    "                #print (train_batch_one_hot)\n",
    "                #print (train_batch_labels)\n",
    "                \n",
    "                #train_batch = np.load(one_hot_train_dir + 'inputs/batch/' + file)\n",
    "                #train_labels = np.load(one_hot_train_dir + 'labels/batch/nparr_batch_labels_' + file.split('_')[2])\n",
    "                #train_one_hot_labels = np.load(one_hot_train_dir + 'one_hot_labels/batch/nparr_batch_one_hot_labels_' + file.split('_')[2])\n",
    "                \n",
    "                # Kick off the training\n",
    "                _,los,conf_matrix,xent_mean,pi = sess.run(\n",
    "                [train_step,loss,confusion_matrix,cross_entropy_mean,predicted_indices],\n",
    "                feed_dict = {input_tensor : train_batch,\n",
    "                             labels: train_batch_labels,\n",
    "                             one_hot_labels : train_batch_one_hot,\n",
    "                             learning_rate_input : learning_rate,\n",
    "                             batch_norm_train : True,\n",
    "                             conv_keep_prob : 1.0,\n",
    "                             fc_keep_prob : 0.6\n",
    "                    })\n",
    "                \n",
    "                if (train_conf_matrix is None):\n",
    "                    train_conf_matrix = conf_matrix\n",
    "                else:\n",
    "                    train_conf_matrix += conf_matrix\n",
    "            \n",
    "                \n",
    "                xent_counter += 1\n",
    "                xent_summary += np.sum(xent_mean)\n",
    "                #train_writer.add_summary(xent_scalar,xent_counter)\n",
    "                with open (log_dir + str(_guid) + '.txt','a') as eg:\n",
    "                    eg.write('The training cross entropy sum and avg at step:' + str(xent_counter) + ' is:')\n",
    "                    eg.write(str(np.sum(xent_mean)) + ';')\n",
    "                    eg.write(str(los) + '\\n')\n",
    "                    \n",
    "                xent_train_summary = tf.Summary(value=[tf.Summary.Value(tag=\"cross_entropy_sum\",simple_value=np.sum(xent_mean))])\n",
    "                #xent_train_summary.value.add(tag=\"cross_entropy_sum\",simple_value = np.sum(xent_mean))\n",
    "                train_writer.add_summary(xent_train_summary,xent_counter)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            # Print confusion matrix out\n",
    "            with open (log_dir + str(_guid) + '.txt','a') as eg:\n",
    "                eg.write('\\n' + 'Training Confusion Matrix:' + '\\n' + str(train_conf_matrix))\n",
    "                true_pos = np.sum(np.diag(train_conf_matrix))\n",
    "                all_pos = np.sum(train_conf_matrix)\n",
    "                eg.write('\\n' + 'Training Accuracy is: ' + str(float(true_pos / all_pos)) + '\\n')  # Another way to get accuracy!\n",
    "                eg.write('Average cross entropy error is:' + str(xent_summary/(mini_batch_runs * mini_batch_size)) + '\\n')\n",
    "            \n",
    "            loss_train_summary = tf.Summary(value=[tf.Summary.Value(tag=\"loss_train_summary\",simple_value=xent_summary/(mini_batch_runs * mini_batch_size))])\n",
    "                \n",
    "            \n",
    "            #loss_train_summary.value.add(tag=\"loss_train\",simple_value = xent_summary/(mini_batch_runs * mini_batch_size))\n",
    "            \n",
    "            acc_train_summary = tf.Summary(value=[tf.Summary.Value(tag=\"acc_train_summary\",simple_value=float(true_pos / all_pos))])\n",
    "            \n",
    "            #acc_train_summary.value.add(tag=\"train_accuracy\",simple_value = float(true_pos / all_pos))\n",
    "            \n",
    "            train_writer.add_summary(loss_train_summary,i)\n",
    "            train_writer.add_summary(acc_train_summary,i)\n",
    "            \n",
    "            #_,scalar = sess.run([accuracy,acc_scalar])\n",
    "            #train_writer.add_summary(scalar,i)\n",
    "            \n",
    "            #sess.run(reset_metrics_vars)\n",
    "            \n",
    "            \n",
    "            # Save model every 10 epochs\n",
    "            if ((i + 1) % 10 == 0):\n",
    "                #print ('Saving checkpoint after epoch:' + str(i + 1))\n",
    "                saver.save(sess=sess,save_path=checkpoint_dir + 'Char_Sent_Classification_CNN.ckpt',global_step = i )\n",
    "                \n",
    "            #if ((i + 1) % 20 == 0):\n",
    "            # Run on validation data to check validation accuracy\n",
    "            #print ('Training Epoch ' + str(i + 1) + ' is complete. Running Validation Stats..')\n",
    "             \n",
    "            os.chdir(one_hot_valid_dir + 'inputs/batch/')\n",
    "            valid_batch_files = glob.glob('*.npy')\n",
    "            random.shuffle(valid_batch_files)\n",
    "            \n",
    "            valid_conf_matrix = None\n",
    "            \n",
    "            for file in valid_batch_files:\n",
    "                \n",
    "                valid_batch = np.load(one_hot_valid_dir + 'inputs/batch/' + file)\n",
    "                valid_labels = np.load(one_hot_valid_dir + 'labels/batch/nparr_batch_labels_' + file.split('_')[2])\n",
    "                valid_one_hot_labels = np.load(one_hot_valid_dir + 'one_hot_labels/batch/nparr_batch_one_hot_labels_' + file.split('_')[2])\n",
    "               \n",
    "                \n",
    "                val_conf_matrix = sess.run(\n",
    "                confusion_matrix,\n",
    "                feed_dict = {input_tensor : valid_batch,\n",
    "                            labels: valid_labels,\n",
    "                            one_hot_labels : valid_one_hot_labels,\n",
    "                            batch_norm_train : False,\n",
    "                            conv_keep_prob : 1.0,\n",
    "                            fc_keep_prob : 1.0\n",
    "                         \n",
    "                })\n",
    "                \n",
    "                if (valid_conf_matrix is None):\n",
    "                    valid_conf_matrix = val_conf_matrix\n",
    "                else:\n",
    "                    valid_conf_matrix += val_conf_matrix\n",
    "                    \n",
    "            \n",
    "            #val_acc,val_summary = sess.run([accuracy,acc_scalar])\n",
    "            #valid_writer.add_summary(val_summary,i)\n",
    "            \n",
    "            #sess.run(reset_metrics_vars)\n",
    "            \n",
    "            with open (log_dir + str(_guid) + '.txt','a') as eg:\n",
    "                eg.write('Validation Confusion Matrix:' + '\\n' + str(valid_conf_matrix))\n",
    "                true_pos = np.sum(np.diag(valid_conf_matrix))\n",
    "                all_pos = np.sum(valid_conf_matrix)\n",
    "                eg.write('\\n' + 'Validation Accuracy is: ' + str(float(true_pos / all_pos)) + '\\n') \n",
    "\n",
    "                \n",
    "            acc_valid_summary = tf.Summary(value=[tf.Summary.Value(tag=\"acc_valid_summary\",simple_value=float(true_pos / all_pos))])\n",
    "            \n",
    "            #acc_valid_summary.value.add(tag=\"train_accuracy\",simple_value = float(true_pos / all_pos))\n",
    "            valid_writer.add_summary(acc_valid_summary,i)\n",
    "            \n",
    "            \n",
    "            \n",
    "execute_training()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the saved checkpoints to do inference on the batch of test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(cutoff_len):\n",
    "    \n",
    "    with tf.Graph().as_default() as grap:\n",
    "        input_tensor, batch_norm_train, logits, softmax_logits,conv_keep_prob,fc_keep_prob = build_graph(cutoff_len)\n",
    "        \n",
    "    with open(one_hot_test_dir + 'ytest.txt','w') as predfile:\n",
    "    \n",
    "        with tf.Session(graph=grap) as sess:\n",
    "        \n",
    "            init = tf.global_variables_initializer()\n",
    "            sess = tf.Session()\n",
    "            sess.run(init)\n",
    "        \n",
    "            saver = tf.train.Saver()\n",
    "        \n",
    "            checkpoint_file_path = '/home/ubuntu/Desktop/challenge/offline_challenge_to_send/offline_challenge_to_send/checkpoints/Char_Sent_Classification_CNN.ckpt-99'\n",
    "\n",
    "            print ('Loading checkpoint file:' + checkpoint_file_path)\n",
    "            saver.restore(sess,checkpoint_file_path)\n",
    "\n",
    "            os.chdir(one_hot_test_dir + 'inputs/')\n",
    "            test_files = glob.glob('*.npy')\n",
    "               \n",
    "            for file in test_files:\n",
    "            \n",
    "                _idx = file.split('_')[3].replace('.npy','')\n",
    "            \n",
    "            \n",
    "                nparr = np.load(one_hot_test_dir + 'inputs/' + file)\n",
    "                npad = ((0,cutoff_len - nparr.shape[0]),(0,0))\n",
    "                nparr_pad = np.pad(nparr, pad_width=npad, mode='constant', constant_values=0)\n",
    "            \n",
    "                nparr_2 = np.expand_dims(nparr_pad,axis=0)\n",
    "             \n",
    "                predictions,soft = sess.run(\n",
    "                [\n",
    "                    logits,softmax_logits\n",
    "                ],feed_dict = {input_tensor : nparr_2,\n",
    "                                batch_norm_train : False,\n",
    "                                conv_keep_prob : 1.0,\n",
    "                                fc_keep_prob : 1.0})\n",
    "                \n",
    "                predicted_index = tf.argmax(soft, axis=1)\n",
    "                \n",
    "                predfile.write(str(_idx,) + ',' + str(predicted_index.eval(session=sess)[0]) + '\\n')\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "#inference(cutoff_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def helper():\n",
    "    all_files = os.listdir(one_hot_train_dir + 'inputs/')\n",
    "\n",
    "    with open (home_dir + 'train_npy.txt','w') as tnpy:\n",
    "        for row in all_files:\n",
    "            tnpy.write(row + '\\n')\n",
    "            \n",
    "helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APPENDIX below: \n",
    "\n",
    "This is an alternative model based off Text Understanding From Scratch by Xiang ZHang and Yann LeCun. It is not used in the final model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
