{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Desktop/tensorflow-gpu/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/Desktop/tensorflow-gpu/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    }
   ],
   "source": [
    "import os, random , math, shutil, glob\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Initialize all the Global Variables for Train, Validation, and Test Data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "''' Below are all the \"global\" constants used in the model (not hyperparameters or graph parameters)'''\n",
    "\n",
    "train_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/train'\n",
    "valid_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/valid'\n",
    "test_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/test'\n",
    "\n",
    "# Stores all the checkpoint models\n",
    "checkpoint_dir = '/home/ubuntu/Desktop/challenge/offline_challenge_to_send/offline_challenge_to_send/checkpoints/'\n",
    "\n",
    "# Index denotes the column placement in the character embedding matrix\n",
    "# This is used to create the one-hot character vector of sentence inputs\n",
    "one_hot_column_label = {'a':0,'b':1,'c':2,'d':3,'e':4,'f':5,'g':6,'h':7,'i':8,'j':9,'k':10,'l':11,'m':12,'n':13,'o':14,\n",
    "                        'p':15,'q':16,'r':17,'s':18,'t':19,'u':20,'v':21,'w':22,'x':23,'y':24,'z':25,'A':26,'B':27,'C':28,'D':29,'E':30,'F':31,\n",
    "                        'G':32,'H':33,'I':34,'J':35,'K':36,'L':37,'M':38,'N':39,'O':40,'P':41,'Q':42,'R':43,'S':44,'T':45,\n",
    "                        'U':46,'V':47,'W':48,'X':49,'Y':50,'Z':51,'0':52,'1':53,'2':54,'3':55,'4':56,'5':57,'6':58,'7':59,\n",
    "                        '8':60,'9':61,'-':62,',':63,';':64,'.':65,'!':66,'?':67,':':68,'\"':69,'\\'':70,'/':71,'\\\\':72,'|':73,'_':74,\n",
    "                        '@':75,'#':76,'$':77,'%':78,'^':79,'&':80,'*':81,'~':82,'`':83,'+':84,'-':85,'=':86,'<':87,'>':88,'(':89,')':90,'[':91,\n",
    "                        ']':92,'{':93,'}':94}\n",
    "\n",
    "# Number of labels to classify\n",
    "num_labels = 5\n",
    "\n",
    "# Test Data Parameters\n",
    "X_test = pd.read_csv('/home/ubuntu/Desktop/challenge/offline_challenge_to_send/offline_challenge_to_send/xtest_obfuscated.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a 70-30 train-validation split. Get the length of the longest sentence, which will be used to build the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The longest sentence is of length:452\n",
      "The shortest sentence is of length:168\n",
      "The average length is:413.3893519515271\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combine and label before train / valid split\n",
    "full_train_file = pd.concat([train_file,label_file],axis=1)\n",
    "full_train_file['novel'] = full_train_file['Label'].map(label_story) \n",
    "\n",
    "\n",
    "# We get the max_len for padding and for the convolution filter definition in the model\n",
    "max_len = 0\n",
    "total_len = 0\n",
    "min_len = 999\n",
    "for index,row in full_train_file.iterrows():\n",
    "    total_len += len(row[0])\n",
    "    if (max_len < len(row[0])):\n",
    "        max_len = len(row[0])\n",
    "    if (min_len > len(row[0])):\n",
    "        min_len = len(row[0])\n",
    "        \n",
    "\n",
    "print ('The longest sentence is of length:' + str(max_len))\n",
    "print ('The shortest sentence is of length:' + str(min_len))\n",
    "print ('The average length is:' + str(total_len/full_train_file.shape[0]))\n",
    "\n",
    "# Train and test split (stratified sampling, 70-30 split)\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(full_train_file,label_file,test_size=0.3,train_size=0.7,random_state=13814,shuffle=True,stratify=label_file)\n",
    "\n",
    "# Verify that stratified sampling worked (histogram distributions must be same)\n",
    "X_train.hist(column=\"Label\") # First histogram is training\n",
    "X_valid.hist(column=\"Label\") # Second histogram is validation\n",
    "\n",
    "\n",
    "'Some more \"global\" variables'\n",
    "cutoff_len = max_len\n",
    "train_count = X_train.shape[0]\n",
    "valid_count = X_valid.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert each sentence to one-hot character matrix to drive the non-static character embedding matrix creation, and store this character matrix representation in disk cache.\n",
    "\n",
    "Also store in disk cache, the label and a one-hot representation of the label, for the cross-entropy loss.\n",
    "\n",
    "NOTE that an new column ID was added to the xtest_obfuscated.txt file, to ID each sentence in the file. This is the '_id' variable below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Creates the one-hot character vector representation of sentences which will be used as the layer before a\n",
    "# character-embedding matrix\n",
    "def save_numpy_one_hot(X_dataset,unique_char_size,save_dir,one_hot_column_label):\n",
    "    \n",
    "    for index,row in X_dataset.iterrows():\n",
    "        \n",
    "        sent = row[0]\n",
    "        np_one_hot = np.zeros(shape=(len(sent),unique_char_size),dtype=np.int32)\n",
    "        \n",
    "        for i in range(0,len(sent)):\n",
    "            np_one_hot[i][one_hot_column_label[sent[i]]] = 1\n",
    "            \n",
    "        # Save the one-hot encoding for each sentence \n",
    "        np.save(save_dir + 'inputs/' + 'one_hot_sentence_' + str(index) + '.npy',np_one_hot)\n",
    "        \n",
    "        #Save the label\n",
    "        np.save(save_dir + 'labels/' +  'sentence_label_' + str(index) + '.npy',np.asarray(row[1]))\n",
    "        \n",
    "        # Save a one-hot version of the label (for xentropy loss function)\n",
    "        np_one_hot_label = np.zeros(shape=(12),dtype=np.int32)\n",
    "        np_one_hot_label[row[1]] = 1\n",
    "        \n",
    "        np.save(save_dir + 'one_hot_labels/' + 'one_hot_sentence_label_' + str(index) + '.npy',np_one_hot_label)\n",
    "\n",
    "def save_numpy_one_hot_test(X_dataset,unique_char_size,save_dir,one_hot_column_label):\n",
    "    \n",
    "    for index,row in X_dataset.iterrows():\n",
    "        \n",
    "        sent = row[0]\n",
    "        _id = row[1]\n",
    "        np_one_hot = np.zeros(shape=(len(sent),unique_char_size),dtype=np.int32)\n",
    "        \n",
    "        for i in range(0,len(sent)):\n",
    "            np_one_hot[i][one_hot_column_label[sent[i]]] = 1\n",
    "            \n",
    "        # Save the one-hot encoding for each sentence \n",
    "        np.save(save_dir + 'inputs/' + 'one_hot_sentence_' + str(_id) + '.npy',np_one_hot)\n",
    "        \n",
    "def inp_create_one_hot_char_mat(unique_char_size,train_dir,valid_dir,test_dir):\n",
    "    \n",
    "    \"\"\"\n",
    "    char_embedding_n_dim: for one-hot matrix, the number of characters in the corpus (assumed as 26 for this challenge, given the data)\n",
    "    train_dir: directory for all training cache files\n",
    "    valid_dir: directory for all validation cache files\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    # This will create a one-hot char representation for each sentence in the train and valid files\n",
    "    save_numpy_one_hot(X_train,unique_char_size,train_dir,one_hot_column_label)\n",
    "    save_numpy_one_hot(X_valid,unique_char_size,valid_dir,one_hot_column_label)\n",
    "    save_numpy_one_hot_test(X_test,unique_char_size,test_dir,one_hot_column_label)\n",
    "    \n",
    "\n",
    "\n",
    "# One  - off folder creation \n",
    "if (not os.path.exists(one_hot_train_dir)):\n",
    "        print ('Setting up directories and preparing one-hot caracter vectors')\n",
    "        os.mkdir(one_hot_train_dir)\n",
    "        os.mkdir(one_hot_valid_dir)\n",
    "        os.mkdir(one_hot_test_dir)\n",
    "        os.mkdir(one_hot_train_dir + 'inputs/')\n",
    "        os.mkdir(one_hot_valid_dir + 'inputs/')\n",
    "        os.mkdir(one_hot_test_dir + 'inputs/')\n",
    "        os.mkdir(one_hot_train_dir + 'labels/')\n",
    "        os.mkdir(one_hot_valid_dir + 'labels/')\n",
    "        os.mkdir(one_hot_train_dir + 'one_hot_labels/')\n",
    "        os.mkdir(one_hot_valid_dir + 'one_hot_labels/')\n",
    "        os.mkdir(one_hot_train_dir + 'tensorboard/')\n",
    "        os.mkdir(one_hot_valid_dir + 'tensorboard/')\n",
    "        inp_create_one_hot_char_mat(26,one_hot_train_dir,one_hot_valid_dir,one_hot_test_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build the graph and defined the loss and optimizer operations.\n",
    "\n",
    "The graph is based off the following paper:\n",
    "eXpose: A Character-Level Convolutional Neural Network with Embeddings For Detecting Malicious URLs, File Paths and Registry Keys by Joshua Saxe, Konstantin Berlin\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Start to build the model\n",
    "# Let's build the graph first\n",
    "def build_graph(cutoff_len):\n",
    "    \n",
    "    # Hyper Parameters with descriptions\n",
    "    num_char_dict = 26 # The size of the character dictionary. Assumed 26, for this dataset.\n",
    "    char_embedding_n_dim = 32 # The number of columns in the character embedding matrix\n",
    "    \n",
    "    first_window_size = 2 # First window applied to sentence\n",
    "    second_window_size = 3 # Second window\n",
    "    third_window_size = 4 # And so on...\n",
    "    fourth_window_size = 5\n",
    "    \n",
    "    \n",
    "    first_layer_no_filters = 256\n",
    "    second_layer_no_filters = 256\n",
    "    third_layer_no_filters = 256\n",
    "    fourth_layer_no_filters = 256\n",
    "    \n",
    "    first_fc_out = 1024\n",
    "    second_fc_out = 1024\n",
    "    third_fc_out = 1024\n",
    "    # Hyper parameters end \n",
    "    \n",
    "    reuse_flag = tf.AUTO_REUSE\n",
    "    \n",
    "    # Build the graph\n",
    "    with tf.variable_scope('layer_one_char_embedding',reuse=reuse_flag):\n",
    "        \n",
    "        batch_norm_train = tf.placeholder(name=\"batch_norm_train\", dtype=tf.bool)\n",
    "        \n",
    "        '''This layer defines the character embedding matrix, and returns character embedding for batches of sentences'''\n",
    "        \n",
    "        # The input_tensor is of shape [None, cutoff_len, num_char_dict] where cutoff_len is the length limit for the 2-D embedding matrix\n",
    "        input_tensor = tf.placeholder(dtype=tf.float32,shape=[None,cutoff_len,num_char_dict],name=\"input_tensor\") \n",
    "        \n",
    "        # Initializer for embedding matrix\n",
    "        l1w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        # The character embedding matrix declared below. The 2nd rank is the number of dimensions representing each character\n",
    "        char_embedding_mat = tf.get_variable(name=\"char_embedding\",dtype=tf.float32,shape=[num_char_dict,char_embedding_n_dim],initializer=l1w_init)\n",
    "        \n",
    "        input_tensor_2D = tf.reshape(input_tensor,shape=[-1,num_char_dict]) \n",
    "        l1_char_embedding = tf.matmul(input_tensor_2D,char_embedding_mat)\n",
    "        \n",
    "        # reshape char embedding matrix to 3D matrix for the temporal 1D convolution operation\n",
    "        input_tensor_with_embed = tf.reshape(l1_char_embedding,shape=[-1,cutoff_len,char_embedding_n_dim])\n",
    "        \n",
    "        # Some keep probabilities for adding dropout\n",
    "        conv_keep_prob = tf.placeholder(dtype=tf.float32,shape=[],name=\"conv_dropout_prob\")\n",
    "        fc_keep_prob = tf.placeholder(dtype=tf.float32,shape=[],name=\"fc_dropout_prob\")\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('layer_two_kernels',reuse=reuse_flag):\n",
    "        \n",
    "        l2w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        \n",
    "        # 1-D convolution layer\n",
    "        first_conv_layer = tf.layers.conv1d(inputs = input_tensor_with_embed, filters = first_layer_no_filters,kernel_size = first_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"first_conv_layer\")\n",
    "        # Batch normalization\n",
    "        first_conv_batch = tf.contrib.layers.batch_norm(first_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        \n",
    "        # Pooling Layer\n",
    "        first_sumpool_layer = tf.reduce_sum(first_conv_batch, axis=1,keepdims=False) # Sums across the 'length' \n",
    "        \n",
    "        # Dropout\n",
    "        first_layer = tf.nn.dropout(first_sumpool_layer,keep_prob = conv_keep_prob )\n",
    "        \n",
    "        \n",
    "        # Repeat for second stack\n",
    "        second_conv_layer = tf.layers.conv1d(inputs = input_tensor_with_embed, filters = second_layer_no_filters,kernel_size = second_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(), name=\"second_conv_layer\")\n",
    "        second_conv_batch = tf.contrib.layers.batch_norm(second_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        second_sumpool_layer = tf.reduce_sum(second_conv_batch, axis=1,keepdims=False) # Sums across the 'length' \n",
    "        \n",
    "        second_layer = tf.nn.dropout(second_sumpool_layer,keep_prob = conv_keep_prob )\n",
    "        \n",
    "        \n",
    "        #...and third stack...\n",
    "        third_conv_layer = tf.layers.conv1d(inputs = input_tensor_with_embed, filters = third_layer_no_filters,kernel_size = third_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(), name=\"third_conv_layer\")\n",
    "        third_conv_batch = tf.contrib.layers.batch_norm(third_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        third_sumpool_layer = tf.reduce_sum(third_conv_batch, axis=1,keepdims=False) # Sums across the 'length' \n",
    "        \n",
    "        \n",
    "        third_layer = tf.nn.dropout(third_sumpool_layer,keep_prob = conv_keep_prob )\n",
    "        \n",
    "        \n",
    "        #..and fourth stack..\n",
    "        fourth_conv_layer = tf.layers.conv1d(inputs = input_tensor_with_embed, filters = fourth_layer_no_filters,kernel_size = fourth_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(), name=\"fourth_conv_layer\")\n",
    "        fourth_conv_batch = tf.contrib.layers.batch_norm(fourth_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        fourth_sumpool_layer = tf.reduce_sum(fourth_conv_batch, axis=1,keepdims=False) # Sums across the 'length' \n",
    "        fourth_layer = tf.nn.dropout(fourth_sumpool_layer,keep_prob = conv_keep_prob )\n",
    "        \n",
    "        all_conv = tf.concat(axis=1,values=[first_layer,second_layer,third_layer,fourth_layer],\n",
    "                                            #fifth_conv_dropout,sixth_conv_dropout,seventh_conv_dropout],\n",
    "                                            name=\"all_conv\")\n",
    "        \n",
    "        print ('Stacked Convolution Tensor as below')\n",
    "        print(all_conv)\n",
    "        \n",
    "        \n",
    "    # Now pass it through three FC layers\n",
    "    with tf.variable_scope('layer_three_fc',reuse=reuse_flag):\n",
    "       \n",
    "        l3w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        l3b_init = tf.random_normal_initializer(mean=0, stddev=0.1, dtype=tf.float32)\n",
    "        \n",
    "        first_fc = tf.contrib.layers.fully_connected(inputs=all_conv,num_outputs=first_fc_out,activation_fn = tf.nn.relu,\n",
    "                                                    weights_initializer = l3w_init,biases_initializer = l3b_init,scope=\"first_fc\")\n",
    "        \n",
    "        first_fc_batch = tf.contrib.layers.batch_norm(first_fc,center = True, scale=False,is_training=batch_norm_train)\n",
    "        first_fc_final = tf.nn.dropout(first_fc_batch,keep_prob = fc_keep_prob) \n",
    "        \n",
    "    \n",
    "    with tf.variable_scope('layer_four_fc',reuse=reuse_flag):\n",
    "        l4w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        l4b_init = tf.random_normal_initializer(mean=0, stddev=0.1, dtype=tf.float32)\n",
    "        \n",
    "        second_fc = tf.contrib.layers.fully_connected(inputs=first_fc_final,num_outputs=second_fc_out,activation_fn = tf.nn.relu,\n",
    "                                                    weights_initializer = l4w_init,biases_initializer = l4b_init,scope=\"second_fc\")\n",
    "        second_fc_batch = tf.contrib.layers.batch_norm(second_fc,center = True, scale=False,is_training=batch_norm_train)\n",
    "        second_fc_final = tf.nn.dropout(second_fc_batch,keep_prob = fc_keep_prob)\n",
    "        \n",
    "    # Third FC layer and softmax\n",
    "    with tf.variable_scope('layer_five_fc',reuse=reuse_flag):\n",
    "        l5w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        l5b_init = tf.random_normal_initializer(mean=0, stddev=0.1, dtype=tf.float32)\n",
    "        \n",
    "        third_fc = tf.contrib.layers.fully_connected(inputs = second_fc_final,num_outputs=num_labels,weights_initializer=l5w_init,\n",
    "                                                    biases_initializer = l5b_init,activation_fn=tf.nn.relu,scope=\"third_fc\")\n",
    "        \n",
    "        third_fc_batch = tf.contrib.layers.batch_norm(third_fc,center=True,scale=False,is_training=batch_norm_train)\n",
    "        third_fc_final = tf.nn.dropout(third_fc_batch,keep_prob = fc_keep_prob)\n",
    "        \n",
    "    with tf.variable_scope('layer_six_softmax',reuse=reuse_flag):\n",
    "        softmax_logits = tf.nn.softmax(logits=third_fc_final,name=\"final_logits_softmax\")\n",
    "        \n",
    "    return input_tensor, batch_norm_train, third_fc_final,softmax_logits, conv_keep_prob,fc_keep_prob\n",
    "        \n",
    "\n",
    "\n",
    "def build_loss_optimizer(logits,softmax_logits,num_labels):\n",
    "    \n",
    "    ''' Loss and Optimizer builder, with Softmax Cross Entropy loss'''\n",
    "    \n",
    "    with tf.variable_scope('cross_entropy'):\n",
    "        \n",
    "        learning_rate_input = tf.placeholder(\n",
    "            tf.float32, [], name='learning_rate_input')\n",
    "        one_hot_labels = tf.placeholder(\n",
    "            name=\"one_hot_labels\",shape=[None,num_labels],dtype=tf.int32)\n",
    "        labels = tf.placeholder(\n",
    "            name=\"labels\",shape=[None],dtype=tf.int64)\n",
    "        \n",
    "        cross_entropy_mean = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels,logits=softmax_logits)\n",
    "        \n",
    "        'Toggle between optimizers..'\n",
    "        #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_input)\n",
    "        #optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate_input)\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate_input)\n",
    "        \n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # For batch normalization ops update\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            train_step = optimizer.minimize(cross_entropy_mean)\n",
    "        \n",
    "        predicted_indices = tf.argmax(softmax_logits, 1, name=\"predicted_indices\")\n",
    "        correct_prediction = tf.equal(predicted_indices, labels, name='correct_prediction') # Labels are 0-indexed\n",
    "        confusion_matrix = tf.confusion_matrix(\n",
    "            labels, predicted_indices, num_classes=num_labels, name=\"confusion_matrix\")\n",
    "        \n",
    "        # Because the accuracy will be calculated across batch splits, this facilitates resetting the metrics inter-epochs\n",
    "        #with tf.variable_scope('streaming_ops'):\n",
    "        #    accuracy,update_op_acc = tf.metrics.accuracy(labels,predicted_indices)\n",
    "            \n",
    "        #metrics_vars = tf.contrib.framework.get_variables('cross_entropy/streaming_ops',collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "        #reset_metrics_vars = tf.variables_initializer(metrics_vars) # Running sess.run will reset the streaming metrics within epochs\n",
    "        \n",
    "        # For tensorboard \n",
    "        #xent_mean_scalar= tf.summary.scalar('cross_entropy_mean',cross_entropy_mean)\n",
    "        #acc_scalar = tf.summary.scalar('accuracy',accuracy)\n",
    "        \n",
    "        return train_step, confusion_matrix,predicted_indices,cross_entropy_mean, learning_rate_input,one_hot_labels,labels\n",
    "               #reset_metrics_vars, accuracy,update_op_acc, xent_mean_scalar, acc_scalar, \\\n",
    "               \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the graph is built, randomly batch the inputs from disk cache, and feed them to the graph to train a model.\n",
    "\n",
    "Training takes between 30-45 min on a Tesla M60 GPU from an AWS g3 EC2 instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Prepare batches in the cache\n",
    "# Used for training batch preparation\n",
    "def prepare_randomized_batches(save_dir,batch_size,cutoff_len,file_count):\n",
    "    \n",
    "    # Clean up for rerunning\n",
    "    if (os.path.exists(save_dir + 'inputs/batch/')):\n",
    "        shutil.rmtree(save_dir + 'inputs/batch/')\n",
    "    os.mkdir(save_dir + 'inputs/batch/')\n",
    "        \n",
    "    if (os.path.exists(save_dir + 'labels/batch/')):\n",
    "        shutil.rmtree(save_dir + 'labels/batch/')\n",
    "    os.mkdir(save_dir + 'labels/batch/')\n",
    "        \n",
    "    if (os.path.exists(save_dir + 'one_hot_labels/batch/')):\n",
    "        shutil.rmtree(save_dir + 'one_hot_labels/batch/')\n",
    "    os.mkdir(save_dir + 'one_hot_labels/batch/')\n",
    "    \n",
    "    # For batch preparation\n",
    "    ls_arrays = []\n",
    "    ls_arrays_one_hot_label = []\n",
    "    ls_arrays_label = []\n",
    "    \n",
    "    # A counter\n",
    "    j = 0\n",
    "    \n",
    "    # Randomize the Batch preparation\n",
    "    os.chdir(save_dir + 'inputs/')\n",
    "    \n",
    "    print ('The input directory is:' + save_dir + 'inputs/')\n",
    "    \n",
    "    file_list = glob.glob('*.npy')\n",
    "    random.shuffle(file_list)\n",
    "    \n",
    "    \n",
    "    # Prepare the batches\n",
    "    for file_item in file_list:\n",
    "    \n",
    "        nparr = np.load(save_dir + 'inputs/' + file_item)\n",
    "                \n",
    "        # Make sure it isnt an empty file\n",
    "        if nparr is not None and nparr.shape[0] != 0:\n",
    "                    \n",
    "            #Bottom-Pad  the sentences by adding 0s to the end of the sentence\n",
    "            npad = ((0,cutoff_len - nparr.shape[0]),(0,0))\n",
    "            nparr_pad = np.pad(nparr, pad_width=npad, mode='constant', constant_values=0)\n",
    "            ls_arrays.append(nparr_pad.tolist())\n",
    "            \n",
    "            one_hot_label = np.load(save_dir + 'one_hot_labels/' + 'one_hot_sentence_label_' + file_item.split('_')[3])\n",
    "            ls_arrays_one_hot_label.append(one_hot_label)\n",
    "            \n",
    "            label = np.load(save_dir + 'labels/' + 'sentence_label_' + file_item.split('_')[3])\n",
    "            ls_arrays_label.append(label)\n",
    "            \n",
    "            j += 1\n",
    "                \n",
    "            # Save batches of the files every batch_size step\n",
    "            if (j % batch_size == 0 or j == file_count):\n",
    "                    \n",
    "                nparr_batch = np.asarray(ls_arrays)\n",
    "                nparr_batch_one_hot_labels = np.asarray(ls_arrays_one_hot_label)\n",
    "                nparr_batch_labels = np.asarray(ls_arrays_label)\n",
    "                    \n",
    "                np.save(save_dir + 'inputs/batch/nparr_batch_' + str(j) + '.npy',nparr_batch)\n",
    "                np.save(save_dir + 'labels/batch/nparr_batch_labels_' + str(j) +  '.npy',nparr_batch_labels)\n",
    "                np.save(save_dir + 'one_hot_labels/batch/nparr_batch_one_hot_labels_' + str(j) +'.npy',nparr_batch_one_hot_labels)\n",
    "                    \n",
    "                ls_arrays = []\n",
    "                ls_arrays_one_hot_label = []\n",
    "                ls_arrays_label = []\n",
    "                \n",
    "                \n",
    "# Gets a random batch of a specified size\n",
    "# This function is not used in the final model\n",
    "# But can be used for mini-batch Gradient Descent \n",
    "def get_a_random_batch(save_dir,batch_size,cutoff_len):\n",
    "\n",
    "        ls_batch_list = [] # Stores the names of all the files randomly selected without replacement\n",
    "    \n",
    "        i = 0\n",
    "        while (i < batch_size):\n",
    "     \n",
    "            rand_choice = random.choice(os.listdir(save_dir + 'inputs/'))\n",
    "            \n",
    "            if rand_choice not in ls_batch_list:\n",
    "                ls_batch_list.append(rand_choice)\n",
    "                i = i + 1\n",
    "        \n",
    "        ls_arrays = []\n",
    "        ls_arrays_one_hot_label = []\n",
    "        ls_arrays_label = []\n",
    "        \n",
    "        for item in ls_batch_list:\n",
    "            \n",
    "            nparr = np.load(save_dir + 'inputs/' + item)\n",
    "            if nparr is not None and nparr.shape[0] != 0:\n",
    "                \n",
    "                #Bottom-Pad  the sentences by adding 0s to the end of the sentence\n",
    "                npad = ((0,cutoff_len - nparr.shape[0]),(0,0))\n",
    "                nparr_pad = np.pad(nparr, pad_width=npad, mode='constant', constant_values=0)\n",
    "                ls_arrays.append(nparr_pad.tolist())\n",
    "                \n",
    "                one_hot_label = np.load(save_dir + 'one_hot_labels/' + 'one_hot_sentence_label_' + item.split('_')[3])\n",
    "                ls_arrays_one_hot_label.append(one_hot_label)\n",
    "                \n",
    "                label = np.load(save_dir + 'labels/' + 'sentence_label_' + item.split('_')[3])\n",
    "                ls_arrays_label.append(label)\n",
    "                \n",
    "                \n",
    "        nparr_batch = np.asarray(ls_arrays)\n",
    "        nparr_batch_one_hot_labels = np.asarray(ls_arrays_one_hot_label)\n",
    "        nparr_batch_labels = np.asarray(ls_arrays_label)\n",
    "        \n",
    "        return nparr_batch, nparr_batch_labels, nparr_batch_one_hot_labels\n",
    "              \n",
    "\n",
    "        \n",
    "'''\n",
    "The main training function below. Trains the data using full-batch gradient descent, \n",
    "and saves checkpoints of the model \n",
    "'''    \n",
    "def execute_training():\n",
    "    \n",
    "    print ('Starting the training process')\n",
    "    \n",
    "    num_epochs = 100\n",
    "    #validation_batch_size = 500 \n",
    "    train_batch_size = 500\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    print ('Building the graph')\n",
    "    \n",
    "    # Build graph object\n",
    "    with tf.Graph().as_default() as grap:\n",
    "        input_tensor, batch_norm_train, logits, softmax_logits,conv_keep_prob,fc_keep_prob = build_graph(cutoff_len)\n",
    "        \n",
    "        ' An Alternate model to use, commented out below'\n",
    "        ' Based on https://arxiv.org/pdf/1502.01710v5.pdf (Text Understanding from Scratch by Xiang Zhang and Yann LeCun)'\n",
    "        #input_tensor, batch_norm_train, logits, softmax_logits,\n",
    "            #conv_keep_prob,fc_keep_prob = build_graph_convnets(cutoff_len)\n",
    "            \n",
    "        train_step, confusion_matrix,predicted_indices,cross_entropy_mean, learning_rate_input,one_hot_labels,labels = build_loss_optimizer(logits,softmax_logits,num_labels)\n",
    "        #reset_metrics_vars, \\\n",
    "        #accuracy,update_op_acc,xent_mean_scalar, acc_scalar, \\\n",
    "        \n",
    "            \n",
    "    print ('Kicking off the session')\n",
    "    \n",
    "    # Initiate session for execution\n",
    "    with tf.Session(graph=grap) as sess:\n",
    "        \n",
    "        saver = tf.train.Saver() # For saving checkpoints for test inference \n",
    "        \n",
    "        # Initialize\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        \n",
    "        # Prepares randomized batches in the /inputs/batch folder\n",
    "        print ('Preparing randomized training batches')\n",
    "        prepare_randomized_batches(one_hot_train_dir,train_batch_size,cutoff_len,train_count)\n",
    "        print ('Preparing randomized validation batches')\n",
    "        prepare_randomized_batches(one_hot_valid_dir,train_batch_size,cutoff_len,valid_count)\n",
    "        \n",
    "        for i in range(0,num_epochs):\n",
    "        \n",
    "            print ('Starting Training, the Epoch is:' + str(i + 1))\n",
    "            \n",
    "            # Switch to the /inputs/batch folder and fetch all the batch files \n",
    "            # Which will be passed to the training mechanism\n",
    "            os.chdir(one_hot_train_dir + 'inputs/batch/')\n",
    "            train_batch_files = glob.glob('*.npy')\n",
    "            random.shuffle(train_batch_files)\n",
    "            \n",
    "            train_conf_matrix = None\n",
    "            \n",
    "            for file in train_batch_files:\n",
    "                \n",
    "                train_batch = np.load(one_hot_train_dir + 'inputs/batch/' + file)\n",
    "                train_labels = np.load(one_hot_train_dir + 'labels/batch/nparr_batch_labels_' + file.split('_')[2])\n",
    "                train_one_hot_labels = np.load(one_hot_train_dir + 'one_hot_labels/batch/nparr_batch_one_hot_labels_' + file.split('_')[2])\n",
    "                \n",
    "                # Kick off the training\n",
    "                _,conf_matrix,xent_mean = sess.run(\n",
    "                [train_step,confusion_matrix,cross_entropy_mean],\n",
    "                feed_dict = {input_tensor : train_batch,\n",
    "                             labels: train_labels,\n",
    "                             one_hot_labels : train_one_hot_labels,\n",
    "                             learning_rate_input : learning_rate,\n",
    "                             batch_norm_train : True,\n",
    "                             conv_keep_prob : 1.0,\n",
    "                             fc_keep_prob : 0.7\n",
    "                    })\n",
    "                \n",
    "                if (train_conf_matrix is None):\n",
    "                    train_conf_matrix = conf_matrix\n",
    "                else:\n",
    "                    train_conf_matrix += conf_matrix\n",
    "                \n",
    "            # Print confusion matrix out\n",
    "            print('Training Confusion Matrix:' + '\\n' + str(train_conf_matrix))\n",
    "            true_pos = np.sum(np.diag(train_conf_matrix))\n",
    "            all_pos = np.sum(train_conf_matrix)\n",
    "            print('Training Accuracy is: ' + str(float(true_pos / all_pos)))  # Another way to get accuracy!\n",
    "            \n",
    "            # Save model every 10 epochs\n",
    "            if ((i + 1) % 10 == 0):\n",
    "                print ('Saving checkpoint after epoch:' + str(i))\n",
    "                saver.save(sess=sess,save_path=checkpoint_dir + 'SAP_Challenge_Nitin_Venkateswaran.ckpt',global_step = i )\n",
    "                \n",
    "            \n",
    "            # Run on validation data to check validation accuracy\n",
    "            print ('Training Epoch ' + str(i + 1) + ' is complete. Running Validation Stats..')\n",
    "            \n",
    "            os.chdir(one_hot_valid_dir + 'inputs/batch/')\n",
    "            valid_batch_files = glob.glob('*.npy')\n",
    "            random.shuffle(valid_batch_files)\n",
    "            \n",
    "            valid_conf_matrix = None\n",
    "            \n",
    "            for file in valid_batch_files:\n",
    "                \n",
    "                valid_batch = np.load(one_hot_valid_dir + 'inputs/batch/' + file)\n",
    "                valid_labels = np.load(one_hot_valid_dir + 'labels/batch/nparr_batch_labels_' + file.split('_')[2])\n",
    "                valid_one_hot_labels = np.load(one_hot_valid_dir + 'one_hot_labels/batch/nparr_batch_one_hot_labels_' + file.split('_')[2])\n",
    "               \n",
    "                \n",
    "                val_conf_matrix = sess.run(\n",
    "                confusion_matrix,\n",
    "                feed_dict = {input_tensor : valid_batch,\n",
    "                            labels: valid_labels,\n",
    "                            one_hot_labels : valid_one_hot_labels,\n",
    "                            batch_norm_train : False,\n",
    "                            conv_keep_prob : 1.0,\n",
    "                            fc_keep_prob : 1.0\n",
    "                         \n",
    "                })\n",
    "                \n",
    "                if (valid_conf_matrix is None):\n",
    "                    valid_conf_matrix = val_conf_matrix\n",
    "                else:\n",
    "                    valid_conf_matrix += val_conf_matrix\n",
    "\n",
    "            \n",
    "            print('Validation Confusion Matrix:' + '\\n' + str(valid_conf_matrix))\n",
    "            true_pos = np.sum(np.diag(valid_conf_matrix))\n",
    "            all_pos = np.sum(valid_conf_matrix)\n",
    "            print('Validation Accuracy is: ' + str(float(true_pos / all_pos))) \n",
    "\n",
    "            \n",
    "execute_training()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the saved checkpoints to do inference on the batch of 3000 test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference(cutoff_len):\n",
    "    \n",
    "    with tf.Graph().as_default() as grap:\n",
    "        input_tensor, batch_norm_train, logits, softmax_logits,conv_keep_prob,fc_keep_prob = build_graph(cutoff_len)\n",
    "        \n",
    "    with open(one_hot_test_dir + 'ytest.txt','w') as predfile:\n",
    "    \n",
    "        with tf.Session(graph=grap) as sess:\n",
    "        \n",
    "            init = tf.global_variables_initializer()\n",
    "            sess = tf.Session()\n",
    "            sess.run(init)\n",
    "        \n",
    "            saver = tf.train.Saver()\n",
    "        \n",
    "            checkpoint_file_path = '/home/ubuntu/Desktop/challenge/offline_challenge_to_send/offline_challenge_to_send/checkpoints/SAP_Challenge_Nitin_Venkateswaran.ckpt-99'\n",
    "\n",
    "            print ('Loading checkpoint file:' + checkpoint_file_path)\n",
    "            saver.restore(sess,checkpoint_file_path)\n",
    "\n",
    "            os.chdir(one_hot_test_dir + 'inputs/')\n",
    "            test_files = glob.glob('*.npy')\n",
    "               \n",
    "            for file in test_files:\n",
    "            \n",
    "                _idx = file.split('_')[3].replace('.npy','')\n",
    "            \n",
    "            \n",
    "                nparr = np.load(one_hot_test_dir + 'inputs/' + file)\n",
    "                npad = ((0,cutoff_len - nparr.shape[0]),(0,0))\n",
    "                nparr_pad = np.pad(nparr, pad_width=npad, mode='constant', constant_values=0)\n",
    "            \n",
    "                nparr_2 = np.expand_dims(nparr_pad,axis=0)\n",
    "             \n",
    "                predictions,soft = sess.run(\n",
    "                [\n",
    "                    logits,softmax_logits\n",
    "                ],feed_dict = {input_tensor : nparr_2,\n",
    "                                batch_norm_train : False,\n",
    "                                conv_keep_prob : 1.0,\n",
    "                                fc_keep_prob : 1.0})\n",
    "                \n",
    "                predicted_index = tf.argmax(soft, axis=1)\n",
    "                \n",
    "                predfile.write(str(_idx,) + ',' + str(predicted_index.eval(session=sess)[0]) + '\\n')\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "inference(cutoff_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APPENDIX below: \n",
    "\n",
    "This is an alternative model based off Text Understanding From Scratch by Xiang ZHang and Yann LeCun. It is not used in the final model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Alternative model based off\n",
    "# https://arxiv.org/pdf/1502.01710v5.pdf\n",
    "\n",
    "def build_graph_convnets(cutoff_len):\n",
    "    \n",
    "    num_chars = 26\n",
    "    \n",
    "    char_embedding_n_dim = 32\n",
    "    \n",
    "    first_window_size = 7 # First window applied to sentence\n",
    "    second_window_size = 7 # Second window\n",
    "    third_window_size = 3 # And so on...\n",
    "    fourth_window_size = 3\n",
    "    fifth_window_size = 3\n",
    "    sixth_window_size = 3\n",
    "    \n",
    "    # No third, fourth, or fifth pool size, copying the paper's approach \n",
    "    first_pool_size = 3\n",
    "    second_pool_size = 3\n",
    "    sixth_pool_size = 3\n",
    "    \n",
    "    # One channel output for each of the six conv layers\n",
    "    first_layer_no_filters = 256 \n",
    "    second_layer_no_filters = 256 \n",
    "    third_layer_no_filters = 256 \n",
    "    fourth_layer_no_filters = 256 \n",
    "    fifth_layer_no_filters = 256 \n",
    "    sixth_layer_no_filters = 256 \n",
    "    \n",
    "    first_fc_out = 2056\n",
    "    second_fc_out = 2056\n",
    "    third_fc_out = num_labels \n",
    "    # Hyper parameters end \n",
    "    \n",
    "    reuse_flag = tf.AUTO_REUSE\n",
    "    \n",
    "    # Layer Zero is just to define the input tensors and other 'global' tensors such as dropout probabilities\n",
    "    with tf.variable_scope('layer_zero_convnet',reuse=reuse_flag):\n",
    "        \n",
    "        \n",
    "        \n",
    "         # This layer defines the character embedding matrix, and returns character embedding for batches of sentences\n",
    "        # tensor: input_tensor is of shape [None, 26] where the first dimension is the batch_size * max_len for that batch\n",
    "        input_tensor = tf.placeholder(dtype=tf.float32,shape=[None,cutoff_len,26],name=\"input_tensor\") # Unroll the first 2 dimensions at the numpy array before feeding\n",
    "        l1w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        char_embedding_mat = tf.get_variable(name=\"char_embedding\",dtype=tf.float32,shape=[26,char_embedding_n_dim],initializer=l1w_init)\n",
    "        \n",
    "        # This gets the char embeddings for the batch\n",
    "        # Shape is [batch_size * cutoff_len, char_embedding_n_dim]\n",
    "        input_tensor_2D = tf.reshape(input_tensor,shape=[-1,26]) # Reshaped to 2D with batch_size * cutoff_len as 1st dim\n",
    "        l1_char_embedding = tf.matmul(input_tensor_2D,char_embedding_mat)\n",
    "        \n",
    "        # reshape input tensor to 3D matrix for the temporal 1D convolution operation\n",
    "        # The input tensor is already 1 of m encoded\n",
    "        input_tensor_with_embed = tf.reshape(l1_char_embedding,shape=[-1,cutoff_len,char_embedding_n_dim])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # The input tensor is already one-hot encoded , with a character dictionary size of 26 characters\n",
    "        #input_tensor = tf.placeholder(dtype=tf.float32,shape=[None,cutoff_len,26],name=\"input_tensor\")\n",
    "        conv_keep_prob = tf.placeholder(dtype=tf.float32,shape=[],name=\"conv_dropout_prob\")\n",
    "        fc_keep_prob = tf.placeholder(dtype=tf.float32,shape=[],name=\"fc_dropout_prob\")\n",
    "        \n",
    "        # Toggle application of either population or sample mean and variance during batch normalization\n",
    "        # is False during validation / testing to allow population statistics to apply\n",
    "        batch_norm_train = tf.placeholder(name=\"batch_norm_train\", dtype=tf.bool)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('layer_one_convnet',reuse=reuse_flag):\n",
    "        \n",
    "        l1w_init = tf.random_normal_initializer(mean=0,stddev=0.02,dtype=tf.float32)\n",
    "        first_conv_layer = tf.layers.conv1d(inputs = input_tensor_with_embed, filters = first_layer_no_filters,kernel_size = first_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l1w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"first_conv_layer\")\n",
    "        first_conv_batch = tf.contrib.layers.batch_norm(first_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        first_maxpool_layer = tf.layers.average_pooling1d(inputs = first_conv_batch, pool_size = first_pool_size, strides = first_pool_size,\n",
    "                                                      padding='valid', data_format='channels_last',name=\"first_maxpool_layer\")\n",
    "\n",
    "        \n",
    "    with tf.variable_scope('layer_two_convnet',reuse=reuse_flag):\n",
    "        \n",
    "        l2w_init = tf.random_normal_initializer(mean=0,stddev=0.02,dtype=tf.float32)\n",
    "        second_conv_layer = tf.layers.conv1d(inputs = first_maxpool_layer, filters = second_layer_no_filters,kernel_size = second_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"second_conv_layer\")\n",
    "        second_conv_batch = tf.contrib.layers.batch_norm(second_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        second_maxpool_layer = tf.layers.average_pooling1d(inputs = second_conv_batch, pool_size = second_pool_size, strides = second_pool_size,\n",
    "                                                      padding='valid', data_format='channels_last',name=\"second_maxpool_layer\")\n",
    "\n",
    "\n",
    "        \n",
    "    with tf.variable_scope('layer_three_convnet',reuse=reuse_flag):\n",
    "        l3w_init = tf.random_normal_initializer(mean=0,stddev=0.02,dtype=tf.float32)\n",
    "        third_conv_layer = tf.layers.conv1d(inputs = second_maxpool_layer, filters = third_layer_no_filters,kernel_size = third_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l3w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"third_conv_layer\")\n",
    "        third_conv_batch = tf.contrib.layers.batch_norm(third_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('layer_four_convnet',reuse=reuse_flag):\n",
    "        l4w_init = tf.random_normal_initializer(mean=0,stddev=0.02,dtype=tf.float32)\n",
    "        fourth_conv_layer = tf.layers.conv1d(inputs = third_conv_batch, filters = fourth_layer_no_filters,kernel_size = fourth_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l4w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"fourth_conv_layer\")\n",
    "        fourth_conv_batch = tf.contrib.layers.batch_norm(fourth_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('layer_five_convnet',reuse=reuse_flag):\n",
    "        l5w_init = tf.random_normal_initializer(mean=0,stddev=0.02,dtype=tf.float32)\n",
    "        fifth_conv_layer = tf.layers.conv1d(inputs = fourth_conv_batch, filters = fifth_layer_no_filters,kernel_size = fifth_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l5w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"fifth_conv_layer\")\n",
    "        fifth_conv_batch = tf.contrib.layers.batch_norm(fifth_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('layer_six_convnet',reuse=reuse_flag):\n",
    "        l6w_init = tf.random_normal_initializer(mean=0,stddev=0.02,dtype=tf.float32)\n",
    "        sixth_conv_layer = tf.layers.conv1d(inputs = fifth_conv_batch, filters = sixth_layer_no_filters,kernel_size = sixth_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l6w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"sixth_conv_layer\")\n",
    "        sixth_conv_batch = tf.contrib.layers.batch_norm(sixth_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        sixth_maxpool_layer = tf.layers.average_pooling1d(inputs = sixth_conv_batch, pool_size = sixth_pool_size, strides = sixth_pool_size,\n",
    "                                                      padding='valid', data_format='channels_last',name=\"sixth_maxpool_layer\")\n",
    "        \n",
    "        sixth_reshaped_layer = tf.reshape(sixth_maxpool_layer,shape=[-1,sixth_maxpool_layer.get_shape()[1] * sixth_maxpool_layer.get_shape()[2]])\n",
    "        \n",
    "        print (sixth_reshaped_layer)\n",
    "        \n",
    "    \n",
    "    with tf.variable_scope('layer_seven_convnet',reuse=reuse_flag):\n",
    "        l7w_init = tf.random_normal_initializer(mean=0,stddev=0.02,dtype=tf.float32)\n",
    "        l7b_init = tf.zeros_initializer()\n",
    "        \n",
    "        seventh_fc = tf.contrib.layers.fully_connected(inputs=sixth_reshaped_layer,num_outputs=first_fc_out,activation_fn = tf.nn.relu,\n",
    "                                                       weights_initializer = l7w_init,biases_initializer = l7b_init,scope=\"seventh_fc\")\n",
    "        seventh_fc_batch = tf.contrib.layers.batch_norm(seventh_fc,center = True, scale=False,is_training=batch_norm_train)\n",
    "        seventh_fc_final = tf.nn.dropout(seventh_fc_batch,keep_prob = fc_keep_prob)\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('layer_eight_convnet',reuse=reuse_flag):\n",
    "        l8w_init = tf.random_normal_initializer(mean=0,stddev=0.02,dtype=tf.float32)\n",
    "        l8b_init = tf.zeros_initializer()\n",
    "        \n",
    "        eigth_fc = tf.contrib.layers.fully_connected(inputs=seventh_fc_final,num_outputs=second_fc_out,activation_fn = tf.nn.relu,\n",
    "                                                       weights_initializer = l8w_init,biases_initializer = l8b_init,scope=\"eigth_fc\")\n",
    "        eigth_fc_batch = tf.contrib.layers.batch_norm(eigth_fc,center = True, scale=False,is_training=batch_norm_train)\n",
    "        eigth_fc_final = tf.nn.dropout(eigth_fc_batch,keep_prob = fc_keep_prob)\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('layer_ninth_convnet',reuse=reuse_flag):\n",
    "        l9w_init = tf.random_normal_initializer(mean=0,stddev=0.02,dtype=tf.float32) \n",
    "        l9b_init = tf.zeros_initializer()\n",
    "        \n",
    "        ninth_fc = tf.contrib.layers.fully_connected(inputs=eigth_fc_final,num_outputs=num_labels,activation_fn = tf.nn.relu,\n",
    "                                                     weights_initializer = l9w_init,biases_initializer = l9b_init,scope=\"ninth_fc\")\n",
    "        \n",
    "        ninth_final_softmax = tf.nn.softmax(ninth_fc,name=\"ninth_final_softmax\")\n",
    "        \n",
    "        return input_tensor, batch_norm_train, ninth_fc,ninth_final_softmax, conv_keep_prob,fc_keep_prob\n",
    "        \n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
