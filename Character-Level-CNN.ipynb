{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/Desktop/tensorflow-gpu/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/Desktop/tensorflow-gpu/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os, random , math, shutil, glob, uuid\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Initialize all the Global Variables for Train, Validation, and Test Data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Below are all the \"global\" constants used in the model (not hyperparameters or graph parameters)'''\n",
    "\n",
    "\n",
    "\n",
    "home_dir = '/home/ubuntu/Desktop/nlp_deep/amazon_data_sample/'\n",
    "one_hot_train_dir = home_dir + 'train/'\n",
    "one_hot_valid_dir = home_dir + 'valid/'\n",
    "one_hot_test_dir = home_dir + 'test/'\n",
    "train_file = home_dir + 'train_sample.csv'\n",
    "test_file = home_dir + 'test_sample.csv'\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "home_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/'\n",
    "one_hot_train_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/train/'\n",
    "one_hot_valid_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/valid/'\n",
    "one_hot_test_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/test/'\n",
    "train_file = home_dir + 'amazon_review_full_csv/train_sample.csv'\n",
    "test_file = home_dir + 'amazon_review_full_csv/test_sample.csv'\n",
    "'''\n",
    "\n",
    "\n",
    "# Stores all the checkpoint models\n",
    "checkpoint_dir = '/home/ubuntu/Desktop/nlp_deep/checkpoints/'\n",
    "#checkpoint_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/checkpoints/'\n",
    "\n",
    "\n",
    "\n",
    "# Index denotes the column placement in the character embedding matrix\n",
    "# This is used to create the one-hot character vector of sentence inputs\n",
    "\n",
    "'''\n",
    "one_hot_column_label = {'a':68,'b':67,'c':66,'d':65,'e':64,'f':63,'g':62,'h':61,'i':60,'j':59,'k':58,'l':57,'m':56,'n':55,'o':54,\n",
    "                        'p':53,'q':52,'r':51,'s':50,'t':49,'u':48,'v':47,'w':46,'x':45,'y':44,'z':43,'0':42,'1':41,'2':40,'3':39,'4':38,'5':37,'6':36,'7':35,\n",
    "                        '8':34,'9':33,'-':32,',':31,';':30,'.':29,'!':28,'?':27,':':26,'\"':25,'\\'':24,'/':23,'\\\\':22,'|':21,'_':20,\n",
    "                        '@':19,'#':18,'$':17,'%':16,'^':15,'&':14,'*':13,'~':12,'`':11,'+':10,'-':9,'=':8,'<':7,'>':6,'(':5,')':4,'[':3,\n",
    "                        ']':2,'{':1,'}':0}\n",
    "'''\n",
    "\n",
    "one_hot_column_label = {'a':42,'b':41,'c':40,'d':39,'e':38,'f':37,'g':36,'h':35,'i':34,'j':33,'k':32,'l':31,'m':30,'n':29,'o':28,\n",
    "                        'p':27,'q':26,'r':25,'s':24,'t':23,'u':22,'v':21,'w':20,'x':19,'y':18,'z':17,'0':16,'1':15,'2':14,'3':13,'4':12,'5':11,'6':10,'7':9,\n",
    "                        '8':8,'9':7,'-':6,'#':5,'.':4,'!':3,'?':2,':':1,';':0}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "one_hot_column_label = {'a':0,'b':1,'c':2,'d':3,'e':4,'f':5,'g':6,'h':7,'i':8,'j':9,'k':10,'l':11,'m':12,'n':13,'o':14,\n",
    "                        'p':15,'q':16,'r':17,'s':18,'t':19,'u':20,'v':21,'w':22,'x':23,'y':24,'z':25,'A':26,'B':27,'C':28,'D':29,'E':30,'F':31,\n",
    "                        'G':32,'H':33,'I':34,'J':35,'K':36,'L':37,'M':38,'N':39,'O':40,'P':41,'Q':42,'R':43,'S':44,'T':45,\n",
    "                        'U':46,'V':47,'W':48,'X':49,'Y':50,'Z':51,'0':52,'1':53,'2':54,'3':55,'4':56,'5':57,'6':58,'7':59,\n",
    "                        '8':60,'9':61,'-':62,',':63,';':64,'.':65,'!':66,'?':67,':':68,'\"':69,'\\'':70,'/':71,'\\\\':72,'|':73,'_':74,\n",
    "                        '@':75,'#':76,'$':77,'%':78,'^':79,'&':80,'*':81,'~':82,'`':83,'+':84,'-':85,'=':86,'<':87,'>':88,'(':89,')':90,'[':91,\n",
    "                        ']':92,'{':93,'}':94}\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "# For 1-1 mapping of character to letter encoding\n",
    "one_hot_column_label = {'a':1,'b':2,'c':3,'d':4,'e':5,'f':6,'g':7,'h':8,'i':9,'j':10,'k':11,'l':12,'m':13,'n':14,'o':15,\n",
    "                        'p':16,'q':17,'r':18,'s':19,'t':20,'u':21,'v':22,'w':23,'x':24,'y':25,'z':26,'0':27,'1':28,'2':29,'3':30,'4':31,'5':32,'6':33,'7':34,\n",
    "                        '8':35,'9':36,'?':37,'!':38,' ':39,'$':40,'.':41}\n",
    "'''\n",
    "'''\n",
    "one_hot_column_label = {'a':0,'b':1,'c':2,'d':3,'e':4,'f':5,'g':6,'h':7,'i':8,'j':9,'k':10,'l':11,'m':12,'n':13,'o':14,\n",
    "                        'p':15,'q':16,'r':17,'s':18,'t':19,'u':20,'v':21,'w':22,'x':23,'y':24,'z':25,'0':26,'1':27,'2':28,'3':29,'4':30,'5':31,'6':32,'7':33,\n",
    "                        '8':34,'9':35,'?':36,'!':37,'$':38,'.':39,' ':40}\n",
    "\n",
    "'''\n",
    "\n",
    "num_chars_dict = 43\n",
    "\n",
    "# Number of labels to classify\n",
    "num_labels = 5\n",
    "\n",
    "# Tensorboard directories\n",
    "train_tensorboard_dir = home_dir + 'train/tensorboard/'\n",
    "valid_tensorboard_dir = home_dir + 'valid/tensorboard/'\n",
    "\n",
    "log_dir = '/home/ubuntu/Desktop/nlp_deep/Char_CNN_log/'\n",
    "#log_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/log/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a 80-20 train-validation split. Get the length of the longest sentence, which will be used to build the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "The longest sentence is of length:988\n",
      "The shortest sentence is of length:9\n",
      "The average length is:291.02272602272603\n",
      "The 75th percentile is:399.0\n",
      "The 90th percentile is:535.0\n",
      "Verify Stratified Sampling\n",
      "988\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def strip_characters(sent):\n",
    "    \n",
    "    # Strip unnecessary characters from the review\n",
    "    \n",
    "    #sent = sent.replace('-','')\n",
    "    sent = sent.replace(',','')\n",
    "    #sent = sent.replace(';','')\n",
    "    #sent = sent.replace('.','')\n",
    "    #sent = sent.replace(':','')\n",
    "    sent = sent.replace('\"','')\n",
    "    \n",
    "    sent = sent.replace('/','')\n",
    "    sent = sent.replace('\\\\','')\n",
    "    sent = sent.replace('|','')\n",
    "    sent = sent.replace('_','')\n",
    "    sent = sent.replace('@','')\n",
    "    #sent = sent.replace('#','')\n",
    "    sent = sent.replace('$','')\n",
    "    sent = sent.replace('^','')                        \n",
    "    sent = sent.replace('&','')                        \n",
    "    sent = sent.replace('*','')                        \n",
    "    sent = sent.replace('~','')                            \n",
    "    sent = sent.replace('`','')    \n",
    "    sent = sent.replace('+','')                        \n",
    "    sent = sent.replace('-','')    \n",
    "    sent = sent.replace('=','')\n",
    "    sent = sent.replace('<','')\n",
    "    sent = sent.replace('>','')                        \n",
    "    sent = sent.replace('(','')                        \n",
    "    sent = sent.replace(')','')                        \n",
    "    sent = sent.replace('[','')                        \n",
    "    sent = sent.replace(']','')                        \n",
    "    sent = sent.replace('{','')                        \n",
    "    sent = sent.replace('}','')        \n",
    "    \n",
    "    \n",
    "    # remove the stop words\n",
    "    stop_words = list(get_stop_words('en'))\n",
    "    nltk_words = list(stopwords.words('english'))\n",
    "    stop_words.extend(nltk_words)\n",
    "    \n",
    "    #print (sent)\n",
    "    sent_words = sent.split(' ')\n",
    "    sent_stopped_words = [w for w in sent_words if not w in stop_words]\n",
    "    stopped_sent = ' '.join(sent_stopped_words)\n",
    "    #print (stopped_sent)\n",
    "    \n",
    "                        \n",
    "    return stopped_sent\n",
    "                        \n",
    "                        \n",
    "nltk.download('stopwords')                        \n",
    "# Combine and label before train / valid split\n",
    "train_data = pd.read_csv(train_file)\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "# Are the datasets balanced - yes\n",
    "#print ('Checking Dataset Balance')\n",
    "#train_data.hist(column='Review')\n",
    "#test_data.hist(column='Review')\n",
    "\n",
    "\n",
    "# We get the max_len for padding and for the convolution filter definition in the model\n",
    "max_len = 0\n",
    "total_len = 0\n",
    "min_len = 999\n",
    "ls_len = []\n",
    "\n",
    "\n",
    "stripped_file_train =  home_dir + 'file_strip_train.csv'   \n",
    "stripped_file_test = home_dir + 'file_strip_test.csv'\n",
    "\n",
    "with open(stripped_file_train,'w') as strip:   \n",
    "    \n",
    "    strip.write('Review' + ',' + 'Description' + '\\n')\n",
    "\n",
    "    for index,row in train_data.iterrows():\n",
    "    \n",
    "        sent_row = strip_characters(row[2])\n",
    "        strip.write(str(row[0]) + ','  + sent_row + '\\n')    \n",
    "        \n",
    "        \n",
    "with open(stripped_file_test,'w') as strip:\n",
    "    \n",
    "    strip.write('Review' + ',' + 'Description' + '\\n')\n",
    "    \n",
    "    for index,row in test_data.iterrows():\n",
    "        \n",
    "        sent_row = strip_characters(row[2])\n",
    "        strip.write(str(row[0]) + ','  + sent_row + '\\n')    \n",
    "\n",
    "        \n",
    "\n",
    "strip_train_data = pd.read_csv(stripped_file_train)\n",
    "X_test = pd.read_csv(stripped_file_test)\n",
    "\n",
    "        \n",
    "label_data = strip_train_data.loc[:,'Review']\n",
    "all_data = strip_train_data\n",
    "all_data = all_data.append(X_test,ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "for index, row in all_data.iterrows():\n",
    "    total_len += len(row[1])\n",
    "    ls_len.append(len(row[1]))\n",
    "    if (max_len < len(row[1])):\n",
    "        max_len = len(row[1])\n",
    "    if (min_len > len(row[1])):\n",
    "        min_len = len(row[1])\n",
    "            \n",
    "np_len = np.asarray(ls_len)\n",
    "\n",
    "print ('The longest sentence is of length:' + str(max_len))\n",
    "print ('The shortest sentence is of length:' + str(min_len))\n",
    "print ('The average length is:' + str(total_len/strip_train_data.shape[0]))\n",
    "print ('The 75th percentile is:' + str(np.percentile(np_len,75)))\n",
    "print ('The 90th percentile is:' + str(np.percentile(np_len,90)))\n",
    "\n",
    "\n",
    "\n",
    "# Train and test split (stratified sampling, 70-30 split)\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(strip_train_data,label_data,test_size=0.2,train_size=0.8,random_state=13814,shuffle=True,stratify=label_data)\n",
    "\n",
    "# Verify that stratified sampling worked (histogram distributions must be same)\n",
    "print ('Verify Stratified Sampling')\n",
    "X_train.hist(column=\"Review\") # First histogram is training\n",
    "X_valid.hist(column=\"Review\") # Second histogram is validation\n",
    "\n",
    "\n",
    "'Some more \"global\" variables'\n",
    "cutoff_len = int(np.percentile(np_len,100)) # a graph parameter\n",
    "print(cutoff_len)\n",
    "train_count = X_train.shape[0]\n",
    "valid_count = X_valid.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert each sentence to one-hot character matrix to drive the non-static character embedding matrix creation, and store this character matrix representation in disk cache.\n",
    "\n",
    "Also store in disk cache, the label and a one-hot representation of the label, for the cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up directories and preparing one-hot character vectors\n",
      "Number chars:43\n"
     ]
    }
   ],
   "source": [
    "# Creates the one-hot character vector representation of sentences which will be used as the layer before a\n",
    "# character-embedding matrix\n",
    "def save_numpy_one_hot(X_dataset,unique_char_size,save_dir,one_hot_column_label):\n",
    "    \n",
    "    for index,row in X_dataset.iterrows():\n",
    "        \n",
    "        #sent = row[2]\n",
    "        sent = row[1]\n",
    "        #print (sent)\n",
    "        np_one_hot = np.zeros(shape=(len(sent),unique_char_size),dtype=np.int32)\n",
    "        #np_one_hot = np.zeros(shape=len(sent),dtype=np.int32)\n",
    "        \n",
    "        for i in range(0,len(sent)):\n",
    "            if (sent[i].lower() in one_hot_column_label):\n",
    "                np_one_hot[i][one_hot_column_label[sent[i].lower()]] = 1 # else, it remains 0 encoded.\n",
    "                #np_one_hot[i] = one_hot_column_label[sent[i].lower()] # For character embedding lookup\n",
    "            #else:\n",
    "            #    np_one_hot[i][41] = 1 # unknown character\n",
    "        #print(np_one_hot)\n",
    "            \n",
    "        # Save the one-hot encoding for each sentence \n",
    "        np.save(save_dir + 'inputs/' + 'one_hot_sentence_' + str(index) + '.npy',np_one_hot)\n",
    "        \n",
    "        #Save the label\n",
    "        lbl = int(row[0]) - 1 # Must start with 0-indexing\n",
    "        np.save(save_dir + 'labels/' +  'sentence_label_' + str(index) + '.npy',np.asarray(lbl))\n",
    "        \n",
    "        # Save a one-hot version of the label (for xentropy loss function)\n",
    "        np_one_hot_label = np.zeros(shape=(5),dtype=np.int32)\n",
    "        np_one_hot_label[lbl] = 1\n",
    "        \n",
    "        np.save(save_dir + 'one_hot_labels/' + 'one_hot_sentence_label_' + str(index) + '.npy',np_one_hot_label)\n",
    "        \n",
    "        \n",
    "def save_numpy_one_hot_test(X_dataset,unique_char_size,save_dir,one_hot_column_label):\n",
    "    \n",
    "    for index,row in X_dataset.iterrows():\n",
    "        \n",
    "        sent = row[1]\n",
    "        np_one_hot = np.zeros(shape=(len(sent),unique_char_size),dtype=np.int32)\n",
    "        #np_one_hot = np.zeros(shape=len(sent),dtype=np.int32)\n",
    "        \n",
    "        for i in range(0,len(sent)):\n",
    "            if (sent[i].lower() in one_hot_column_label):\n",
    "                np_one_hot[i][one_hot_column_label[sent[i].lower()]] = 1\n",
    "                #np_one_hot[i] = one_hot_column_label[sent[i].lower()]\n",
    "            #else:\n",
    "            #    np_one_hot[i][41] = 1\n",
    "            \n",
    "        # Save the one-hot encoding for each sentence \n",
    "        np.save(save_dir + 'inputs/' + 'one_hot_sentence_' + str(index) + '.npy',np_one_hot)\n",
    "        \n",
    "        \n",
    "def inp_create_one_hot_char_mat(unique_char_size,train_dir,valid_dir,test_dir):\n",
    "    \n",
    "    save_numpy_one_hot(X_train,unique_char_size,train_dir,one_hot_column_label)\n",
    "    save_numpy_one_hot(X_valid,unique_char_size,valid_dir,one_hot_column_label)\n",
    "    save_numpy_one_hot_test(X_test,unique_char_size,test_dir,one_hot_column_label)\n",
    "    \n",
    "\n",
    "# One  - off folder creation \n",
    "if (not os.path.exists(one_hot_train_dir)):\n",
    "        \n",
    "        print ('Setting up directories and preparing one-hot character vectors')\n",
    "        print ('Number chars:' + str(num_chars_dict))\n",
    "        os.mkdir(one_hot_train_dir)\n",
    "        os.mkdir(one_hot_valid_dir)\n",
    "        os.mkdir(one_hot_test_dir)\n",
    "        os.mkdir(one_hot_train_dir + 'inputs/')\n",
    "        os.mkdir(one_hot_valid_dir + 'inputs/')\n",
    "        os.mkdir(one_hot_test_dir + 'inputs/')\n",
    "        os.mkdir(one_hot_train_dir + 'labels/')\n",
    "        os.mkdir(one_hot_valid_dir + 'labels/')\n",
    "        os.mkdir(one_hot_train_dir + 'one_hot_labels/')\n",
    "        os.mkdir(one_hot_valid_dir + 'one_hot_labels/')\n",
    "        os.mkdir(one_hot_train_dir + 'tensorboard/')\n",
    "        os.mkdir(one_hot_valid_dir + 'tensorboard/')\n",
    "        inp_create_one_hot_char_mat(num_chars_dict,one_hot_train_dir,one_hot_valid_dir,one_hot_test_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build the graph and define the loss and optimizer operations.\n",
    "\n",
    "The below function is not used - please see build_graph_convnet function below\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start to build the model\n",
    "# Let's build the graph first\n",
    "\n",
    "# This function is Not Used in the main model - please see build_graph_convnets below\n",
    "def build_graph(cutoff_len):\n",
    "    \n",
    "    # Hyper Parameters with descriptions\n",
    "    num_char_dict = num_chars_dict\n",
    "    char_embedding_n_dim = 256 # The number of columns in the character embedding matrix\n",
    "    \n",
    "    first_window_size = 3 # First window applied to sentence\n",
    "    second_window_size = 4 # Second window\n",
    "    third_window_size = 5 # And so on...\n",
    "    fourth_window_size = 6\n",
    "    \n",
    "    \n",
    "    first_layer_no_filters = 256\n",
    "    second_layer_no_filters = 256\n",
    "    third_layer_no_filters = 256\n",
    "    fourth_layer_no_filters = 256\n",
    "    \n",
    "    first_fc_out = 1024\n",
    "    second_fc_out = 1024\n",
    "    \n",
    "    # Hyper parameters end \n",
    "    \n",
    "    reuse_flag = tf.AUTO_REUSE\n",
    "    \n",
    "    # Build the graph\n",
    "    with tf.variable_scope('layer_one_char_embedding',reuse=reuse_flag):\n",
    "        \n",
    "        batch_norm_train = tf.placeholder(name=\"batch_norm_train\", dtype=tf.bool)\n",
    "        \n",
    "        '''This layer defines the character embedding matrix, and returns character embedding for batches of sentences'''\n",
    "        \n",
    "        # The input_tensor is of shape [None, cutoff_len, num_char_dict] where cutoff_len is the length limit for the 2-D embedding matrix\n",
    "        #input_tensor = tf.placeholder(dtype=tf.float32,shape=[None,cutoff_len,num_char_dict],name=\"input_tensor\") \n",
    "        input_tensor = tf.placeholder(dtype=tf.int32,shape=[None,cutoff_len],name=\"input_tensor\") \n",
    "        \n",
    "        # Initializer for embedding matrix\n",
    "        l1w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        # The character embedding matrix declared below. The 2nd rank is the number of dimensions representing each character\n",
    "        char_embedding_mat = tf.get_variable(name=\"char_embedding\",dtype=tf.float32,shape=[num_char_dict,char_embedding_n_dim],initializer=l1w_init)\n",
    "        \n",
    "        input_tensor_2D = tf.reshape(input_tensor,shape=[-1,num_char_dict]) \n",
    "        #l1_char_embedding = tf.matmul(input_tensor_2D,char_embedding_mat)\n",
    "        \n",
    "        l1_char_embedding = tf.nn.embedding_lookup(char_embedding_mat,input_tensor,name=\"l1_char_embedding\")\n",
    "        print(l1_char_embedding)\n",
    "        \n",
    "        # reshape char embedding matrix to 3D matrix for the temporal 1D convolution operation\n",
    "        input_tensor_with_embed = tf.reshape(l1_char_embedding,shape=[-1,cutoff_len,char_embedding_n_dim])\n",
    "        \n",
    "        # Some keep probabilities for adding dropout\n",
    "        conv_keep_prob = tf.placeholder(dtype=tf.float32,shape=[],name=\"conv_dropout_prob\")\n",
    "        fc_keep_prob = tf.placeholder(dtype=tf.float32,shape=[],name=\"fc_dropout_prob\")\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('layer_two_kernels',reuse=reuse_flag):\n",
    "        \n",
    "        l2w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        \n",
    "        # 1-D convolution layer\n",
    "        first_conv_layer = tf.layers.conv1d(inputs = input_tensor_with_embed, filters = first_layer_no_filters,kernel_size = first_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"first_conv_layer\")\n",
    "        # Batch normalization\n",
    "        first_conv_batch = tf.contrib.layers.batch_norm(first_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        \n",
    "        # Pooling Layer\n",
    "        #first_sumpool_layer = tf.reduce_sum(first_conv_batch, axis=1,keepdims=False) # Global sumpool 1-D\n",
    "        first_maxpool_layer = tf.reduce_max(first_conv_batch, axis=1,keepdims=False) # Global maxpool 1-D\n",
    "        \n",
    "        # Dropout\n",
    "        first_layer = tf.nn.dropout(first_maxpool_layer,keep_prob = conv_keep_prob )\n",
    "        \n",
    "        \n",
    "        # Repeat for second stack\n",
    "        second_conv_layer = tf.layers.conv1d(inputs = input_tensor_with_embed, filters = second_layer_no_filters,kernel_size = second_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(), name=\"second_conv_layer\")\n",
    "        second_conv_batch = tf.contrib.layers.batch_norm(second_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        #second_sumpool_layer = tf.reduce_sum(second_conv_batch, axis=1,keepdims=False) \n",
    "        second_maxpool_layer = tf.reduce_max(second_conv_batch, axis=1,keepdims=False) \n",
    "        \n",
    "        second_layer = tf.nn.dropout(second_maxpool_layer,keep_prob = conv_keep_prob )\n",
    "        \n",
    "        \n",
    "        #...and third stack...\n",
    "        third_conv_layer = tf.layers.conv1d(inputs = input_tensor_with_embed, filters = third_layer_no_filters,kernel_size = third_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(), name=\"third_conv_layer\")\n",
    "        third_conv_batch = tf.contrib.layers.batch_norm(third_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        #third_sumpool_layer = tf.reduce_sum(third_conv_batch, axis=1,keepdims=False) \n",
    "        third_maxpool_layer = tf.reduce_max(third_conv_batch, axis=1,keepdims=False) \n",
    "        \n",
    "        \n",
    "        third_layer = tf.nn.dropout(third_maxpool_layer,keep_prob = conv_keep_prob )\n",
    "        \n",
    "        \n",
    "        #..and fourth stack..\n",
    "        fourth_conv_layer = tf.layers.conv1d(inputs = input_tensor_with_embed, filters = fourth_layer_no_filters,kernel_size = fourth_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(), name=\"fourth_conv_layer\")\n",
    "        fourth_conv_batch = tf.contrib.layers.batch_norm(fourth_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        #fourth_sumpool_layer = tf.reduce_sum(fourth_conv_batch, axis=1,keepdims=False) # Sums across the 'length' \n",
    "        fourth_maxpool_layer = tf.reduce_sum(fourth_conv_batch, axis=1,keepdims=False) # Sums across the 'length' \n",
    "        \n",
    "        fourth_layer = tf.nn.dropout(fourth_maxpool_layer,keep_prob = conv_keep_prob )\n",
    "        \n",
    "        all_conv = tf.concat(axis=1,values=[first_layer,second_layer,third_layer,fourth_layer],\n",
    "                                            #fifth_conv_dropout,sixth_conv_dropout,seventh_conv_dropout],\n",
    "                                            name=\"all_conv\")\n",
    "        \n",
    "        print ('Stacked Convolution Tensor as below')\n",
    "        print(all_conv)\n",
    "        \n",
    "        \n",
    "    # Now pass it through three FC layers\n",
    "    with tf.variable_scope('layer_three_fc',reuse=reuse_flag):\n",
    "       \n",
    "        l3w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        #l3b_init = tf.random_normal_initializer(mean=0, stddev=0.1, dtype=tf.float32)\n",
    "        l3b_init = tf.zeros_initializer()\n",
    "        \n",
    "        first_fc = tf.contrib.layers.fully_connected(inputs=all_conv,num_outputs=first_fc_out,activation_fn = tf.nn.relu,\n",
    "                                                    weights_initializer = l3w_init,biases_initializer = l3b_init,scope=\"first_fc\")\n",
    "        \n",
    "        first_fc_batch = tf.contrib.layers.batch_norm(first_fc,center = True, scale=False,is_training=batch_norm_train)\n",
    "        first_fc_final = tf.nn.dropout(first_fc_batch,keep_prob = fc_keep_prob) \n",
    "        \n",
    "    '''\n",
    "    with tf.variable_scope('layer_four_fc',reuse=reuse_flag):\n",
    "        l4w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        #l4b_init = tf.random_normal_initializer(mean=0, stddev=0.1, dtype=tf.float32)\n",
    "        l4b_init = tf.zeros_initializer()\n",
    "        \n",
    "        second_fc = tf.contrib.layers.fully_connected(inputs=first_fc_final,num_outputs=second_fc_out,activation_fn = tf.nn.relu,\n",
    "                                                    weights_initializer = l4w_init,biases_initializer = l4b_init,scope=\"second_fc\")\n",
    "        second_fc_batch = tf.contrib.layers.batch_norm(second_fc,center = True, scale=False,is_training=batch_norm_train)\n",
    "        second_fc_final = tf.nn.dropout(second_fc_batch,keep_prob = fc_keep_prob)\n",
    "    '''\n",
    "    # Third FC layer and softmax\n",
    "    with tf.variable_scope('layer_five_fc',reuse=reuse_flag):\n",
    "        l5w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        #l5b_init = tf.random_normal_initializer(mean=0, stddev=0.1, dtype=tf.float32)\n",
    "        l5b_init = tf.zeros_initializer()\n",
    "        \n",
    "        third_fc = tf.contrib.layers.fully_connected(inputs = first_fc_final,num_outputs=num_labels,weights_initializer=l5w_init,\n",
    "                                                    biases_initializer = l5b_init,activation_fn=tf.nn.relu,scope=\"third_fc\")\n",
    "        \n",
    "        third_fc_batch = tf.contrib.layers.batch_norm(third_fc,center=True,scale=False,is_training=batch_norm_train)\n",
    "        third_fc_final = tf.nn.dropout(third_fc_batch,keep_prob = fc_keep_prob)\n",
    "    \n",
    "    with tf.variable_scope('layer_six_softmax',reuse=reuse_flag):\n",
    "        softmax_logits = tf.nn.softmax(logits=third_fc_final,name=\"final_logits_softmax\")\n",
    "        \n",
    "    return input_tensor, batch_norm_train, third_fc_final,softmax_logits, conv_keep_prob,fc_keep_prob\n",
    "        \n",
    "\n",
    "\n",
    "def build_loss_optimizer(logits,softmax_logits,num_labels):\n",
    "    \n",
    "    ''' Loss and Optimizer builder, with Softmax Cross Entropy loss'''\n",
    "    \n",
    "    with tf.variable_scope('cross_entropy'):\n",
    "        \n",
    "        learning_rate_input = tf.placeholder(\n",
    "            tf.float32, [], name='learning_rate_input')\n",
    "        one_hot_labels = tf.placeholder(\n",
    "            name=\"one_hot_labels\",shape=[None,num_labels],dtype=tf.int32)\n",
    "        labels = tf.placeholder(\n",
    "            name=\"labels\",shape=[None],dtype=tf.int64)\n",
    "        \n",
    "        #cross_entropy_mean = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits=logits)\n",
    "        #cross_entropy_mean = tf.nn.softmax_cross_entropy_with_logits_v2(labels = one_hot_labels,logits=logits,name=\"cross_entropy_mean_v2\")\n",
    "        cross_entropy_mean = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logits,name=\"cross_entropy_mean_sparse\")\n",
    "        loss = tf.reduce_mean(cross_entropy_mean,name=\"cross_entropy_loss\")\n",
    "        \n",
    "        'Toggle between optimizers..'\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_input)\n",
    "        #optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate_input)\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate_input)\n",
    "        \n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # For batch normalization ops update\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            train_step = optimizer.minimize(loss)\n",
    "        \n",
    "        predicted_indices = tf.argmax(softmax_logits, 1, name=\"predicted_indices\")\n",
    "        correct_prediction = tf.equal(predicted_indices, labels, name='correct_prediction') # Labels are 0-indexed\n",
    "        confusion_matrix = tf.confusion_matrix(\n",
    "            labels, predicted_indices, num_classes=num_labels, name=\"confusion_matrix\")\n",
    "        \n",
    "        '''\n",
    "        # Because the accuracy will be calculated across batch splits, this facilitates resetting the metrics inter-epochs\n",
    "        with tf.variable_scope('streaming_ops'):\n",
    "            accuracy,update_op_acc = tf.metrics.accuracy(labels,predicted_indices)\n",
    "            \n",
    "        metrics_vars = tf.contrib.framework.get_variables('cross_entropy/streaming_ops',collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "        reset_metrics_vars = tf.variables_initializer(metrics_vars) # Running sess.run will reset the streaming metrics within epochs\n",
    "        \n",
    "        # For tensorboard \n",
    "        xent_mean_scalar= tf.summary.scalar('cross_entropy_mean',tf.reduce_mean(cross_entropy_mean))\n",
    "        acc_scalar = tf.summary.scalar('accuracy',accuracy)\n",
    "        '''\n",
    "        \n",
    "        return train_step, confusion_matrix,predicted_indices,cross_entropy_mean, learning_rate_input,labels, one_hot_labels,loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative model based off\n",
    "# https://arxiv.org/pdf/1502.01710v5.pdf\n",
    "\n",
    "def build_graph_convnets(cutoff_len):\n",
    "    \n",
    "    num_chars = num_chars_dict\n",
    "    \n",
    "    char_embedding_n_dim = 256\n",
    "    \n",
    "    first_window_size = 7 # First window applied to sentence\n",
    "    second_window_size = 7 # Second window\n",
    "    third_window_size = 3 # And so on...\n",
    "    fourth_window_size = 3\n",
    "    fifth_window_size = 3\n",
    "    sixth_window_size = 3\n",
    "    \n",
    "    # No third, fourth, or fifth pool size, copying the paper's approach \n",
    "    first_pool_size = 3\n",
    "    second_pool_size = 3\n",
    "    sixth_pool_size = 3\n",
    "    \n",
    "    # One channel output for each of the six conv layers\n",
    "    first_layer_no_filters = 1024 \n",
    "    second_layer_no_filters = 1024\n",
    "    third_layer_no_filters = 1024\n",
    "    fourth_layer_no_filters = 1024\n",
    "    fifth_layer_no_filters = 1024\n",
    "    sixth_layer_no_filters = 1024\n",
    "    \n",
    "    first_fc_out = 2048\n",
    "    second_fc_out = 2048\n",
    "    third_fc_out = num_labels \n",
    "    # Hyper parameters end \n",
    "    \n",
    "    reuse_flag = tf.AUTO_REUSE\n",
    "    \n",
    "    # Layer Zero is just to define the input tensors and other 'global' tensors such as dropout probabilities\n",
    "    with tf.variable_scope('layer_zero_convnet',reuse=reuse_flag):\n",
    "        \n",
    "         # This layer defines the character embedding matrix, and returns character embedding for batches of sentences\n",
    "        # tensor: input_tensor is of shape [None, 26] where the first dimension is the batch_size * max_len for that batch\n",
    "        #input_tensor = tf.placeholder(dtype=tf.float32,shape=[None,cutoff_len,num_chars_dict],name=\"input_tensor\") # Unroll the first 2 dimensions at the numpy array before feeding\n",
    "        input_tensor = tf.placeholder(dtype=tf.float32,shape=[None,cutoff_len,num_chars_dict],name=\"input_tensor\") # Unroll the first 2 dimensions at the numpy array before feeding\n",
    "        conv_keep_prob = tf.placeholder(dtype=tf.float32,shape=[],name=\"conv_dropout_prob\")\n",
    "        fc_keep_prob = tf.placeholder(dtype=tf.float32,shape=[],name=\"fc_dropout_prob\")\n",
    "        batch_norm_train = tf.placeholder(name=\"batch_norm_train\", dtype=tf.bool)\n",
    "        print (input_tensor)\n",
    "        \n",
    "        #l0w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        #l0b_init = tf.zeros_initializer()\n",
    "        #char_embedding_mat = tf.get_variable(name=\"char_embedding\",dtype=tf.float32,shape=[num_chars_dict,char_embedding_n_dim],initializer=l0w_init,trainable=True)\n",
    "        #char_emb_bias = tf.get_variable(name=\"char_embedding_bias\",dtype=tf.float32,shape=[char_embedding_n_dim],initializer = l0b_init)\n",
    "        \n",
    "        # This gets the char embeddings for the batch\n",
    "        # Shape is [batch_size * cutoff_len, char_embedding_n_dim]\n",
    "        #input_tensor_2D = tf.reshape(input_tensor,shape=[-1,num_chars_dict]) # Reshaped to 2D with batch_size * cutoff_len as 1st dim\n",
    "        #l1_char_embedding = tf.matmul(input_tensor_2D,char_embedding_mat) + char_emb_bias\n",
    "        \n",
    "        #l1_char_embedding = tf.nn.embedding_lookup(char_embedding_mat,input_tensor,name=\"l1_char_embedding\")\n",
    "        #print(l1_char_embedding)\n",
    "        \n",
    "        # reshape input tensor to 3D matrix for the temporal 1D convolution operation\n",
    "        #input_tensor_with_embed = tf.reshape(l1_char_embedding,shape=[-1,cutoff_len,char_embedding_n_dim])\n",
    "        #input_tensor_with_embed = l1_char_embedding\n",
    "        #input_tensor_embed_relu = tf.nn.relu(input_tensor_with_embed,name=\"input_tensor_embed_relu\")\n",
    "        #input_tensor_batch = tf.contrib.layers.batch_norm(input_tensor_embed_relu,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #print(input_tensor_batch)\n",
    "        \n",
    "        # The input tensor is already one-hot encoded , with a character dictionary size of 26 characters\n",
    "        #input_tensor = tf.placeholder(dtype=tf.float32,shape=[None,cutoff_len,26],name=\"input_tensor\")\n",
    "        \n",
    "        \n",
    "        # Toggle application of either population or sample mean and variance during batch normalization\n",
    "        # is False during validation / testing to allow population statistics to apply\n",
    "        \n",
    "        \n",
    "        #first_embed_batch = tf.contrib.layers.batch_norm(input_tensor_with_embed,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #first_embed_relu = tf.nn.leaky_relu(first_embed_batch,name=\"first_embed_relu\")\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('layer_one_convnet',reuse=reuse_flag):\n",
    "        \n",
    "        #l1w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l1w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        \n",
    "        first_conv_layer = tf.layers.conv1d(inputs = input_tensor, filters = first_layer_no_filters,kernel_size = first_window_size,strides=1,padding='valid',\n",
    "                                            activation=tf.nn.relu, use_bias=True, kernel_initializer= l1w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"first_conv_layer\")\n",
    "        first_conv_batch = tf.contrib.layers.batch_norm(first_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #first_conv_relu = tf.nn.leaky_relu(first_conv_batch,name=\"first_conv_relu\")\n",
    "        first_maxpool_layer = tf.layers.max_pooling1d(inputs = first_conv_batch, pool_size = first_pool_size, strides=first_pool_size,\n",
    "                                                      padding='valid',name=\"first_maxpool_layer\")\n",
    "        \n",
    "\n",
    "        \n",
    "    with tf.variable_scope('layer_two_convnet',reuse=reuse_flag):\n",
    "        \n",
    "        #l2w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l2w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        second_conv_layer = tf.layers.conv1d(inputs = first_maxpool_layer, filters = second_layer_no_filters,kernel_size = second_window_size,strides=1,padding='valid'\n",
    "                                            ,activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"second_conv_layer\")\n",
    "        second_conv_batch = tf.contrib.layers.batch_norm(second_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #second_conv_relu = tf.nn.leaky_relu(second_conv_batch,name=\"second_conv_relu\")\n",
    "        second_maxpool_layer = tf.layers.max_pooling1d(inputs = second_conv_batch, pool_size = second_pool_size, strides= second_pool_size,\n",
    "                                                      padding='valid',name=\"second_maxpool_layer\")\n",
    "\n",
    "\n",
    "        \n",
    "    with tf.variable_scope('layer_three_convnet',reuse=reuse_flag):\n",
    "        #l3w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l3w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        third_conv_layer = tf.layers.conv1d(inputs = second_maxpool_layer, filters = third_layer_no_filters,kernel_size = third_window_size,strides=1,padding='valid',\n",
    "                                            activation=tf.nn.relu, use_bias=True, kernel_initializer= l3w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"third_conv_layer\")\n",
    "        third_conv_batch = tf.contrib.layers.batch_norm(third_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #third_conv_relu = tf.nn.leaky_relu(third_conv_batch,name=\"third_conv_relu\")\n",
    "        \n",
    "    with tf.variable_scope('layer_four_convnet',reuse=reuse_flag):\n",
    "        #l4w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l4w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        fourth_conv_layer = tf.layers.conv1d(inputs = third_conv_batch, filters = fourth_layer_no_filters,kernel_size = fourth_window_size,strides=1,padding='valid',\n",
    "                                            activation=tf.nn.relu, use_bias=True, kernel_initializer= l4w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"fourth_conv_layer\")\n",
    "        fourth_conv_batch = tf.contrib.layers.batch_norm(fourth_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #fourth_conv_relu = tf.nn.leaky_relu(fourth_conv_batch,name=\"fourth_conv_relu\")\n",
    "        \n",
    "    with tf.variable_scope('layer_five_convnet',reuse=reuse_flag):\n",
    "        #l5w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l5w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        fifth_conv_layer = tf.layers.conv1d(inputs = fourth_conv_batch, filters = fifth_layer_no_filters,kernel_size = fifth_window_size,strides=1,padding='valid',\n",
    "                                            activation=tf.nn.relu, use_bias=True, kernel_initializer= l5w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"fifth_conv_layer\")\n",
    "        fifth_conv_batch = tf.contrib.layers.batch_norm(fifth_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #fifth_conv_relu = tf.nn.leaky_relu(fifth_conv_batch,name=\"fifth_conv_relu\")\n",
    "        \n",
    "    with tf.variable_scope('layer_six_convnet',reuse=reuse_flag):\n",
    "        #l6w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l6w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        sixth_conv_layer = tf.layers.conv1d(inputs = fifth_conv_batch, filters = sixth_layer_no_filters,kernel_size = sixth_window_size,strides=1,padding='valid',\n",
    "                                            activation=tf.nn.relu, use_bias=True, kernel_initializer= l6w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"sixth_conv_layer\")\n",
    "        sixth_conv_batch = tf.contrib.layers.batch_norm(sixth_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #sixth_conv_relu = tf.nn.leaky_relu(sixth_conv_batch,name=\"sixth_conv_relu\")\n",
    "        sixth_maxpool_layer = tf.layers.max_pooling1d(inputs = sixth_conv_batch, pool_size = sixth_pool_size, strides=sixth_pool_size,\n",
    "                                                      padding='valid',name=\"sixth_maxpool_layer\")\n",
    "        \n",
    "        #sixth_reshaped_layer = tf.reshape(sixth_maxpool_layer,shape=[-1,sixth_maxpool_layer.get_shape()[1] * sixth_maxpool_layer.get_shape()[2]])\n",
    "        sixth_reshaped_layer = tf.contrib.layers.flatten(sixth_maxpool_layer) \n",
    "        print (sixth_reshaped_layer)\n",
    "        \n",
    "    \n",
    "    with tf.variable_scope('layer_seven_convnet',reuse=reuse_flag):\n",
    "        #l7w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l7w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        l7b_init = tf.zeros_initializer()\n",
    "        \n",
    "        seventh_fc = tf.contrib.layers.fully_connected(inputs=sixth_reshaped_layer,num_outputs=first_fc_out,activation_fn = tf.nn.relu,\n",
    "                                                       weights_initializer = l7w_init,biases_initializer = l7b_init,scope=\"seventh_fc\")\n",
    "        seventh_fc_batch = tf.contrib.layers.batch_norm(seventh_fc,center = True, scale=True,is_training=batch_norm_train)\n",
    "        #seventh_fc_relu = tf.nn.leaky_relu(seventh_fc_batch,name=\"seventh_fc_relu\")\n",
    "        seventh_fc_final = tf.nn.dropout(seventh_fc_batch,keep_prob = fc_keep_prob)\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('layer_eight_convnet',reuse=reuse_flag):\n",
    "        #l8w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l8w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        l8b_init = tf.zeros_initializer()\n",
    "        \n",
    "        eigth_fc = tf.contrib.layers.fully_connected(inputs=seventh_fc_final,num_outputs=second_fc_out,activation_fn = tf.nn.relu,\n",
    "                                                       weights_initializer = l8w_init,biases_initializer = l8b_init,scope=\"eigth_fc\")\n",
    "        eigth_fc_batch = tf.contrib.layers.batch_norm(eigth_fc,center = True, scale=True,is_training=batch_norm_train)\n",
    "        #eigth_fc_relu = tf.nn.leaky_relu(eigth_fc_batch,name=\"eigth_fc_relu\")\n",
    "        eigth_fc_final = tf.nn.dropout(eigth_fc_batch,keep_prob = fc_keep_prob)\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('layer_ninth_convnet',reuse=reuse_flag):\n",
    "        #l9w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32) \n",
    "        l9w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        l9b_init = tf.zeros_initializer()\n",
    "        \n",
    "        ninth_fc = tf.contrib.layers.fully_connected(inputs=eigth_fc_final,num_outputs=num_labels,activation_fn = None,\n",
    "                                                     weights_initializer = l9w_init,biases_initializer = l9b_init,scope=\"ninth_fc\")\n",
    "        #ninth_fc_batch = tf.contrib.layers.batch_norm(ninth_fc,center = True, scale=True,is_training=batch_norm_train)\n",
    "\n",
    "        ninth_final_softmax = tf.nn.softmax(ninth_fc,name=\"ninth_final_softmax\")\n",
    "        \n",
    "        return input_tensor, batch_norm_train, ninth_fc,ninth_final_softmax, conv_keep_prob,fc_keep_prob\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the graph is built, randomly batch the inputs from disk cache, and feed them to the graph to train a model.\n",
    "\n",
    "Training takes between 30-45 min on a Tesla M60 GPU from an AWS g3 EC2 instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the training process\n",
      "Building the graph\n",
      "Tensor(\"layer_zero_convnet/input_tensor:0\", shape=(?, 988, 43), dtype=float32)\n",
      "WARNING:tensorflow:From /home/ubuntu/Desktop/tensorflow-gpu/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "Tensor(\"layer_six_convnet/Flatten/flatten/Reshape:0\", shape=(?, 33792), dtype=float32)\n",
      "Kicking off the session\n",
      "Preparing randomized validation batches\n",
      "The input directory is:/home/ubuntu/Desktop/nlp_deep/amazon_data_sample/valid/inputs/\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8d232504332b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m \u001b[0mexecute_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-8d232504332b>\u001b[0m in \u001b[0;36mexecute_training\u001b[0;34m()\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Preparing randomized validation batches'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         \u001b[0mprepare_randomized_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mone_hot_valid_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcutoff_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mxent_counter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-8d232504332b>\u001b[0m in \u001b[0;36mprepare_randomized_batches\u001b[0;34m(save_dir, batch_size, cutoff_len, file_count)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0mls_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnparr_pad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mone_hot_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'one_hot_labels/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'one_hot_sentence_label_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_item\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Prepare batches in the cache\n",
    "# Used for training batch preparation\n",
    "def prepare_randomized_batches(save_dir,batch_size,cutoff_len,file_count):\n",
    "    \n",
    "    # Clean up for rerunning\n",
    "    if (os.path.exists(save_dir + 'inputs/batch/')):\n",
    "        shutil.rmtree(save_dir + 'inputs/batch/')\n",
    "    os.mkdir(save_dir + 'inputs/batch/')\n",
    "        \n",
    "    if (os.path.exists(save_dir + 'labels/batch/')):\n",
    "        shutil.rmtree(save_dir + 'labels/batch/')\n",
    "    os.mkdir(save_dir + 'labels/batch/')\n",
    "        \n",
    "    if (os.path.exists(save_dir + 'one_hot_labels/batch/')):\n",
    "        shutil.rmtree(save_dir + 'one_hot_labels/batch/')\n",
    "    os.mkdir(save_dir + 'one_hot_labels/batch/')\n",
    "    \n",
    "    # For batch preparation\n",
    "    ls_arrays = []\n",
    "    ls_arrays_one_hot_label = []\n",
    "    ls_arrays_label = []\n",
    "    \n",
    "    # A counter\n",
    "    j = 0\n",
    "    \n",
    "    # Randomize the Batch preparation\n",
    "    os.chdir(save_dir + 'inputs/')\n",
    "    \n",
    "    print ('The input directory is:' + save_dir + 'inputs/')\n",
    "    \n",
    "    file_list = glob.glob('*.npy')\n",
    "    random.shuffle(file_list)\n",
    "    \n",
    "    \n",
    "    # Prepare the batches\n",
    "    for file_item in file_list:\n",
    "    \n",
    "        nparr = np.load(save_dir + 'inputs/' + file_item)\n",
    "                \n",
    "        # Make sure it isnt an empty file\n",
    "        if nparr is not None and nparr.shape[0] != 0:\n",
    "                    \n",
    "            # Bottom-Pad  the sentences by adding 0s to the end of the sentence\n",
    "            # Or Truncate sentences that are longer than the cutoff length\n",
    "            \n",
    "            if (nparr.shape[0] <= cutoff_len):\n",
    "                npad = ((0,cutoff_len - nparr.shape[0]),(0,0))\n",
    "                #npad = (0,cutoff_len - nparr.shape[0])\n",
    "                nparr_pad = np.pad(nparr, pad_width=npad, mode='constant', constant_values=0)\n",
    "            else: # Truncate to the cutoff length\n",
    "                #nparr_pad = nparr[:cutoff_len,:]\n",
    "                nparr_pad = nparr[:cutoff_len,:]\n",
    "            \n",
    "            \n",
    "            ls_arrays.append(nparr_pad.tolist())\n",
    "            \n",
    "            one_hot_label = np.load(save_dir + 'one_hot_labels/' + 'one_hot_sentence_label_' + file_item.split('_')[3])\n",
    "            ls_arrays_one_hot_label.append(one_hot_label.tolist())\n",
    "            \n",
    "            label = np.load(save_dir + 'labels/' + 'sentence_label_' + file_item.split('_')[3])\n",
    "            ls_arrays_label.append(label.tolist())\n",
    "            \n",
    "            j += 1\n",
    "                \n",
    "            # Save batches of the files every batch_size step\n",
    "            if (j % batch_size == 0 or j == file_count):\n",
    "                    \n",
    "                nparr_batch = np.asarray(ls_arrays)\n",
    "                nparr_batch_one_hot_labels = np.asarray(ls_arrays_one_hot_label)\n",
    "                nparr_batch_labels = np.asarray(ls_arrays_label)\n",
    "                    \n",
    "                np.save(save_dir + 'inputs/batch/nparr_batch_' + str(j) + '.npy',nparr_batch)\n",
    "                np.save(save_dir + 'labels/batch/nparr_batch_labels_' + str(j) +  '.npy',nparr_batch_labels)\n",
    "                np.save(save_dir + 'one_hot_labels/batch/nparr_batch_one_hot_labels_' + str(j) +'.npy',nparr_batch_one_hot_labels)\n",
    "                    \n",
    "                ls_arrays = []\n",
    "                ls_arrays_one_hot_label = []\n",
    "                ls_arrays_label = []\n",
    "                \n",
    "                \n",
    "# Gets a random batch of a specified size\n",
    "# This function is not used in the final model\n",
    "# But can be used for mini-batch Gradient Descent \n",
    "def get_a_random_batch(save_dir,batch_size,cutoff_len):\n",
    "\n",
    "        ls_batch_list = [] # Stores the names of all the files randomly selected without replacement\n",
    "    \n",
    "        i = 0\n",
    "        while (i < batch_size):\n",
    "            \n",
    "            rand_choice = random.choice(os.listdir(save_dir + 'inputs/'))\n",
    "            if rand_choice not in ls_batch_list:\n",
    "                ls_batch_list.append(rand_choice)\n",
    "                i = i + 1\n",
    "        \n",
    "        ls_arrays = []\n",
    "        ls_arrays_one_hot_label = []\n",
    "        ls_arrays_label = []\n",
    "        \n",
    "        print ('The list length is:' + str(len(ls_batch_list)))\n",
    "        \n",
    "        for item in ls_batch_list:\n",
    "            \n",
    "            nparr = np.load(save_dir + 'inputs/' + item)\n",
    "            if nparr is not None and nparr.shape[0] != 0:\n",
    "                \n",
    "                #Bottom-Pad  the sentences by adding 0s to the end of the sentence\n",
    "                # Or truncate the sentence upto the length\n",
    "                if (nparr.shape[0] <= cutoff_len):\n",
    "                    npad = ((0,cutoff_len - nparr.shape[0]),(0,0))\n",
    "                    #npad = (0,cutoff_len - nparr.shape[0])\n",
    "                    nparr_pad = np.pad(nparr, pad_width=npad, mode='constant', constant_values=0)\n",
    "                else:\n",
    "                    nparr_pad = nparr[:cutoff_len,:]\n",
    "                \n",
    "                ls_arrays.append(nparr_pad.tolist())\n",
    "                \n",
    "                one_hot_label = np.load(save_dir + 'one_hot_labels/' + 'one_hot_sentence_label_' + item.split('_')[3])\n",
    "                ls_arrays_one_hot_label.append(one_hot_label.tolist())\n",
    "                \n",
    "                label = np.load(save_dir + 'labels/' + 'sentence_label_' + item.split('_')[3])\n",
    "                ls_arrays_label.append(label.tolist())\n",
    "                \n",
    "                \n",
    "        nparr_batch = np.asarray(ls_arrays)\n",
    "        nparr_batch_one_hot_labels = np.asarray(ls_arrays_one_hot_label)\n",
    "        nparr_batch_labels = np.asarray(ls_arrays_label)\n",
    "        \n",
    "        return nparr_batch, nparr_batch_labels, nparr_batch_one_hot_labels\n",
    "              \n",
    "\n",
    "        \n",
    "'''\n",
    "The main training function below. Trains the data using mini-batch gradient descent, \n",
    "and saves checkpoints of the model \n",
    "'''    \n",
    "def execute_training():\n",
    "    \n",
    "    print ('Starting the training process')\n",
    "    \n",
    "    num_epochs = 5000\n",
    "    mini_batch_size = 128\n",
    "    batch_size = 200\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    mini_batch_runs = 100\n",
    "    \n",
    "    if (os.path.exists(train_tensorboard_dir)):\n",
    "        shutil.rmtree(train_tensorboard_dir)\n",
    "    os.mkdir(train_tensorboard_dir)\n",
    "    \n",
    "    \n",
    "    if (os.path.exists(valid_tensorboard_dir)):\n",
    "        shutil.rmtree(valid_tensorboard_dir)\n",
    "    os.mkdir(valid_tensorboard_dir)\n",
    "    \n",
    "    print ('Building the graph')\n",
    "    \n",
    "    # Build graph object\n",
    "    with tf.Graph().as_default() as grap:\n",
    "        #input_tensor, batch_norm_train, logits, softmax_logits,conv_keep_prob,fc_keep_prob = build_graph(cutoff_len)\n",
    "        \n",
    "        input_tensor, batch_norm_train, logits, softmax_logits, conv_keep_prob,fc_keep_prob = build_graph_convnets(cutoff_len)\n",
    "            \n",
    "        train_step, confusion_matrix,predicted_indices,cross_entropy_mean, learning_rate_input,labels,one_hot_labels,loss =  build_loss_optimizer(logits,softmax_logits,num_labels)\n",
    "        \n",
    "            \n",
    "    print ('Kicking off the session')\n",
    "    \n",
    "    # Initiate session for execution\n",
    "    with tf.Session(graph=grap) as sess:\n",
    "        \n",
    "        saver = tf.train.Saver() # For saving checkpoints for test inference \n",
    "        \n",
    "        # Initialize\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        #sess.run(reset_metrics_vars)\n",
    "        \n",
    "        train_writer = tf.summary.FileWriter(train_tensorboard_dir,sess.graph)\n",
    "        valid_writer = tf.summary.FileWriter(valid_tensorboard_dir)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Prepares randomized batches in the /inputs/batch folder\n",
    "        #prepare_randomized_batches(one_hot_train_dir,train_batch_size,cutoff_len,train_count)\n",
    "        \n",
    "        print ('Preparing randomized validation batches')\n",
    "        prepare_randomized_batches(one_hot_valid_dir,batch_size,cutoff_len,valid_count)\n",
    "        \n",
    "        xent_counter = 0\n",
    "        \n",
    "        _guid = uuid.uuid4()\n",
    "        \n",
    "        \n",
    "        for i in range(0,num_epochs):\n",
    "            \n",
    "            print ('Starting Training, the Epoch is:' + str(i + 1))\n",
    "            xent_summary = 0\n",
    "            \n",
    "            # Switch to the /inputs/batch folder and fetch all the batch files \n",
    "            # Which will be passed to the training mechanism\n",
    "            #os.chdir(one_hot_train_dir + 'inputs/batch/')\n",
    "            #train_batch_files = glob.glob('*.npy')\n",
    "            #random.shuffle(train_batch_files)\n",
    "            \n",
    "            train_conf_matrix = None\n",
    "            \n",
    "            #for file in train_batch_files:\n",
    "            for j in range(0,mini_batch_runs): #mini batch runs per epoch\n",
    "                \n",
    "                \n",
    "                \n",
    "                #print ('Get a random training batch:'  + str(j + 1))\n",
    "                train_batch, train_batch_labels, train_batch_one_hot = get_a_random_batch(one_hot_train_dir,mini_batch_size,cutoff_len)\n",
    "                #print (train_batch.shape)\n",
    "                #print (train_batch_one_hot)\n",
    "                #print (train_batch_labels)\n",
    "                \n",
    "                #train_batch = np.load(one_hot_train_dir + 'inputs/batch/' + file)\n",
    "                #train_labels = np.load(one_hot_train_dir + 'labels/batch/nparr_batch_labels_' + file.split('_')[2])\n",
    "                #train_one_hot_labels = np.load(one_hot_train_dir + 'one_hot_labels/batch/nparr_batch_one_hot_labels_' + file.split('_')[2])\n",
    "                \n",
    "                # Kick off the training\n",
    "                _,los,conf_matrix,xent_mean,pi = sess.run(\n",
    "                [train_step,loss,confusion_matrix,cross_entropy_mean,predicted_indices],\n",
    "                feed_dict = {input_tensor : train_batch,\n",
    "                             labels: train_batch_labels,\n",
    "                             one_hot_labels : train_batch_one_hot,\n",
    "                             learning_rate_input : learning_rate,\n",
    "                             batch_norm_train : True,\n",
    "                             conv_keep_prob : 1.0,\n",
    "                             fc_keep_prob : 0.5\n",
    "                    })\n",
    "                \n",
    "                if (train_conf_matrix is None):\n",
    "                    train_conf_matrix = conf_matrix\n",
    "                else:\n",
    "                    train_conf_matrix += conf_matrix\n",
    "            \n",
    "                \n",
    "                xent_counter += 1\n",
    "                xent_summary += np.sum(xent_mean)\n",
    "                #train_writer.add_summary(xent_scalar,xent_counter)\n",
    "                with open (log_dir + str(_guid) + '.txt','a') as eg:\n",
    "                    eg.write('The training cross entropy sum and avg at step:' + str(xent_counter) + ' is:')\n",
    "                    eg.write(str(np.sum(xent_mean)) + ';')\n",
    "                    eg.write(str(los) + '\\n')\n",
    "                    \n",
    "                xent_train_summary = tf.Summary(value=[tf.Summary.Value(tag=\"cross_entropy_sum\",simple_value=np.sum(xent_mean))])\n",
    "                #xent_train_summary.value.add(tag=\"cross_entropy_sum\",simple_value = np.sum(xent_mean))\n",
    "                train_writer.add_summary(xent_train_summary,xent_counter)\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "            # Print confusion matrix out\n",
    "            with open (log_dir + str(_guid) + '.txt','a') as eg:\n",
    "                eg.write('\\n' + 'Training Confusion Matrix:' + '\\n' + str(train_conf_matrix))\n",
    "                true_pos = np.sum(np.diag(train_conf_matrix))\n",
    "                all_pos = np.sum(train_conf_matrix)\n",
    "                eg.write('\\n' + 'Training Accuracy is: ' + str(float(true_pos / all_pos)) + '\\n')  # Another way to get accuracy!\n",
    "                eg.write('Average cross entropy error is:' + str(xent_summary/(mini_batch_runs * mini_batch_size)) + '\\n')\n",
    "            \n",
    "            loss_train_summary = tf.Summary(value=[tf.Summary.Value(tag=\"loss_train_summary\",simple_value=xent_summary/(mini_batch_runs * mini_batch_size))])\n",
    "                \n",
    "            \n",
    "            #loss_train_summary.value.add(tag=\"loss_train\",simple_value = xent_summary/(mini_batch_runs * mini_batch_size))\n",
    "            \n",
    "            acc_train_summary = tf.Summary(value=[tf.Summary.Value(tag=\"acc_train_summary\",simple_value=float(true_pos / all_pos))])\n",
    "            \n",
    "            #acc_train_summary.value.add(tag=\"train_accuracy\",simple_value = float(true_pos / all_pos))\n",
    "            \n",
    "            train_writer.add_summary(loss_train_summary,i)\n",
    "            train_writer.add_summary(acc_train_summary,i)\n",
    "            \n",
    "            #_,scalar = sess.run([accuracy,acc_scalar])\n",
    "            #train_writer.add_summary(scalar,i)\n",
    "            \n",
    "            #sess.run(reset_metrics_vars)\n",
    "            \n",
    "            \n",
    "            # Save model every 10 epochs\n",
    "            if ((i + 1) % 10 == 0):\n",
    "                #print ('Saving checkpoint after epoch:' + str(i + 1))\n",
    "                saver.save(sess=sess,save_path=checkpoint_dir + 'Char_Sent_Classification_CNN.ckpt',global_step = i )\n",
    "                \n",
    "            #if ((i + 1) % 20 == 0):\n",
    "            # Run on validation data to check validation accuracy\n",
    "            #print ('Training Epoch ' + str(i + 1) + ' is complete. Running Validation Stats..')\n",
    "             \n",
    "            os.chdir(one_hot_valid_dir + 'inputs/batch/')\n",
    "            valid_batch_files = glob.glob('*.npy')\n",
    "            random.shuffle(valid_batch_files)\n",
    "            \n",
    "            valid_conf_matrix = None\n",
    "            \n",
    "            for file in valid_batch_files:\n",
    "                \n",
    "                valid_batch = np.load(one_hot_valid_dir + 'inputs/batch/' + file)\n",
    "                valid_labels = np.load(one_hot_valid_dir + 'labels/batch/nparr_batch_labels_' + file.split('_')[2])\n",
    "                valid_one_hot_labels = np.load(one_hot_valid_dir + 'one_hot_labels/batch/nparr_batch_one_hot_labels_' + file.split('_')[2])\n",
    "               \n",
    "                \n",
    "                val_conf_matrix = sess.run(\n",
    "                confusion_matrix,\n",
    "                feed_dict = {input_tensor : valid_batch,\n",
    "                            labels: valid_labels,\n",
    "                            one_hot_labels : valid_one_hot_labels,\n",
    "                            batch_norm_train : False,\n",
    "                            conv_keep_prob : 1.0,\n",
    "                            fc_keep_prob : 1.0\n",
    "                         \n",
    "                })\n",
    "                \n",
    "                if (valid_conf_matrix is None):\n",
    "                    valid_conf_matrix = val_conf_matrix\n",
    "                else:\n",
    "                    valid_conf_matrix += val_conf_matrix\n",
    "                    \n",
    "            \n",
    "            #val_acc,val_summary = sess.run([accuracy,acc_scalar])\n",
    "            #valid_writer.add_summary(val_summary,i)\n",
    "            \n",
    "            #sess.run(reset_metrics_vars)\n",
    "            \n",
    "            with open (log_dir + str(_guid) + '.txt','a') as eg:\n",
    "                eg.write('Validation Confusion Matrix:' + '\\n' + str(valid_conf_matrix))\n",
    "                true_pos = np.sum(np.diag(valid_conf_matrix))\n",
    "                all_pos = np.sum(valid_conf_matrix)\n",
    "                eg.write('\\n' + 'Validation Accuracy is: ' + str(float(true_pos / all_pos)) + '\\n') \n",
    "\n",
    "                \n",
    "            acc_valid_summary = tf.Summary(value=[tf.Summary.Value(tag=\"acc_valid_summary\",simple_value=float(true_pos / all_pos))])\n",
    "            \n",
    "            #acc_valid_summary.value.add(tag=\"train_accuracy\",simple_value = float(true_pos / all_pos))\n",
    "            valid_writer.add_summary(acc_valid_summary,i)\n",
    "            \n",
    "            \n",
    "            \n",
    "execute_training()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the saved checkpoints to do inference on the batch of test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(cutoff_len):\n",
    "    \n",
    "    with tf.Graph().as_default() as grap:\n",
    "        input_tensor, batch_norm_train, logits, softmax_logits,conv_keep_prob,fc_keep_prob = build_graph(cutoff_len)\n",
    "        \n",
    "    with open(one_hot_test_dir + 'ytest.txt','w') as predfile:\n",
    "    \n",
    "        with tf.Session(graph=grap) as sess:\n",
    "        \n",
    "            init = tf.global_variables_initializer()\n",
    "            sess = tf.Session()\n",
    "            sess.run(init)\n",
    "        \n",
    "            saver = tf.train.Saver()\n",
    "        \n",
    "            checkpoint_file_path = '/home/ubuntu/Desktop/challenge/offline_challenge_to_send/offline_challenge_to_send/checkpoints/Char_Sent_Classification_CNN.ckpt-99'\n",
    "\n",
    "            print ('Loading checkpoint file:' + checkpoint_file_path)\n",
    "            saver.restore(sess,checkpoint_file_path)\n",
    "\n",
    "            os.chdir(one_hot_test_dir + 'inputs/')\n",
    "            test_files = glob.glob('*.npy')\n",
    "               \n",
    "            for file in test_files:\n",
    "            \n",
    "                _idx = file.split('_')[3].replace('.npy','')\n",
    "            \n",
    "            \n",
    "                nparr = np.load(one_hot_test_dir + 'inputs/' + file)\n",
    "                npad = ((0,cutoff_len - nparr.shape[0]),(0,0))\n",
    "                nparr_pad = np.pad(nparr, pad_width=npad, mode='constant', constant_values=0)\n",
    "            \n",
    "                nparr_2 = np.expand_dims(nparr_pad,axis=0)\n",
    "             \n",
    "                predictions,soft = sess.run(\n",
    "                [\n",
    "                    logits,softmax_logits\n",
    "                ],feed_dict = {input_tensor : nparr_2,\n",
    "                                batch_norm_train : False,\n",
    "                                conv_keep_prob : 1.0,\n",
    "                                fc_keep_prob : 1.0})\n",
    "                \n",
    "                predicted_index = tf.argmax(soft, axis=1)\n",
    "                \n",
    "                predfile.write(str(_idx,) + ',' + str(predicted_index.eval(session=sess)[0]) + '\\n')\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "#inference(cutoff_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APPENDIX below: \n",
    "\n",
    "This is an alternative model based off Text Understanding From Scratch by Xiang ZHang and Yann LeCun. It is not used in the final model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
