{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random , math, shutil, glob, uuid\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from stop_words import get_stop_words\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "np.set_printoptions(threshold=np.nan)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Initialize all the Global Variables for Train, Validation, and Test Data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Below are all the \"global\" constants used in the model (not hyperparameters or graph parameters)'''\n",
    "\n",
    "\n",
    "\n",
    "home_dir = '/home/ubuntu/Desktop/nlp_deep/amazon_data_sample/'\n",
    "one_hot_train_dir = home_dir + 'train/'\n",
    "one_hot_valid_dir = home_dir + 'valid/'\n",
    "one_hot_test_dir = home_dir + 'test/'\n",
    "train_file = home_dir + 'train_sample.csv'\n",
    "test_file = home_dir + 'test_sample.csv'\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "home_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/'\n",
    "one_hot_train_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/train/'\n",
    "one_hot_valid_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/valid/'\n",
    "one_hot_test_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/test/'\n",
    "train_file = home_dir + 'amazon_review_full_csv/train_sample.csv'\n",
    "test_file = home_dir + 'amazon_review_full_csv/test_sample.csv'\n",
    "'''\n",
    "\n",
    "\n",
    "# Stores all the checkpoint models\n",
    "checkpoint_dir = '/home/ubuntu/Desktop/nlp_deep/checkpoints/'\n",
    "#checkpoint_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/checkpoints/'\n",
    "\n",
    "\n",
    "\n",
    "# Index denotes the column placement in the character embedding matrix\n",
    "# This is used to create the one-hot character vector of sentence inputs\n",
    "\n",
    "'''\n",
    "one_hot_column_label = {'a':68,'b':67,'c':66,'d':65,'e':64,'f':63,'g':62,'h':61,'i':60,'j':59,'k':58,'l':57,'m':56,'n':55,'o':54,\n",
    "                        'p':53,'q':52,'r':51,'s':50,'t':49,'u':48,'v':47,'w':46,'x':45,'y':44,'z':43,'0':42,'1':41,'2':40,'3':39,'4':38,'5':37,'6':36,'7':35,\n",
    "                        '8':34,'9':33,'-':32,',':31,';':30,'.':29,'!':28,'?':27,':':26,'\"':25,'\\'':24,'/':23,'\\\\':22,'|':21,'_':20,\n",
    "                        '@':19,'#':18,'$':17,'%':16,'^':15,'&':14,'*':13,'~':12,'`':11,'+':10,'-':9,'=':8,'<':7,'>':6,'(':5,')':4,'[':3,\n",
    "                        ']':2,'{':1,'}':0}\n",
    "'''\n",
    "\n",
    "one_hot_column_label = {'a':42,'b':41,'c':40,'d':39,'e':38,'f':37,'g':36,'h':35,'i':34,'j':33,'k':32,'l':31,'m':30,'n':29,'o':28,\n",
    "                        'p':27,'q':26,'r':25,'s':24,'t':23,'u':22,'v':21,'w':20,'x':19,'y':18,'z':17,'0':16,'1':15,'2':14,'3':13,'4':12,'5':11,'6':10,'7':9,\n",
    "                        '8':8,'9':7,'-':6,'#':5,'.':4,'!':3,'?':2,':':1,';':0}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "one_hot_column_label = {'a':0,'b':1,'c':2,'d':3,'e':4,'f':5,'g':6,'h':7,'i':8,'j':9,'k':10,'l':11,'m':12,'n':13,'o':14,\n",
    "                        'p':15,'q':16,'r':17,'s':18,'t':19,'u':20,'v':21,'w':22,'x':23,'y':24,'z':25,'A':26,'B':27,'C':28,'D':29,'E':30,'F':31,\n",
    "                        'G':32,'H':33,'I':34,'J':35,'K':36,'L':37,'M':38,'N':39,'O':40,'P':41,'Q':42,'R':43,'S':44,'T':45,\n",
    "                        'U':46,'V':47,'W':48,'X':49,'Y':50,'Z':51,'0':52,'1':53,'2':54,'3':55,'4':56,'5':57,'6':58,'7':59,\n",
    "                        '8':60,'9':61,'-':62,',':63,';':64,'.':65,'!':66,'?':67,':':68,'\"':69,'\\'':70,'/':71,'\\\\':72,'|':73,'_':74,\n",
    "                        '@':75,'#':76,'$':77,'%':78,'^':79,'&':80,'*':81,'~':82,'`':83,'+':84,'-':85,'=':86,'<':87,'>':88,'(':89,')':90,'[':91,\n",
    "                        ']':92,'{':93,'}':94}\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "# For 1-1 mapping of character to letter encoding\n",
    "one_hot_column_label = {'a':1,'b':2,'c':3,'d':4,'e':5,'f':6,'g':7,'h':8,'i':9,'j':10,'k':11,'l':12,'m':13,'n':14,'o':15,\n",
    "                        'p':16,'q':17,'r':18,'s':19,'t':20,'u':21,'v':22,'w':23,'x':24,'y':25,'z':26,'0':27,'1':28,'2':29,'3':30,'4':31,'5':32,'6':33,'7':34,\n",
    "                        '8':35,'9':36,'?':37,'!':38,' ':39,'$':40,'.':41}\n",
    "'''\n",
    "'''\n",
    "one_hot_column_label = {'a':0,'b':1,'c':2,'d':3,'e':4,'f':5,'g':6,'h':7,'i':8,'j':9,'k':10,'l':11,'m':12,'n':13,'o':14,\n",
    "                        'p':15,'q':16,'r':17,'s':18,'t':19,'u':20,'v':21,'w':22,'x':23,'y':24,'z':25,'0':26,'1':27,'2':28,'3':29,'4':30,'5':31,'6':32,'7':33,\n",
    "                        '8':34,'9':35,'?':36,'!':37,'$':38,'.':39,' ':40}\n",
    "\n",
    "'''\n",
    "\n",
    "num_chars_dict = 43\n",
    "\n",
    "# Number of labels to classify\n",
    "num_labels = 5\n",
    "\n",
    "# Tensorboard directories\n",
    "train_tensorboard_dir = home_dir + 'train/tensorboard/'\n",
    "valid_tensorboard_dir = home_dir + 'valid/tensorboard/'\n",
    "\n",
    "log_dir = '/home/ubuntu/Desktop/nlp_deep/Char_CNN_log/'\n",
    "#log_dir = '/home/nitin/Desktop/sdb1/all_files/amazon_reviews_data/log/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a 80-20 train-validation split. Get the length of the longest sentence, which will be used to build the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def strip_characters(sent):\n",
    "    \n",
    "    # Strip unnecessary characters from the review\n",
    "    \n",
    "    #sent = sent.replace('-','')\n",
    "    sent = sent.replace(',','')\n",
    "    #sent = sent.replace(';','')\n",
    "    #sent = sent.replace('.','')\n",
    "    #sent = sent.replace(':','')\n",
    "    sent = sent.replace('\"','')\n",
    "    \n",
    "    sent = sent.replace('/','')\n",
    "    sent = sent.replace('\\\\','')\n",
    "    sent = sent.replace('|','')\n",
    "    sent = sent.replace('_','')\n",
    "    sent = sent.replace('@','')\n",
    "    #sent = sent.replace('#','')\n",
    "    sent = sent.replace('$','')\n",
    "    sent = sent.replace('^','')                        \n",
    "    sent = sent.replace('&','')                        \n",
    "    sent = sent.replace('*','')                        \n",
    "    sent = sent.replace('~','')                            \n",
    "    sent = sent.replace('`','')    \n",
    "    sent = sent.replace('+','')                        \n",
    "    sent = sent.replace('-','')    \n",
    "    sent = sent.replace('=','')\n",
    "    sent = sent.replace('<','')\n",
    "    sent = sent.replace('>','')                        \n",
    "    sent = sent.replace('(','')                        \n",
    "    sent = sent.replace(')','')                        \n",
    "    sent = sent.replace('[','')                        \n",
    "    sent = sent.replace(']','')                        \n",
    "    sent = sent.replace('{','')                        \n",
    "    sent = sent.replace('}','')        \n",
    "    \n",
    "    \n",
    "    # remove the stop words\n",
    "    stop_words = list(get_stop_words('en'))\n",
    "    nltk_words = list(stopwords.words('english'))\n",
    "    stop_words.extend(nltk_words)\n",
    "    \n",
    "    #print (sent)\n",
    "    sent_words = sent.split(' ')\n",
    "    sent_stopped_words = [w for w in sent_words if not w in stop_words]\n",
    "    stopped_sent = ' '.join(sent_stopped_words)\n",
    "    #print (stopped_sent)\n",
    "    \n",
    "                        \n",
    "    return stopped_sent\n",
    "                        \n",
    "                        \n",
    "nltk.download('stopwords')                        \n",
    "# Combine and label before train / valid split\n",
    "train_data = pd.read_csv(train_file)\n",
    "test_data = pd.read_csv(test_file)\n",
    "\n",
    "# Are the datasets balanced - yes\n",
    "#print ('Checking Dataset Balance')\n",
    "#train_data.hist(column='Review')\n",
    "#test_data.hist(column='Review')\n",
    "\n",
    "\n",
    "# We get the max_len for padding and for the convolution filter definition in the model\n",
    "max_len = 0\n",
    "total_len = 0\n",
    "min_len = 999\n",
    "ls_len = []\n",
    "\n",
    "\n",
    "stripped_file_train =  home_dir + 'file_strip_train.csv'   \n",
    "stripped_file_test = home_dir + 'file_strip_test.csv'\n",
    "\n",
    "with open(stripped_file_train,'w') as strip:   \n",
    "    \n",
    "    strip.write('Review' + ',' + 'Description' + '\\n')\n",
    "\n",
    "    for index,row in train_data.iterrows():\n",
    "    \n",
    "        sent_row = strip_characters(row[2])\n",
    "        strip.write(str(row[0]) + ','  + sent_row + '\\n')    \n",
    "        \n",
    "        \n",
    "with open(stripped_file_test,'w') as strip:\n",
    "    \n",
    "    strip.write('Review' + ',' + 'Description' + '\\n')\n",
    "    \n",
    "    for index,row in test_data.iterrows():\n",
    "        \n",
    "        sent_row = strip_characters(row[2])\n",
    "        strip.write(str(row[0]) + ','  + sent_row + '\\n')    \n",
    "\n",
    "        \n",
    "\n",
    "strip_train_data = pd.read_csv(stripped_file_train)\n",
    "X_test = pd.read_csv(stripped_file_test)\n",
    "\n",
    "        \n",
    "label_data = strip_train_data.loc[:,'Review']\n",
    "all_data = strip_train_data\n",
    "all_data = all_data.append(X_test,ignore_index=True)\n",
    "\n",
    "\n",
    "\n",
    "for index, row in all_data.iterrows():\n",
    "    total_len += len(row[1])\n",
    "    ls_len.append(len(row[1]))\n",
    "    if (max_len < len(row[1])):\n",
    "        max_len = len(row[1])\n",
    "    if (min_len > len(row[1])):\n",
    "        min_len = len(row[1])\n",
    "            \n",
    "np_len = np.asarray(ls_len)\n",
    "\n",
    "print ('The longest sentence is of length:' + str(max_len))\n",
    "print ('The shortest sentence is of length:' + str(min_len))\n",
    "print ('The average length is:' + str(total_len/strip_train_data.shape[0]))\n",
    "print ('The 75th percentile is:' + str(np.percentile(np_len,75)))\n",
    "print ('The 90th percentile is:' + str(np.percentile(np_len,90)))\n",
    "\n",
    "\n",
    "\n",
    "# Train and test split (stratified sampling, 70-30 split)\n",
    "X_train,X_valid,y_train,y_valid = train_test_split(strip_train_data,label_data,test_size=0.2,train_size=0.8,random_state=13814,shuffle=True,stratify=label_data)\n",
    "\n",
    "# Verify that stratified sampling worked (histogram distributions must be same)\n",
    "print ('Verify Stratified Sampling')\n",
    "X_train.hist(column=\"Review\") # First histogram is training\n",
    "X_valid.hist(column=\"Review\") # Second histogram is validation\n",
    "\n",
    "\n",
    "'Some more \"global\" variables'\n",
    "cutoff_len = int(np.percentile(np_len,100)) # a graph parameter\n",
    "print(cutoff_len)\n",
    "train_count = X_train.shape[0]\n",
    "valid_count = X_valid.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now convert each sentence to one-hot character matrix to drive the non-static character embedding matrix creation, and store this character matrix representation in disk cache.\n",
    "\n",
    "Also store in disk cache, the label and a one-hot representation of the label, for the cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the one-hot character vector representation of sentences which will be used as the layer before a\n",
    "# character-embedding matrix\n",
    "def save_numpy_one_hot(X_dataset,unique_char_size,save_dir,one_hot_column_label):\n",
    "    \n",
    "    for index,row in X_dataset.iterrows():\n",
    "        \n",
    "        #sent = row[2]\n",
    "        sent = row[1]\n",
    "        #print (sent)\n",
    "        np_one_hot = np.zeros(shape=(len(sent),unique_char_size),dtype=np.int32)\n",
    "        #np_one_hot = np.zeros(shape=len(sent),dtype=np.int32)\n",
    "        \n",
    "        for i in range(0,len(sent)):\n",
    "            if (sent[i].lower() in one_hot_column_label):\n",
    "                np_one_hot[i][one_hot_column_label[sent[i].lower()]] = 1 # else, it remains 0 encoded.\n",
    "                #np_one_hot[i] = one_hot_column_label[sent[i].lower()] # For character embedding lookup\n",
    "            #else:\n",
    "            #    np_one_hot[i][41] = 1 # unknown character\n",
    "        #print(np_one_hot)\n",
    "            \n",
    "        # Save the one-hot encoding for each sentence \n",
    "        np.save(save_dir + 'inputs/' + 'one_hot_sentence_' + str(index) + '.npy',np_one_hot)\n",
    "        \n",
    "        #Save the label\n",
    "        lbl = int(row[0]) - 1 # Must start with 0-indexing\n",
    "        np.save(save_dir + 'labels/' +  'sentence_label_' + str(index) + '.npy',np.asarray(lbl))\n",
    "        \n",
    "        # Save a one-hot version of the label (for xentropy loss function)\n",
    "        np_one_hot_label = np.zeros(shape=(5),dtype=np.int32)\n",
    "        np_one_hot_label[lbl] = 1\n",
    "        \n",
    "        np.save(save_dir + 'one_hot_labels/' + 'one_hot_sentence_label_' + str(index) + '.npy',np_one_hot_label)\n",
    "        \n",
    "        \n",
    "def save_numpy_one_hot_test(X_dataset,unique_char_size,save_dir,one_hot_column_label):\n",
    "    \n",
    "    for index,row in X_dataset.iterrows():\n",
    "        \n",
    "        sent = row[1]\n",
    "        np_one_hot = np.zeros(shape=(len(sent),unique_char_size),dtype=np.int32)\n",
    "        #np_one_hot = np.zeros(shape=len(sent),dtype=np.int32)\n",
    "        \n",
    "        for i in range(0,len(sent)):\n",
    "            if (sent[i].lower() in one_hot_column_label):\n",
    "                np_one_hot[i][one_hot_column_label[sent[i].lower()]] = 1\n",
    "                #np_one_hot[i] = one_hot_column_label[sent[i].lower()]\n",
    "            #else:\n",
    "            #    np_one_hot[i][41] = 1\n",
    "            \n",
    "        # Save the one-hot encoding for each sentence \n",
    "        np.save(save_dir + 'inputs/' + 'one_hot_sentence_' + str(index) + '.npy',np_one_hot)\n",
    "        \n",
    "        \n",
    "def inp_create_one_hot_char_mat(unique_char_size,train_dir,valid_dir,test_dir):\n",
    "    \n",
    "    save_numpy_one_hot(X_train,unique_char_size,train_dir,one_hot_column_label)\n",
    "    save_numpy_one_hot(X_valid,unique_char_size,valid_dir,one_hot_column_label)\n",
    "    save_numpy_one_hot_test(X_test,unique_char_size,test_dir,one_hot_column_label)\n",
    "    \n",
    "\n",
    "# One  - off folder creation \n",
    "if (not os.path.exists(one_hot_train_dir)):\n",
    "        \n",
    "        print ('Setting up directories and preparing one-hot character vectors')\n",
    "        print ('Number chars:' + str(num_chars_dict))\n",
    "        os.mkdir(one_hot_train_dir)\n",
    "        os.mkdir(one_hot_valid_dir)\n",
    "        os.mkdir(one_hot_test_dir)\n",
    "        os.mkdir(one_hot_train_dir + 'inputs/')\n",
    "        os.mkdir(one_hot_valid_dir + 'inputs/')\n",
    "        os.mkdir(one_hot_test_dir + 'inputs/')\n",
    "        os.mkdir(one_hot_train_dir + 'labels/')\n",
    "        os.mkdir(one_hot_valid_dir + 'labels/')\n",
    "        os.mkdir(one_hot_train_dir + 'one_hot_labels/')\n",
    "        os.mkdir(one_hot_valid_dir + 'one_hot_labels/')\n",
    "        os.mkdir(one_hot_train_dir + 'tensorboard/')\n",
    "        os.mkdir(one_hot_valid_dir + 'tensorboard/')\n",
    "        inp_create_one_hot_char_mat(num_chars_dict,one_hot_train_dir,one_hot_valid_dir,one_hot_test_dir)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, build the graph and define the loss and optimizer operations.\n",
    "\n",
    "The below function is not used - please see build_graph_convnet function below\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start to build the model\n",
    "# Let's build the graph first\n",
    "\n",
    "# This function is Not Used in the main model - please see build_graph_convnets below\n",
    "def build_graph(cutoff_len):\n",
    "    \n",
    "    # Hyper Parameters with descriptions\n",
    "    num_char_dict = num_chars_dict\n",
    "    char_embedding_n_dim = 256 # The number of columns in the character embedding matrix\n",
    "    \n",
    "    first_window_size = 3 # First window applied to sentence\n",
    "    second_window_size = 4 # Second window\n",
    "    third_window_size = 5 # And so on...\n",
    "    fourth_window_size = 6\n",
    "    \n",
    "    \n",
    "    first_layer_no_filters = 256\n",
    "    second_layer_no_filters = 256\n",
    "    third_layer_no_filters = 256\n",
    "    fourth_layer_no_filters = 256\n",
    "    \n",
    "    first_fc_out = 1024\n",
    "    second_fc_out = 1024\n",
    "    \n",
    "    # Hyper parameters end \n",
    "    \n",
    "    reuse_flag = tf.AUTO_REUSE\n",
    "    \n",
    "    # Build the graph\n",
    "    with tf.variable_scope('layer_one_char_embedding',reuse=reuse_flag):\n",
    "        \n",
    "        batch_norm_train = tf.placeholder(name=\"batch_norm_train\", dtype=tf.bool)\n",
    "        \n",
    "        '''This layer defines the character embedding matrix, and returns character embedding for batches of sentences'''\n",
    "        \n",
    "        # The input_tensor is of shape [None, cutoff_len, num_char_dict] where cutoff_len is the length limit for the 2-D embedding matrix\n",
    "        #input_tensor = tf.placeholder(dtype=tf.float32,shape=[None,cutoff_len,num_char_dict],name=\"input_tensor\") \n",
    "        input_tensor = tf.placeholder(dtype=tf.int32,shape=[None,cutoff_len],name=\"input_tensor\") \n",
    "        \n",
    "        # Initializer for embedding matrix\n",
    "        l1w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        # The character embedding matrix declared below. The 2nd rank is the number of dimensions representing each character\n",
    "        char_embedding_mat = tf.get_variable(name=\"char_embedding\",dtype=tf.float32,shape=[num_char_dict,char_embedding_n_dim],initializer=l1w_init)\n",
    "        \n",
    "        input_tensor_2D = tf.reshape(input_tensor,shape=[-1,num_char_dict]) \n",
    "        #l1_char_embedding = tf.matmul(input_tensor_2D,char_embedding_mat)\n",
    "        \n",
    "        l1_char_embedding = tf.nn.embedding_lookup(char_embedding_mat,input_tensor,name=\"l1_char_embedding\")\n",
    "        print(l1_char_embedding)\n",
    "        \n",
    "        # reshape char embedding matrix to 3D matrix for the temporal 1D convolution operation\n",
    "        input_tensor_with_embed = tf.reshape(l1_char_embedding,shape=[-1,cutoff_len,char_embedding_n_dim])\n",
    "        \n",
    "        # Some keep probabilities for adding dropout\n",
    "        conv_keep_prob = tf.placeholder(dtype=tf.float32,shape=[],name=\"conv_dropout_prob\")\n",
    "        fc_keep_prob = tf.placeholder(dtype=tf.float32,shape=[],name=\"fc_dropout_prob\")\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('layer_two_kernels',reuse=reuse_flag):\n",
    "        \n",
    "        l2w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        \n",
    "        # 1-D convolution layer\n",
    "        first_conv_layer = tf.layers.conv1d(inputs = input_tensor_with_embed, filters = first_layer_no_filters,kernel_size = first_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"first_conv_layer\")\n",
    "        # Batch normalization\n",
    "        first_conv_batch = tf.contrib.layers.batch_norm(first_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        \n",
    "        # Pooling Layer\n",
    "        #first_sumpool_layer = tf.reduce_sum(first_conv_batch, axis=1,keepdims=False) # Global sumpool 1-D\n",
    "        first_maxpool_layer = tf.reduce_max(first_conv_batch, axis=1,keepdims=False) # Global maxpool 1-D\n",
    "        \n",
    "        # Dropout\n",
    "        first_layer = tf.nn.dropout(first_maxpool_layer,keep_prob = conv_keep_prob )\n",
    "        \n",
    "        \n",
    "        # Repeat for second stack\n",
    "        second_conv_layer = tf.layers.conv1d(inputs = input_tensor_with_embed, filters = second_layer_no_filters,kernel_size = second_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(), name=\"second_conv_layer\")\n",
    "        second_conv_batch = tf.contrib.layers.batch_norm(second_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        #second_sumpool_layer = tf.reduce_sum(second_conv_batch, axis=1,keepdims=False) \n",
    "        second_maxpool_layer = tf.reduce_max(second_conv_batch, axis=1,keepdims=False) \n",
    "        \n",
    "        second_layer = tf.nn.dropout(second_maxpool_layer,keep_prob = conv_keep_prob )\n",
    "        \n",
    "        \n",
    "        #...and third stack...\n",
    "        third_conv_layer = tf.layers.conv1d(inputs = input_tensor_with_embed, filters = third_layer_no_filters,kernel_size = third_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(), name=\"third_conv_layer\")\n",
    "        third_conv_batch = tf.contrib.layers.batch_norm(third_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        #third_sumpool_layer = tf.reduce_sum(third_conv_batch, axis=1,keepdims=False) \n",
    "        third_maxpool_layer = tf.reduce_max(third_conv_batch, axis=1,keepdims=False) \n",
    "        \n",
    "        \n",
    "        third_layer = tf.nn.dropout(third_maxpool_layer,keep_prob = conv_keep_prob )\n",
    "        \n",
    "        \n",
    "        #..and fourth stack..\n",
    "        fourth_conv_layer = tf.layers.conv1d(inputs = input_tensor_with_embed, filters = fourth_layer_no_filters,kernel_size = fourth_window_size,strides=1,padding='valid',\n",
    "                                            data_format='channels_last',activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(), name=\"fourth_conv_layer\")\n",
    "        fourth_conv_batch = tf.contrib.layers.batch_norm(fourth_conv_layer,center=True,scale=False,is_training=batch_norm_train)\n",
    "        #fourth_sumpool_layer = tf.reduce_sum(fourth_conv_batch, axis=1,keepdims=False) # Sums across the 'length' \n",
    "        fourth_maxpool_layer = tf.reduce_sum(fourth_conv_batch, axis=1,keepdims=False) # Sums across the 'length' \n",
    "        \n",
    "        fourth_layer = tf.nn.dropout(fourth_maxpool_layer,keep_prob = conv_keep_prob )\n",
    "        \n",
    "        all_conv = tf.concat(axis=1,values=[first_layer,second_layer,third_layer,fourth_layer],\n",
    "                                            #fifth_conv_dropout,sixth_conv_dropout,seventh_conv_dropout],\n",
    "                                            name=\"all_conv\")\n",
    "        \n",
    "        print ('Stacked Convolution Tensor as below')\n",
    "        print(all_conv)\n",
    "        \n",
    "        \n",
    "    # Now pass it through three FC layers\n",
    "    with tf.variable_scope('layer_three_fc',reuse=reuse_flag):\n",
    "       \n",
    "        l3w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        #l3b_init = tf.random_normal_initializer(mean=0, stddev=0.1, dtype=tf.float32)\n",
    "        l3b_init = tf.zeros_initializer()\n",
    "        \n",
    "        first_fc = tf.contrib.layers.fully_connected(inputs=all_conv,num_outputs=first_fc_out,activation_fn = tf.nn.relu,\n",
    "                                                    weights_initializer = l3w_init,biases_initializer = l3b_init,scope=\"first_fc\")\n",
    "        \n",
    "        first_fc_batch = tf.contrib.layers.batch_norm(first_fc,center = True, scale=False,is_training=batch_norm_train)\n",
    "        first_fc_final = tf.nn.dropout(first_fc_batch,keep_prob = fc_keep_prob) \n",
    "        \n",
    "    '''\n",
    "    with tf.variable_scope('layer_four_fc',reuse=reuse_flag):\n",
    "        l4w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        #l4b_init = tf.random_normal_initializer(mean=0, stddev=0.1, dtype=tf.float32)\n",
    "        l4b_init = tf.zeros_initializer()\n",
    "        \n",
    "        second_fc = tf.contrib.layers.fully_connected(inputs=first_fc_final,num_outputs=second_fc_out,activation_fn = tf.nn.relu,\n",
    "                                                    weights_initializer = l4w_init,biases_initializer = l4b_init,scope=\"second_fc\")\n",
    "        second_fc_batch = tf.contrib.layers.batch_norm(second_fc,center = True, scale=False,is_training=batch_norm_train)\n",
    "        second_fc_final = tf.nn.dropout(second_fc_batch,keep_prob = fc_keep_prob)\n",
    "    '''\n",
    "    # Third FC layer and softmax\n",
    "    with tf.variable_scope('layer_five_fc',reuse=reuse_flag):\n",
    "        l5w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        #l5b_init = tf.random_normal_initializer(mean=0, stddev=0.1, dtype=tf.float32)\n",
    "        l5b_init = tf.zeros_initializer()\n",
    "        \n",
    "        third_fc = tf.contrib.layers.fully_connected(inputs = first_fc_final,num_outputs=num_labels,weights_initializer=l5w_init,\n",
    "                                                    biases_initializer = l5b_init,activation_fn=tf.nn.relu,scope=\"third_fc\")\n",
    "        \n",
    "        third_fc_batch = tf.contrib.layers.batch_norm(third_fc,center=True,scale=False,is_training=batch_norm_train)\n",
    "        third_fc_final = tf.nn.dropout(third_fc_batch,keep_prob = fc_keep_prob)\n",
    "    \n",
    "    with tf.variable_scope('layer_six_softmax',reuse=reuse_flag):\n",
    "        softmax_logits = tf.nn.softmax(logits=third_fc_final,name=\"final_logits_softmax\")\n",
    "        \n",
    "    return input_tensor, batch_norm_train, third_fc_final,softmax_logits, conv_keep_prob,fc_keep_prob\n",
    "        \n",
    "\n",
    "\n",
    "def build_loss_optimizer(logits,softmax_logits,num_labels):\n",
    "    \n",
    "    ''' Loss and Optimizer builder, with Softmax Cross Entropy loss'''\n",
    "    \n",
    "    with tf.variable_scope('cross_entropy'):\n",
    "        \n",
    "        learning_rate_input = tf.placeholder(\n",
    "            tf.float32, [], name='learning_rate_input')\n",
    "        one_hot_labels = tf.placeholder(\n",
    "            name=\"one_hot_labels\",shape=[None,num_labels],dtype=tf.int32)\n",
    "        labels = tf.placeholder(\n",
    "            name=\"labels\",shape=[None],dtype=tf.int64)\n",
    "        \n",
    "        #cross_entropy_mean = tf.losses.softmax_cross_entropy(onehot_labels = one_hot_labels, logits=logits)\n",
    "        #cross_entropy_mean = tf.nn.softmax_cross_entropy_with_logits_v2(labels = one_hot_labels,logits=logits,name=\"cross_entropy_mean_v2\")\n",
    "        cross_entropy_mean = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels,logits=logits,name=\"cross_entropy_mean_sparse\")\n",
    "        loss = tf.reduce_mean(cross_entropy_mean,name=\"cross_entropy_loss\")\n",
    "        \n",
    "        'Toggle between optimizers..'\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate_input)\n",
    "        #optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate_input)\n",
    "        #optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate_input)\n",
    "        \n",
    "        extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS) # For batch normalization ops update\n",
    "        with tf.control_dependencies(extra_update_ops):\n",
    "            train_step = optimizer.minimize(loss)\n",
    "        \n",
    "        predicted_indices = tf.argmax(softmax_logits, 1, name=\"predicted_indices\")\n",
    "        correct_prediction = tf.equal(predicted_indices, labels, name='correct_prediction') # Labels are 0-indexed\n",
    "        confusion_matrix = tf.confusion_matrix(\n",
    "            labels, predicted_indices, num_classes=num_labels, name=\"confusion_matrix\")\n",
    "        \n",
    "        '''\n",
    "        # Because the accuracy will be calculated across batch splits, this facilitates resetting the metrics inter-epochs\n",
    "        with tf.variable_scope('streaming_ops'):\n",
    "            accuracy,update_op_acc = tf.metrics.accuracy(labels,predicted_indices)\n",
    "            \n",
    "        metrics_vars = tf.contrib.framework.get_variables('cross_entropy/streaming_ops',collection=tf.GraphKeys.LOCAL_VARIABLES)\n",
    "        reset_metrics_vars = tf.variables_initializer(metrics_vars) # Running sess.run will reset the streaming metrics within epochs\n",
    "        \n",
    "        # For tensorboard \n",
    "        xent_mean_scalar= tf.summary.scalar('cross_entropy_mean',tf.reduce_mean(cross_entropy_mean))\n",
    "        acc_scalar = tf.summary.scalar('accuracy',accuracy)\n",
    "        '''\n",
    "        \n",
    "        return train_step, confusion_matrix,predicted_indices,cross_entropy_mean, learning_rate_input,labels, one_hot_labels,loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative model based off\n",
    "# https://arxiv.org/pdf/1502.01710v5.pdf\n",
    "\n",
    "def build_graph_convnets(cutoff_len):\n",
    "    \n",
    "    num_chars = num_chars_dict\n",
    "    \n",
    "    char_embedding_n_dim = 256\n",
    "    \n",
    "    first_window_size = 7 # First window applied to sentence\n",
    "    second_window_size = 7 # Second window\n",
    "    third_window_size = 3 # And so on...\n",
    "    fourth_window_size = 3\n",
    "    fifth_window_size = 3\n",
    "    sixth_window_size = 3\n",
    "    \n",
    "    # No third, fourth, or fifth pool size, copying the paper's approach \n",
    "    first_pool_size = 3\n",
    "    second_pool_size = 3\n",
    "    sixth_pool_size = 3\n",
    "    \n",
    "    # One channel output for each of the six conv layers\n",
    "    first_layer_no_filters = 1024 \n",
    "    second_layer_no_filters = 1024\n",
    "    third_layer_no_filters = 1024\n",
    "    fourth_layer_no_filters = 1024\n",
    "    fifth_layer_no_filters = 1024\n",
    "    sixth_layer_no_filters = 1024\n",
    "    \n",
    "    first_fc_out = 2048\n",
    "    second_fc_out = 2048\n",
    "    third_fc_out = num_labels \n",
    "    # Hyper parameters end \n",
    "    \n",
    "    reuse_flag = tf.AUTO_REUSE\n",
    "    \n",
    "    # Layer Zero is just to define the input tensors and other 'global' tensors such as dropout probabilities\n",
    "    with tf.variable_scope('layer_zero_convnet',reuse=reuse_flag):\n",
    "        \n",
    "         # This layer defines the character embedding matrix, and returns character embedding for batches of sentences\n",
    "        # tensor: input_tensor is of shape [None, 26] where the first dimension is the batch_size * max_len for that batch\n",
    "        #input_tensor = tf.placeholder(dtype=tf.float32,shape=[None,cutoff_len,num_chars_dict],name=\"input_tensor\") # Unroll the first 2 dimensions at the numpy array before feeding\n",
    "        input_tensor = tf.placeholder(dtype=tf.float32,shape=[None,cutoff_len,num_chars_dict],name=\"input_tensor\") # Unroll the first 2 dimensions at the numpy array before feeding\n",
    "        conv_keep_prob = tf.placeholder(dtype=tf.float32,shape=[],name=\"conv_dropout_prob\")\n",
    "        fc_keep_prob = tf.placeholder(dtype=tf.float32,shape=[],name=\"fc_dropout_prob\")\n",
    "        batch_norm_train = tf.placeholder(name=\"batch_norm_train\", dtype=tf.bool)\n",
    "        print (input_tensor)\n",
    "        \n",
    "        #l0w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        #l0b_init = tf.zeros_initializer()\n",
    "        #char_embedding_mat = tf.get_variable(name=\"char_embedding\",dtype=tf.float32,shape=[num_chars_dict,char_embedding_n_dim],initializer=l0w_init,trainable=True)\n",
    "        #char_emb_bias = tf.get_variable(name=\"char_embedding_bias\",dtype=tf.float32,shape=[char_embedding_n_dim],initializer = l0b_init)\n",
    "        \n",
    "        # This gets the char embeddings for the batch\n",
    "        # Shape is [batch_size * cutoff_len, char_embedding_n_dim]\n",
    "        #input_tensor_2D = tf.reshape(input_tensor,shape=[-1,num_chars_dict]) # Reshaped to 2D with batch_size * cutoff_len as 1st dim\n",
    "        #l1_char_embedding = tf.matmul(input_tensor_2D,char_embedding_mat) + char_emb_bias\n",
    "        \n",
    "        #l1_char_embedding = tf.nn.embedding_lookup(char_embedding_mat,input_tensor,name=\"l1_char_embedding\")\n",
    "        #print(l1_char_embedding)\n",
    "        \n",
    "        # reshape input tensor to 3D matrix for the temporal 1D convolution operation\n",
    "        #input_tensor_with_embed = tf.reshape(l1_char_embedding,shape=[-1,cutoff_len,char_embedding_n_dim])\n",
    "        #input_tensor_with_embed = l1_char_embedding\n",
    "        #input_tensor_embed_relu = tf.nn.relu(input_tensor_with_embed,name=\"input_tensor_embed_relu\")\n",
    "        #input_tensor_batch = tf.contrib.layers.batch_norm(input_tensor_embed_relu,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #print(input_tensor_batch)\n",
    "        \n",
    "        # The input tensor is already one-hot encoded , with a character dictionary size of 26 characters\n",
    "        #input_tensor = tf.placeholder(dtype=tf.float32,shape=[None,cutoff_len,26],name=\"input_tensor\")\n",
    "        \n",
    "        \n",
    "        # Toggle application of either population or sample mean and variance during batch normalization\n",
    "        # is False during validation / testing to allow population statistics to apply\n",
    "        \n",
    "        \n",
    "        #first_embed_batch = tf.contrib.layers.batch_norm(input_tensor_with_embed,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #first_embed_relu = tf.nn.leaky_relu(first_embed_batch,name=\"first_embed_relu\")\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('layer_one_convnet',reuse=reuse_flag):\n",
    "        \n",
    "        #l1w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l1w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        \n",
    "        first_conv_layer = tf.layers.conv1d(inputs = input_tensor, filters = first_layer_no_filters,kernel_size = first_window_size,strides=1,padding='valid',\n",
    "                                            activation=tf.nn.relu, use_bias=True, kernel_initializer= l1w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"first_conv_layer\")\n",
    "        first_conv_batch = tf.contrib.layers.batch_norm(first_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #first_conv_relu = tf.nn.leaky_relu(first_conv_batch,name=\"first_conv_relu\")\n",
    "        first_maxpool_layer = tf.layers.max_pooling1d(inputs = first_conv_batch, pool_size = first_pool_size, strides=first_pool_size,\n",
    "                                                      padding='valid',name=\"first_maxpool_layer\")\n",
    "        \n",
    "\n",
    "        \n",
    "    with tf.variable_scope('layer_two_convnet',reuse=reuse_flag):\n",
    "        \n",
    "        #l2w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l2w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        second_conv_layer = tf.layers.conv1d(inputs = first_maxpool_layer, filters = second_layer_no_filters,kernel_size = second_window_size,strides=1,padding='valid'\n",
    "                                            ,activation=tf.nn.relu, use_bias=True, kernel_initializer= l2w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"second_conv_layer\")\n",
    "        second_conv_batch = tf.contrib.layers.batch_norm(second_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #second_conv_relu = tf.nn.leaky_relu(second_conv_batch,name=\"second_conv_relu\")\n",
    "        second_maxpool_layer = tf.layers.max_pooling1d(inputs = second_conv_batch, pool_size = second_pool_size, strides= second_pool_size,\n",
    "                                                      padding='valid',name=\"second_maxpool_layer\")\n",
    "\n",
    "\n",
    "        \n",
    "    with tf.variable_scope('layer_three_convnet',reuse=reuse_flag):\n",
    "        #l3w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l3w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        third_conv_layer = tf.layers.conv1d(inputs = second_maxpool_layer, filters = third_layer_no_filters,kernel_size = third_window_size,strides=1,padding='valid',\n",
    "                                            activation=tf.nn.relu, use_bias=True, kernel_initializer= l3w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"third_conv_layer\")\n",
    "        third_conv_batch = tf.contrib.layers.batch_norm(third_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #third_conv_relu = tf.nn.leaky_relu(third_conv_batch,name=\"third_conv_relu\")\n",
    "        \n",
    "    with tf.variable_scope('layer_four_convnet',reuse=reuse_flag):\n",
    "        #l4w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l4w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        fourth_conv_layer = tf.layers.conv1d(inputs = third_conv_batch, filters = fourth_layer_no_filters,kernel_size = fourth_window_size,strides=1,padding='valid',\n",
    "                                            activation=tf.nn.relu, use_bias=True, kernel_initializer= l4w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"fourth_conv_layer\")\n",
    "        fourth_conv_batch = tf.contrib.layers.batch_norm(fourth_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #fourth_conv_relu = tf.nn.leaky_relu(fourth_conv_batch,name=\"fourth_conv_relu\")\n",
    "        \n",
    "    with tf.variable_scope('layer_five_convnet',reuse=reuse_flag):\n",
    "        #l5w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l5w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        fifth_conv_layer = tf.layers.conv1d(inputs = fourth_conv_batch, filters = fifth_layer_no_filters,kernel_size = fifth_window_size,strides=1,padding='valid',\n",
    "                                            activation=tf.nn.relu, use_bias=True, kernel_initializer= l5w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"fifth_conv_layer\")\n",
    "        fifth_conv_batch = tf.contrib.layers.batch_norm(fifth_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #fifth_conv_relu = tf.nn.leaky_relu(fifth_conv_batch,name=\"fifth_conv_relu\")\n",
    "        \n",
    "    with tf.variable_scope('layer_six_convnet',reuse=reuse_flag):\n",
    "        #l6w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l6w_init = tf.contrib.layers.xavier_initializer(uniform=True,dtype=tf.float32)\n",
    "        sixth_conv_layer = tf.layers.conv1d(inputs = fifth_conv_batch, filters = sixth_layer_no_filters,kernel_size = sixth_window_size,strides=1,padding='valid',\n",
    "                                            activation=tf.nn.relu, use_bias=True, kernel_initializer= l6w_init,\n",
    "                                            bias_initializer=tf.zeros_initializer(),name=\"sixth_conv_layer\")\n",
    "        sixth_conv_batch = tf.contrib.layers.batch_norm(sixth_conv_layer,center=True,scale=True,is_training=batch_norm_train)\n",
    "        #sixth_conv_relu = tf.nn.leaky_relu(sixth_conv_batch,name=\"sixth_conv_relu\")\n",
    "        sixth_maxpool_layer = tf.layers.max_pooling1d(inputs = sixth_conv_batch, pool_size = sixth_pool_size, strides=sixth_pool_size,\n",
    "                                                      padding='valid',name=\"sixth_maxpool_layer\")\n",
    "        \n",
    "        #sixth_reshaped_layer = tf.reshape(sixth_maxpool_layer,shape=[-1,sixth_maxpool_layer.get_shape()[1] * sixth_maxpool_layer.get_shape()[2]])\n",
    "        sixth_reshaped_layer = tf.contrib.layers.flatten(sixth_maxpool_layer) \n",
    "        print (sixth_reshaped_layer)\n",
    "        \n",
    "    \n",
    "    with tf.variable_scope('layer_seven_convnet',reuse=reuse_flag):\n",
    "        #l7w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l7w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        l7b_init = tf.zeros_initializer()\n",
    "        \n",
    "        seventh_fc = tf.contrib.layers.fully_connected(inputs=sixth_reshaped_layer,num_outputs=first_fc_out,activation_fn = tf.nn.relu,\n",
    "                                                       weights_initializer = l7w_init,biases_initializer = l7b_init,scope=\"seventh_fc\")\n",
    "        seventh_fc_batch = tf.contrib.layers.batch_norm(seventh_fc,center = True, scale=True,is_training=batch_norm_train)\n",
    "        #seventh_fc_relu = tf.nn.leaky_relu(seventh_fc_batch,name=\"seventh_fc_relu\")\n",
    "        seventh_fc_final = tf.nn.dropout(seventh_fc_batch,keep_prob = fc_keep_prob)\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('layer_eight_convnet',reuse=reuse_flag):\n",
    "        #l8w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32)\n",
    "        l8w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        l8b_init = tf.zeros_initializer()\n",
    "        \n",
    "        eigth_fc = tf.contrib.layers.fully_connected(inputs=seventh_fc_final,num_outputs=second_fc_out,activation_fn = tf.nn.relu,\n",
    "                                                       weights_initializer = l8w_init,biases_initializer = l8b_init,scope=\"eigth_fc\")\n",
    "        eigth_fc_batch = tf.contrib.layers.batch_norm(eigth_fc,center = True, scale=True,is_training=batch_norm_train)\n",
    "        #eigth_fc_relu = tf.nn.leaky_relu(eigth_fc_batch,name=\"eigth_fc_relu\")\n",
    "        eigth_fc_final = tf.nn.dropout(eigth_fc_batch,keep_prob = fc_keep_prob)\n",
    "        \n",
    "        \n",
    "    with tf.variable_scope('layer_ninth_convnet',reuse=reuse_flag):\n",
    "        #l9w_init = tf.random_normal_initializer(mean=0,stddev=0.05,dtype=tf.float32) \n",
    "        l9w_init = tf.contrib.layers.xavier_initializer(uniform=True, dtype=tf.float32)\n",
    "        l9b_init = tf.zeros_initializer()\n",
    "        \n",
    "        ninth_fc = tf.contrib.layers.fully_connected(inputs=eigth_fc_final,num_outputs=num_labels,activation_fn = None,\n",
    "                                                     weights_initializer = l9w_init,biases_initializer = l9b_init,scope=\"ninth_fc\")\n",
    "        #ninth_fc_batch = tf.contrib.layers.batch_norm(ninth_fc,center = True, scale=True,is_training=batch_norm_train)\n",
    "\n",
    "        ninth_final_softmax = tf.nn.softmax(ninth_fc,name=\"ninth_final_softmax\")\n",
    "        \n",
    "        return input_tensor, batch_norm_train, ninth_fc,ninth_final_softmax, conv_keep_prob,fc_keep_prob\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the graph is built, randomly batch the inputs from disk cache, and feed them to the graph to train a model.\n",
    "\n",
    "Training takes between 30-45 min on a Tesla M60 GPU from an AWS g3 EC2 instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# Prepare batches in the cache\n",
    "# Used for training batch preparation\n",
    "def prepare_randomized_batches(save_dir,batch_size,cutoff_len,file_count):\n",
    "    \n",
    "    # Clean up for rerunning\n",
    "    if (os.path.exists(save_dir + 'inputs/batch/')):\n",
    "        shutil.rmtree(save_dir + 'inputs/batch/')\n",
    "    os.mkdir(save_dir + 'inputs/batch/')\n",
    "        \n",
    "    if (os.path.exists(save_dir + 'labels/batch/')):\n",
    "        shutil.rmtree(save_dir + 'labels/batch/')\n",
    "    os.mkdir(save_dir + 'labels/batch/')\n",
    "        \n",
    "    if (os.path.exists(save_dir + 'one_hot_labels/batch/')):\n",
    "        shutil.rmtree(save_dir + 'one_hot_labels/batch/')\n",
    "    os.mkdir(save_dir + 'one_hot_labels/batch/')\n",
    "    \n",
    "    # For batch preparation\n",
    "    ls_arrays = []\n",
    "    ls_arrays_one_hot_label = []\n",
    "    ls_arrays_label = []\n",
    "    \n",
    "    # A counter\n",
    "    j = 0\n",
    "    \n",
    "    # Randomize the Batch preparation\n",
    "    os.chdir(save_dir + 'inputs/')\n",
    "    \n",
    "    print ('The input directory is:' + save_dir + 'inputs/')\n",
    "    \n",
    "    file_list = glob.glob('*.npy')\n",
    "    random.shuffle(file_list)\n",
    "    \n",
    "    \n",
    "    # Prepare the batches\n",
    "    for file_item in file_list:\n",
    "    \n",
    "        nparr = np.load(save_dir + 'inputs/' + file_item)\n",
    "                \n",
    "        # Make sure it isnt an empty file\n",
    "        if nparr is not None and nparr.shape[0] != 0:\n",
    "                    \n",
    "            # Bottom-Pad  the sentences by adding 0s to the end of the sentence\n",
    "            # Or Truncate sentences that are longer than the cutoff length\n",
    "            \n",
    "            if (nparr.shape[0] <= cutoff_len):\n",
    "                npad = ((0,cutoff_len - nparr.shape[0]),(0,0))\n",
    "                #npad = (0,cutoff_len - nparr.shape[0])\n",
    "                nparr_pad = np.pad(nparr, pad_width=npad, mode='constant', constant_values=0)\n",
    "            else: # Truncate to the cutoff length\n",
    "                #nparr_pad = nparr[:cutoff_len,:]\n",
    "                nparr_pad = nparr[:cutoff_len,:]\n",
    "            \n",
    "            \n",
    "            ls_arrays.append(nparr_pad.tolist())\n",
    "            \n",
    "            one_hot_label = np.load(save_dir + 'one_hot_labels/' + 'one_hot_sentence_label_' + file_item.split('_')[3])\n",
    "            ls_arrays_one_hot_label.append(one_hot_label.tolist())\n",
    "            \n",
    "            label = np.load(save_dir + 'labels/' + 'sentence_label_' + file_item.split('_')[3])\n",
    "            ls_arrays_label.append(label.tolist())\n",
    "            \n",
    "            j += 1\n",
    "                \n",
    "            # Save batches of the files every batch_size step\n",
    "            if (j % batch_size == 0 or j == file_count):\n",
    "                    \n",
    "                nparr_batch = np.asarray(ls_arrays)\n",
    "                nparr_batch_one_hot_labels = np.asarray(ls_arrays_one_hot_label)\n",
    "                nparr_batch_labels = np.asarray(ls_arrays_label)\n",
    "                    \n",
    "                np.save(save_dir + 'inputs/batch/nparr_batch_' + str(j) + '.npy',nparr_batch)\n",
    "                np.save(save_dir + 'labels/batch/nparr_batch_labels_' + str(j) +  '.npy',nparr_batch_labels)\n",
    "                np.save(save_dir + 'one_hot_labels/batch/nparr_batch_one_hot_labels_' + str(j) +'.npy',nparr_batch_one_hot_labels)\n",
    "                    \n",
    "                ls_arrays = []\n",
    "                ls_arrays_one_hot_label = []\n",
    "                ls_arrays_label = []\n",
    "                \n",
    "                \n",
    "# Gets a random batch of a specified size\n",
    "# This function is not used in the final model\n",
    "# But can be used for mini-batch Gradient Descent \n",
    "def get_a_random_batch(save_dir,batch_size,cutoff_len):\n",
    "\n",
    "        ls_batch_list = [] # Stores the names of all the files randomly selected without replacement\n",
    "    \n",
    "        i = 0\n",
    "        while (i < batch_size):\n",
    "            \n",
    "            rand_choice = random.choice(os.listdir(save_dir + 'inputs/'))\n",
    "            if rand_choice not in ls_batch_list:\n",
    "                ls_batch_list.append(rand_choice)\n",
    "                i = i + 1\n",
    "        \n",
    "        ls_arrays = []\n",
    "        ls_arrays_one_hot_label = []\n",
    "        ls_arrays_label = []\n",
    "        \n",
    "        print ('The list length is:' + str(len(ls_batch_list)))\n",
    "        \n",
    "        for item in ls_batch_list:\n",
    "            \n",
    "            nparr = np.load(save_dir + 'inputs/' + item)\n",
    "            if nparr is not None and nparr.shape[0] != 0:\n",
    "                \n",
    "                #Bottom-Pad  the sentences by adding 0s to the end of the sentence\n",
    "                # Or truncate the sentence upto the length\n",
    "                if (nparr.shape[0] <= cutoff_len):\n",
    "                    npad = ((0,cutoff_len - nparr.shape[0]),(0,0))\n",
    "                    #npad = (0,cutoff_len - nparr.shape[0])\n",
    "                    nparr_pad = np.pad(nparr, pad_width=npad, mode='constant', constant_values=0)\n",
    "                else:\n",
    "                    nparr_pad = nparr[:cutoff_len,:]\n",
    "                \n",
    "                ls_arrays.append(nparr_pad.tolist())\n",
    "                \n",
    "                one_hot_label = np.load(save_dir + 'one_hot_labels/' + 'one_hot_sentence_label_' + item.split('_')[3])\n",
    "                ls_arrays_one_hot_label.append(one_hot_label.tolist())\n",
    "                \n",
    "                label = np.load(save_dir + 'labels/' + 'sentence_label_' + item.split('_')[3])\n",
    "                ls_arrays_label.append(label.tolist())\n",
    "                \n",
    "                \n",
    "        nparr_batch = np.asarray(ls_arrays)\n",
    "        nparr_batch_one_hot_labels = np.asarray(ls_arrays_one_hot_label)\n",
    "        nparr_batch_labels = np.asarray(ls_arrays_label)\n",
    "        \n",
    "        return nparr_batch, nparr_batch_labels, nparr_batch_one_hot_labels\n",
    "              \n",
    "\n",
    "        \n",
    "'''\n",
    "The main training function below. Trains the data using mini-batch gradient descent, \n",
    "and saves checkpoints of the model \n",
    "'''    \n",
    "def execute_training():\n",
    "    \n",
    "    print ('Starting the training process')\n",
    "    \n",
    "    num_epochs = 5000\n",
    "    mini_batch_size = 128\n",
    "    batch_size = 200\n",
    "    learning_rate = 0.01\n",
    "    \n",
    "    mini_batch_runs = 100\n",
    "    \n",
    "    if (os.path.exists(train_tensorboard_dir)):\n",
    "        shutil.rmtree(train_tensorboard_dir)\n",
    "    os.mkdir(train_tensorboard_dir)\n",
    "    \n",
    "    \n",
    "    if (os.path.exists(valid_tensorboard_dir)):\n",
    "        shutil.rmtree(valid_tensorboard_dir)\n",
    "    os.mkdir(valid_tensorboard_dir)\n",
    "    \n",
    "    print ('Building the graph')\n",
    "    \n",
    "    # Build graph object\n",
    "    with tf.Graph().as_default() as grap:\n",
    "        #input_tensor, batch_norm_train, logits, softmax_logits,conv_keep_prob,fc_keep_prob = build_graph(cutoff_len)\n",
    "        \n",
    "        input_tensor, batch_norm_train, logits, softmax_logits, conv_keep_prob,fc_keep_prob = build_graph_convnets(cutoff_len)\n",
    "            \n",
    "        train_step, confusion_matrix,predicted_indices,cross_entropy_mean, learning_rate_input,labels,one_hot_labels,loss =  build_loss_optimizer(logits,softmax_logits,num_labels)\n",
    "        \n",
    "            \n",
    "    print ('Kicking off the session')\n",
    "    \n",
    "    # Initiate session for execution\n",
    "    with tf.Session(graph=grap) as sess:\n",
    "        \n",
    "        saver = tf.train.Saver() # For saving checkpoints for test inference \n",
    "        \n",
    "        # Initialize\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        #sess.run(reset_metrics_vars)\n",
    "        \n",
    "        train_writer = tf.summary.FileWriter(train_tensorboard_dir,sess.graph)\n",
    "        valid_writer = tf.summary.FileWriter(valid_tensorboard_dir)\n",
    "        \n",
    "        # Prepares randomized batches in the /inputs/batch folder\n",
    "        #prepare_randomized_batches(one_hot_train_dir,train_batch_size,cutoff_len,train_count)\n",
    "        \n",
    "        print ('Preparing randomized validation batches')\n",
    "        prepare_randomized_batches(one_hot_valid_dir,batch_size,cutoff_len,valid_count)\n",
    "        \n",
    "        xent_counter = 0\n",
    "        \n",
    "        _guid = uuid.uuid4()\n",
    "        \n",
    "        for i in range(0,num_epochs):\n",
    "        \n",
    "            print ('Starting Training, the Epoch is:' + str(i + 1))\n",
    "            xent_summary = 0\n",
    "            \n",
    "            # Switch to the /inputs/batch folder and fetch all the batch files \n",
    "            # Which will be passed to the training mechanism\n",
    "            #os.chdir(one_hot_train_dir + 'inputs/batch/')\n",
    "            #train_batch_files = glob.glob('*.npy')\n",
    "            #random.shuffle(train_batch_files)\n",
    "            \n",
    "            train_conf_matrix = None\n",
    "            \n",
    "            #for file in train_batch_files:\n",
    "            for j in range(0,mini_batch_runs): # 20 mini batch runs per epoch\n",
    "            \n",
    "                print ('Get a random training batch:'  + str(j + 1))\n",
    "                train_batch, train_batch_labels, train_batch_one_hot = get_a_random_batch(one_hot_train_dir,mini_batch_size,cutoff_len)\n",
    "                #print (train_batch.shape)\n",
    "                #print (train_batch_one_hot)\n",
    "                #print (train_batch_labels)\n",
    "                \n",
    "                #train_batch = np.load(one_hot_train_dir + 'inputs/batch/' + file)\n",
    "                #train_labels = np.load(one_hot_train_dir + 'labels/batch/nparr_batch_labels_' + file.split('_')[2])\n",
    "                #train_one_hot_labels = np.load(one_hot_train_dir + 'one_hot_labels/batch/nparr_batch_one_hot_labels_' + file.split('_')[2])\n",
    "                \n",
    "                # Kick off the training\n",
    "                _,los,conf_matrix,xent_mean,pi = sess.run(\n",
    "                [train_step,loss,confusion_matrix,cross_entropy_mean,predicted_indices],\n",
    "                feed_dict = {input_tensor : train_batch,\n",
    "                             labels: train_batch_labels,\n",
    "                             one_hot_labels : train_batch_one_hot,\n",
    "                             learning_rate_input : learning_rate,\n",
    "                             batch_norm_train : True,\n",
    "                             conv_keep_prob : 1.0,\n",
    "                             fc_keep_prob : 0.7\n",
    "                    })\n",
    "                \n",
    "                if (train_conf_matrix is None):\n",
    "                    train_conf_matrix = conf_matrix\n",
    "                else:\n",
    "                    train_conf_matrix += conf_matrix\n",
    "            \n",
    "                #print (pi)\n",
    "                #print (train_batch_labels)\n",
    "                \n",
    "                xent_counter += 1\n",
    "                xent_summary += np.sum(xent_mean)\n",
    "                #train_writer.add_summary(xent_scalar,xent_counter)\n",
    "                with open (log_dir + str(_guid) + '.txt','a') as eg:\n",
    "                    eg.write('The training cross entropy sum and avg at step:' + str(xent_counter) + ' is:')\n",
    "                    eg.write(str(np.sum(xent_mean)) + ';')\n",
    "                    eg.write(str(los) + '\\n')\n",
    "                \n",
    "                \n",
    "                \n",
    "            # Print confusion matrix out\n",
    "            with open (log_dir + str(_guid) + '.txt','a') as eg:\n",
    "                eg.write('\\n' + 'Training Confusion Matrix:' + '\\n' + str(train_conf_matrix))\n",
    "                true_pos = np.sum(np.diag(train_conf_matrix))\n",
    "                all_pos = np.sum(train_conf_matrix)\n",
    "                eg.write('\\n' + 'Training Accuracy is: ' + str(float(true_pos / all_pos)) + '\\n')  # Another way to get accuracy!\n",
    "                eg.write('Average cross entropy error is:' + str(xent_summary/(mini_batch_runs * mini_batch_size)) + '\\n')\n",
    "            \n",
    "            #_,scalar = sess.run([accuracy,acc_scalar])\n",
    "            #train_writer.add_summary(scalar,i)\n",
    "            \n",
    "            #sess.run(reset_metrics_vars)\n",
    "            \n",
    "            \n",
    "            # Save model every 10 epochs\n",
    "            if ((i + 1) % 10 == 0):\n",
    "                print ('Saving checkpoint after epoch:' + str(i + 1))\n",
    "                saver.save(sess=sess,save_path=checkpoint_dir + 'Char_Sent_Classification_CNN.ckpt',global_step = i )\n",
    "                \n",
    "            #if ((i + 1) % 20 == 0):\n",
    "            # Run on validation data to check validation accuracy\n",
    "            print ('Training Epoch ' + str(i + 1) + ' is complete. Running Validation Stats..')\n",
    "             \n",
    "            os.chdir(one_hot_valid_dir + 'inputs/batch/')\n",
    "            valid_batch_files = glob.glob('*.npy')\n",
    "            random.shuffle(valid_batch_files)\n",
    "            \n",
    "            valid_conf_matrix = None\n",
    "            \n",
    "            for file in valid_batch_files:\n",
    "                \n",
    "                valid_batch = np.load(one_hot_valid_dir + 'inputs/batch/' + file)\n",
    "                valid_labels = np.load(one_hot_valid_dir + 'labels/batch/nparr_batch_labels_' + file.split('_')[2])\n",
    "                valid_one_hot_labels = np.load(one_hot_valid_dir + 'one_hot_labels/batch/nparr_batch_one_hot_labels_' + file.split('_')[2])\n",
    "               \n",
    "                \n",
    "                val_conf_matrix = sess.run(\n",
    "                confusion_matrix,\n",
    "                feed_dict = {input_tensor : valid_batch,\n",
    "                            labels: valid_labels,\n",
    "                            one_hot_labels : valid_one_hot_labels,\n",
    "                            batch_norm_train : False,\n",
    "                            conv_keep_prob : 1.0,\n",
    "                            fc_keep_prob : 1.0\n",
    "                         \n",
    "                })\n",
    "                \n",
    "                if (valid_conf_matrix is None):\n",
    "                    valid_conf_matrix = val_conf_matrix\n",
    "                else:\n",
    "                    valid_conf_matrix += val_conf_matrix\n",
    "                    \n",
    "            \n",
    "            #val_acc,val_summary = sess.run([accuracy,acc_scalar])\n",
    "            #valid_writer.add_summary(val_summary,i)\n",
    "            \n",
    "            #sess.run(reset_metrics_vars)\n",
    "            \n",
    "            with open (log_dir + str(_guid) + '.txt','a') as eg:\n",
    "                eg.write('Validation Confusion Matrix:' + '\\n' + str(valid_conf_matrix))\n",
    "                true_pos = np.sum(np.diag(valid_conf_matrix))\n",
    "                all_pos = np.sum(valid_conf_matrix)\n",
    "                eg.write('\\n' + 'Validation Accuracy is: ' + str(float(true_pos / all_pos)) + '\\n') \n",
    "\n",
    "            \n",
    "execute_training()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the saved checkpoints to do inference on the batch of test sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(cutoff_len):\n",
    "    \n",
    "    with tf.Graph().as_default() as grap:\n",
    "        input_tensor, batch_norm_train, logits, softmax_logits,conv_keep_prob,fc_keep_prob = build_graph(cutoff_len)\n",
    "        \n",
    "    with open(one_hot_test_dir + 'ytest.txt','w') as predfile:\n",
    "    \n",
    "        with tf.Session(graph=grap) as sess:\n",
    "        \n",
    "            init = tf.global_variables_initializer()\n",
    "            sess = tf.Session()\n",
    "            sess.run(init)\n",
    "        \n",
    "            saver = tf.train.Saver()\n",
    "        \n",
    "            checkpoint_file_path = '/home/ubuntu/Desktop/challenge/offline_challenge_to_send/offline_challenge_to_send/checkpoints/Char_Sent_Classification_CNN.ckpt-99'\n",
    "\n",
    "            print ('Loading checkpoint file:' + checkpoint_file_path)\n",
    "            saver.restore(sess,checkpoint_file_path)\n",
    "\n",
    "            os.chdir(one_hot_test_dir + 'inputs/')\n",
    "            test_files = glob.glob('*.npy')\n",
    "               \n",
    "            for file in test_files:\n",
    "            \n",
    "                _idx = file.split('_')[3].replace('.npy','')\n",
    "            \n",
    "            \n",
    "                nparr = np.load(one_hot_test_dir + 'inputs/' + file)\n",
    "                npad = ((0,cutoff_len - nparr.shape[0]),(0,0))\n",
    "                nparr_pad = np.pad(nparr, pad_width=npad, mode='constant', constant_values=0)\n",
    "            \n",
    "                nparr_2 = np.expand_dims(nparr_pad,axis=0)\n",
    "             \n",
    "                predictions,soft = sess.run(\n",
    "                [\n",
    "                    logits,softmax_logits\n",
    "                ],feed_dict = {input_tensor : nparr_2,\n",
    "                                batch_norm_train : False,\n",
    "                                conv_keep_prob : 1.0,\n",
    "                                fc_keep_prob : 1.0})\n",
    "                \n",
    "                predicted_index = tf.argmax(soft, axis=1)\n",
    "                \n",
    "                predfile.write(str(_idx,) + ',' + str(predicted_index.eval(session=sess)[0]) + '\\n')\n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "#inference(cutoff_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "APPENDIX below: \n",
    "\n",
    "This is an alternative model based off Text Understanding From Scratch by Xiang ZHang and Yann LeCun. It is not used in the final model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "tensorflow-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
